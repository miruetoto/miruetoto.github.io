---
title : (논문) very first draft of HST 
layout: post 
---

### About this doc

- 이 문서에서는 HST에 대한 드래프트이다. 

### Definition of HST 

- 그래프형태의 자료 $(V,E,f(V))$에서의 hst를 $(V,E,\boldsymbol{h}(V))$라고 정의하자. 각각의 $i=1,\dots,n$에 대하여 $\boldsymbol{h}(v_i)$ 는 크기가 $1\times (\tau+1)$인 벡터이다. 각각의 $i,j \in \\{1,\dots,n\\}$ 에 대하여 노드 $v_i$, $v_j$ 간의 snow-dist 는 아래와 같이 정의한다. 
\begin{align}
d_1(v_i,v_j) := \\|\boldsymbol{h}(v_i)-\boldsymbol{h}(v_j) \\|_ 2^2 
\end{align}
각각의 노드에 새롭게 쌓인 눈의 양만을 알고 싶다면 아래를 살펴보는 것도 좋다. 
\begin{align}
\boldsymbol{\dot h}(v_1), \dots, \boldsymbol{\dot h}(v_n).
\end{align}
이것은 각각 $\boldsymbol{h}(v_1),\dots,\boldsymbol{h}(v_n)$ 을 차분해서 얻을 수 있는 길이가 $1\times \tau$ 벡터이다. 이것을 이용하여 아래와 같이 snow-dist 를 정의할 수도 있다. 
\begin{align}
d_2(v_i,v_j) := \\|\boldsymbol{\dot h}(v_i)-\boldsymbol{\dot h}(v_j) \\|_ 2^2 
\end{align}

- 각 노드에 내리는 눈의 양을 $\epsilon_i=\epsilon(v_i)$ 라고 하자. 눈이 내린 횟수 $\ell$ 을 고정하였을때, $v_i$ 지역으로 눈을 흘려보낼 수 있는 지역들의 모임을 ${\cal N}(i,\ell)$ 라고 하고 엄밀하게는 아래와 같이 정의하자. 
\begin{align}
{\cal N}(i,\ell) := \Big\\{j:(i,j)\in E , ~ h^{\ell}(v_i)+\epsilon(v_i) \leq h^{\ell}(v_j) \Big\\} 
\end{align}
이때 ${\cal N}(i,\ell)$ 은 ${\cal N}_ {v_i}:={\cal N}_ i:=\\{j: (i,j)\in E \\}$ 와는 다름을 유의하라. 각 노드에 쌓인 눈의 양을 아래와 같이 정의할 수 있다. 
\begin{align}
h^{\ell}(v_i)&=h^{\ell-1}(v_i)+ \sum_{j \in {\cal N}(i,\ell-1)}\Big( \frac{\epsilon(v_j)}{\|{\cal N}_ j \| }+\frac{\epsilon(v_i)}{\|{\cal N}_ i \| }\Big)  \\\\ \\ 
&= h^{\ell-1}(v_i)+ \frac{\epsilon(v_i)}{\|{\cal N}_ i \| }\|{\cal N}(i,\ell-1)\|+ \sum_{j \in {\cal N}(i,\ell-1)} \frac{\epsilon(v_j)}{\|{\cal N}_ j \| }
\end{align}

- HST를 매트릭스를 이용하여 정의하여 보자. $\ell = 0,1,2,\dots $ 에 대하여 ${\boldsymbol h}^{\ell}$ 을 각각 길이가 $n$인 벡터라고 하자. 그리고 $n\times n$ 매트릭스 ${\bf A}^{(\ell)}$ 를 아래와 같이 정의하자. 
\begin{align}
{\bf A}^{(\ell)}_ {ij}=
\begin{cases} 
\frac{ 1_ { {\cal N}(i,\ell-1) }(j)}{\|{\cal N}_ j\|}  & i \neq j  \\\\ \\
\frac{\|{\cal N}(i,\ell-1)\|}{\|{\cal N}_ i\|} & i=j
\end{cases} 
\end{align}
여기에서 
\begin{align}
1_A(x)=
\begin{cases}
1 & x \in A \\\\ \\
0 & x \notin A 
\end{cases}
\end{align} 
이다. 이제 $\ell = 1,2,\dots $ 에 대하여 ${\boldsymbol f}=(f(v1),\dots,f(v_n))'$ 의 HST 를 아래와 같이 정의할 수 있다. 
\begin{align}
{\boldsymbol h}^{\ell}= {\bf A}^{(\ell)} {\boldsymbol \epsilon}+ {\boldsymbol h}^{\ell-1} 
\end{align}

--- 

### Basic Assumption 

- 그래프 ${\cal G}=(V,E)$는 어떠한 매니폴드의 ${\cal M}$ 의 ***sample version*** 이라고 하자. 여기에서 ${\cal M}$ 은 유클리드 스페이스 $\mathbb{R}^{N}$에 아이소메트릭하게 임베딩된 ***compact and smooth*** 매니폴드라고 가정한다(벨킨,2008,p1290).  이는 마치 continuous component 와  discrete component 의 관계와 비슷하다. 여기에서 $E$ 대신에 $W$ 를 써서 ${\cal G}=(V,W)$와 같이 정의할 수도 있다. 우리는 일단 아래의 성질을 만족하는 상황에서만 이론을 전개할 것이다. 
\begin{align}
{\cal G} \to {\cal M} \quad as~ n\to \infty 
\end{align}
신호처리식 언어로 표현하면 샘플링 주기가 $0$에 가까워지면 distrcret signal 이 continuous signal 로 수렴한다는 의미이다. 이때 샘플링은 일정한 간격으로 할 수도 있지만 어떤 특정확률에 따라서 랜덤하게 샘플링할 수도 있다. 따라서 기븐 그래프 ${\cal G}=(V,E)$ 가 어떠한 매니폴드의 sample version 이라고 볼 수 도 있지만 어떠한 랜덤엘리먼트의 ***realization*** 이라고 볼 수도 있다. 

- 우리가 분석하고 싶은 신호는 특정 매니폴드를 도메인으로 가지는 함수 $f$이다. 즉 
\begin{align}
f:{\cal M} \to \mathbb{R} 
\end{align}
이다. 

- ***modeling.*** 그래프 $(V,E)$ 가 given일때 각각의 노드 $v_i$에서 정의되는 확률변수 $f(v_i)$는 아래와 같은 분포를 가진다고 가정하자. 
\begin{align}
f(v_i) \sim F_{\theta_i}
\end{align}
그리고 아래를 만족하는 함수 $g$ 가 존재하는 ***scale*** $\tau$ 가 $\\{0\\} \cup \mathbb{N}$ 중에서 적어도 하나 이상은 있다고 하자. (**그리고 이러한 $g$ 가 존재하는 $\tau$ 들의 집합을 ${\cal T}^* $ 라고 하자.**)  
\begin{align}
\theta_i=g\big({\boldsymbol \theta}_ {(-i)},\boldsymbol{\dot h}_ {(-i)} \big)
\end{align}
여기에서 $\tau=0$ 이면 $\theta_i=g\big({\boldsymbol \theta}_ {(-i)}\big)$ 이다. 그리고 
\begin{align}
\begin{cases}
{\boldsymbol \theta}_ {(-i)}:=(\theta_1,\dots,\theta_{i-1},\theta_{i+1},\dots,\theta_n)  \\\\ \\
\boldsymbol{\dot h}_ {(-i)}:=(\boldsymbol{\dot h}_ 1,\dots,\boldsymbol{\dot h}_ {i-1},\boldsymbol{\dot h}_ {i+1},\dots,\boldsymbol{\dot h}_ n) 
\end{cases}
\end{align}
이다. 

---

### Visualizing Examples of Importance 

- 이 챕터에서는 공통적으로 (1) HST를 활용해 거리를 정의하고 (2) 그것을 통하여 중요성을 정의 한 뒤 (3) 적당한 visualization 테크닉을 사용하여 시각화를 한다. 

- 이 챕터에서 사용되는 시각화의 종류는 (1) importance coloring (2) 주성분 분석을 활용한 3-dim 시각화 (3) TDA 식 시각화를 제시한다. 이중에서 (1)은 원래 자료와 가장 직관적으로 연결가능하다는 장점이 있다. (2) 는 자료의 특징을 토폴로지컬한 모양을 통하여 직관적으로 파악가능하다는 장점이 있다. 하지만 (1) 과 (2) 는 공통적으로 특정 scale 이 fix 되어 있어야하는 단점이 있다. 반면에 TDA 식 시각화는 (1),(2) 의 장점은 없지만 다양한 스케일에서 자료의 변화를 한번에 살펴볼 수 있다는 장점이 있다. 이것은 분석에 유리한 ***scale*** 을 찾는데 도움을 준다. 

- 이론적인 부분은 전혀없지만 우리가 정의한것이 매우 다양한 셋팅에서 유용하게 활용될 수 있다는 것을 보여주는 챕터가 될것 이다. 

#### Example 1. 스트럭처가 없는 1차원자료에서 평균이 다른 자료, 분산이 다른 자료들을 보여준다. 

- 그래프자료에서 모든 스트럭처간에 눈이 이동할수 있다고 생각하고 중요성을 정의한 뒤 보여준다. 그룹간의 수를 다르게 하면 희소성을 판단하기 좋은 예제가 될것 같다. 

#### Example 2. 스트럭처가 없는 2차원자료에서 LOF 예제를 분석 

- LOF 예제와 똑같이 만들면 될것 같다. 

#### Example 3. Local min-max 

- 여기에서는 신호가 랜덤이 아니라고 가정한다. HST 가 모양을 잘 찾음을 보여준다. 

#### Example 4. 첩 + 단일주파수 신호

- 여기에서도 신호가 랜덤이 아니라고 가정한다. 예제3의 확장 

#### Example 5. 값이 없는 자료에서 (그냥 그래프자료) 스트럭처의 변화를 포착하여 보여준다. 

- 그래프자료에서 신호값에 해당하는 것을 모두 0으로 두고 HST를 적용한다음 중요성을 보여준다. 소셜네트워크에서 중요한사람? 을 나타내준다. 

#### Example 6. given regular structure 에서 값들의 변화가 있는 경우 (chage point detection) 

- 그냥 HST를 적용하고 보여주기만 하면 된다. 지하철자료가 좋은 예가 될 수 있다. (강남역 vs 신림역 / 요일별 특이점 / 지역별특이점) 

#### Example 7. 시간에 따라서 스트럭처가 변화하는 경우 

- 소셜네트워크에서 메신저를 보낸 횟수의 변화를 포착함. 


--- 


### Properties 

#### Regular graph

- ***claim 1.*** 각각의 노드 $v_i$에서 정의된 확률변수 $f(v_i)$가 서로 독립이고 같은분포를 가진다고 하자. 그리고 $(V,E)$ 가 ***regular graph*** 라고 하자. 그리고 아래를 가정하자. 
\begin{align}
f(v_1),\dots f(v_n) ~ \sim ~ i.i.d. ~ F\big(f(v);\theta\big)
\end{align} 
그러면 모든 $\tau \in \mathbb{N}$ 에 대하여 각 $v_i$에 대응하는 확률벡터 $\boldsymbol{\dot h}(v_i)$ 역시 서로 독립이고 같은 분포를 가진다. 즉  \begin{align}
\boldsymbol{\dot h}(v_1),\dots \boldsymbol{\dot h}(v_n) ~ \sim ~ i.i.d. ~ G\big(\boldsymbol{\dot h}(v);\eta\big)
\end{align}
이다. 이때 $F:\mathbb{R} \to \mathbb{R}$ 이고 $G: \mathbb{R}^{\tau} \to \mathbb{R}$ 임을 유의하라. 

- 적당한 매니폴드 ${\cal M}$ 이 있다고 하자. $T$를 분포 $G$에서 $\mathbb{R}$ 로 가는 ***functional*** 이라고 하자. 여기에서
\begin{align}
T(G;{\cal M})=\mathbb{E} \\| {u\in {\cal M}_ v} \boldsymbol{\dot h}(v)-\boldsymbol{\dot h}(u) \\|_ 2^2 
\end{align}
라고 두고 싶다. 그런데 $T(G;E)$ 가 ***Fisher consistent*** 를 만족하려면 
\begin{align}
T(G)=\mathbb{E} \\| \frac{1}{\|{\cal N}_ v\|}\sum_{u\in {\cal N}_ v} \boldsymbol{\dot h}(v)-\boldsymbol{\dot h}(u) \\|_ 2^2 = \theta
\end{align}
라고 둘 수 있어야 하는데 이것이 좀 애매한게 $\boldsymbol{\dot h}(v)$ 의 분포를 결정짓는 파라메터가 $\theta$ 이라는게 좀 걸린다. 아무튼 이렇게 둘 수 있다면 given data가 있을때 아래와 같이 정의할 수 있다. 
\begin{align}
T(G_n ; E)=\mathbb{E} \\| \frac{1}{\|{\cal N}_ v\|}\sum_{u\in {\cal N}_ v} \boldsymbol{\dot h}(v)-\boldsymbol{\dot h}(u) \\|_ 2^2 
\end{align}

- 여기에서 $\mathbb{E} \\| \boldsymbol{\dot h}(v_i)-\boldsymbol{\dot h}(v_j) \\|_ 2^2$ 는 ***quadratic form of graph Laplacian*** 이다.

- $G_n$을 벡터 $\boldsymbol{\dot h}(v_1),\dots,\boldsymbol{\dot h}(v_n)$에 대응하는 empirical-cdf 라고 하자. 우리가 regular graph 를 가정하였으므로 ***claim 1*** 에 의해서 $\boldsymbol{\dot h}(v_1),\dots \boldsymbol{\dot h}(v_n)$ 역시 ***i.i.d.*** 가 될것이고 따라서 글리벤코-칸텔리 정리를 쓸 수 있을 것이다. 글리벤코-칸텔리정리는 $G_n \to G$ 임을 보여준다. 따라서 적당히 큰 $n$에 대하여 $G_n$은 $G$의 neighborhood에 속한다고 볼 수 있다. 따라서 
\begin{align}
T(G_n) & \approx  T(G)+\int IF(x,T,G)d(G_n-G)(x) \\\\ \\ 
& = T(G)+\int IF(x,T,G) dG_n(x) 
\end{align}
이다. 여기에서 $\int IF(x,T,G)dG(x)=0$ 임이 사용되었다. 따라서 
\begin{align}
\sqrt{n}\Big(T(G_n)-T(G) \Big) \to N(0,V(T,G))
\end{align}
이다. 여기에서 $V(T,G)=\int IF(x,T,G)^2 dG(x)$ 이다. 

---


### Application 

- HST는 기본적으로 레이블이 없는자료에서 매우 폭넓게 활용가능하다. 

#### Smoothing 

- 중요도를 고려하여 샘플링하고 그것을 바탕으로 스무딩을 수행한다. 리만매니폴드로의 근사를 증명할 수 있다. (미하일-버킨, Mikhail Berlkin 2008 참고). 또한 헴펠교재 P.316, 그리고 빅터요하이 등을 참고하면 regression setting 이 나오는데 이걸 smoothing 으로 변형하여 풀어보면 이론적인 성질이 얻어질 수 있다. 또 Damien Garcia 의 로버스트 스무딩도 참고할만하다. 아무튼 관련 이론을 정리하고 지진자료 혹은 기상자료의 smoothing 을 수행하면 의미가 있어보인다. (시공간이 동시에 변화하는 자료들) 적절한 $\tau$는 적당히 스무딩 파라메터 느낌으로 찾는다. 

#### Clustering 

- Spectral clustering 혹은 TDA식 클러스터링을 활용하거나 cut을 사용할 수 있다. 자료는 고민중.. 
