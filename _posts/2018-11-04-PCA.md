---	
layout: post	
title: (정리) 주성분분석	
--- 	

### 비지도 기반 차원축소기법	

- 차원축소란 입력차원 $\\{ {\bf x}_ i\\}_ {i=1}^{n}$의 차원이 매우 클때 그것을 낮은차원의 자료들 $\\{ {\bf z}_ i\\}_ {i=1}^{n}$로 바꾸는 방법을 말한다. 	

- 차원축소를 수행하는 연산이 선형이면 적당한 행렬 ${\bf T}$를 사용하여 ${\bf z}_ i={\bf x}_ i{\bf T}$와 같은 방식으로 쓸 수 있다. 가령 예를 들어서 ${\bf x}_ i=[1,2,4,5,8]$과 같은 자료가 있다고 하자. 이때 ${\bf x}_ i$의 차원은 $1\times 5$이다. 그러면 ${\bf T}$를 $5\times 2$와 같은 행렬을 잡아와서 ${\bf z}_ i={\bf x}_ i{\bf T}$와 같이 쓸 수 있는데 이러면 ${\bf z}_ i$의 차원은 $1 \times 2$가 되고 ${\bf T}$는 5차원자료를 2차원으로 줄이는 변환이 된다. 좀 더 일반적으로는 아래와 같이 선형변환을 표현할 수 있다. 	
\begin{align}	
{\bf Z}_ {n\times p'} = {\bf X}_ {n \times p} {\bf T}_ {p \times p'} 	
\end{align}	

### 주성분분석 

- 주성분 분석은 (1) ${\bf z}_ i$가 ${\bf x}_ i$의 정사영이라는 제약아래에서 (2) ${\bf z}_ i \approx {\bf x}_ i$가 최대한 만족하도록 하는 선형변환 ${\bf T}$를 찾고자 하는 기법이다. 여기에서 (1)은  ${\bf T}'{\bf T}={\bf I}_ {p' \times p'}$이라는 조건과 동치이다(이게 이해안되면 선대다시공부해야함). 	

- (2)를 만족하기 위해서는 $\sum_{i=1}^{n} \\| {\bf z}_ i - {\bf x}_ i \\|^2$을 최소화 하면 될것이다. 그런데 ${\bf z}_ i$와 ${\bf x}_ i$의 차원이 다르므로 직접거리를 잴 수 없다. 따라서 ${\bf z}_ i$에 ${\bf T'}$를 곱한 아래식을 최소화 한다. 	
\begin{align}	
\sum_{i=1}^{n} \\|{\bf z}_ i {\bf T'}-{\bf x}_ i \\|^2= \sum_ {i=1}^{n}  \\| {\bf x}_ i {\bf T} {\bf T'} - {\bf x}_ i \\|^2 	
\end{align}	
고정된 $i$에 대하여 $\\| {\bf x}_ i {\bf T} {\bf T'} - {\bf x}_ i \\|^2 $을 풀면 ${\bf x}_ i{\bf x}_ i' -{\bf x}_ i {\bf T}{\bf T'} {\bf x}_ i'$가 되어서 위의식은 아래와 같이 정리된다. 	
\begin{align}	
\sum_{i=1}^{n}  \\| {\bf x}_ i {\bf T} {\bf T'} - {\bf x}_ i \\|^2 = -tr({\bf T'}{\bf X'}{\bf X}{\bf T})+tr({\bf X'}{\bf X})	
\end{align}	
잠깐 세부계산을 살펴보자. <br/><br/>	
1) $\\| {\bf x}_ i {\bf T} {\bf T'} - {\bf x}_ i \\|^2 $을 풀면 ${\bf x}_ i{\bf x}_ i' -{\bf x}_ i {\bf T}{\bf T'} {\bf x}_ i'$가 되는것은 ${\bf a}$가 row-vector일때 $\\|{\bf a}\\|^2={\bf a}{\bf a}'$임을 이용하면 쉽게 구할 수 있다. <br/><br/>	
2) $\sum_{i=1}^{n}{\bf x}_ i {\bf x}_ i'=tr({\bf X}{\bf X}')=tr({\bf X}'{\bf X})$가 된다. 여기에서 두번째 등호가 성립하는 이유는 $tr({\bf A}{\bf B})=tr({\bf B}{\bf A})$이기 때문이다. <br/><br/>	
3) 혹은 매트릭스쿡북 p6-(17) 공식 ${\bf a}'{\bf a}=tr({\bf a}{\bf a}')$를 이용하여 2)의 수식을 계산할 수도 있다. 	
매트릭스쿡북의 공식은 col-vector일때를 기준으로 쓴 것이므로 row-vector인 ${\bf x}_ i$의 경우는 $ {\bf x}_ i {\bf x}_ i'=tr({\bf x}_ i' {\bf x}_ i)$와 같이 적용할 수 있다. 따라서 	
$\sum_{i=1}^{n} {\bf x}_ i {\bf x}_ i' =\sum_{i=1}^{n}tr({\bf x}_ i' {\bf x}_ i)=tr(\sum_{i=1}^{n}{\bf x}_ i' {\bf x}_ i)=tr({\bf X}'{\bf X})$	
와 같이 된다.  <br/><br/>	
4) 위에서 언급한 ${\bf a}'{\bf a}=tr({\bf a}{\bf a}')$를 다시 이용하면 ${\bf x}_ i {\bf T}{\bf T'} {\bf x}_ i'=tr({\bf T'} {\bf x}_ i'{\bf x}_ i {\bf T})$가 된다. 따라서 	
$\sum_i^{n}{\bf x}_ i {\bf T}{\bf T'} {\bf x}_ i'=tr({\bf T}'\sum_i^n {\bf x}_ i'{\bf x}_ i  {\bf T})=tr({\bf T}'{\bf X}'{\bf X}{\bf T})$	
가 된다. <br/><br/>	

- 세부 계산때문에 다소 난잡해졌지만 PCA는 결국 ${\bf T}'{\bf T}={\bf I}$라는 제약조건하에서 아래식을 최소화하는 ${\bf T}$를 찾으면 된다. 	
\begin{align}	
-tr({\bf T'}{\bf X'}{\bf X}{\bf T})+tr({\bf X'}{\bf X})	
\end{align}	
여기에서 ${\bf X}$는 *given*되어 있으므로 ${\bf T}$에 대한 상수이다. 라그랑주 승수법을 사용하면 결국 PCA는 아래식을 최대화하는 $({\bf T},{\bf \Lambda})$를 찾으면 된다. 	
\begin{align}	
tr({\bf T'}{\bf X'}{\bf X}{\bf T})+ {\bf \Lambda} \left({\bf I}-{\bf T}'{\bf T}\right)	
\end{align}	
여기에서 ${\bf \Lambda}= diag(\lambda_1,\dots, \lambda_p)$이다. 위의 식을 ${\bf T}$로 미분하면 아래와 같이 된다. 	
\begin{align}	
2{\bf X}'{\bf X}{\bf T}- 2{\bf T}{\bf \Lambda}	
\end{align}	
또한 ${\bf \Lambda}$로 미분하면 ${\bf T}'{\bf T}={\bf I}$가 된다. 두 식을 연립하면 아래와 같이 된다. 	
\begin{align}	
\begin{cases}	
{\bf X}'{\bf X}{\bf T}={\bf T}{\bf \Lambda} \\\\ 	
{\bf T}'{\bf T}={\bf I}	
\end{cases}	
\end{align}	
즉 이걸 풀면 된다. 	

- 결론적으로 말하면 식	
\begin{align}	
\begin{cases}	
{\bf X}'{\bf X}{\bf T}={\bf T}{\bf \Lambda} \\\\ 	
{\bf T}'{\bf T}={\bf I}	
\end{cases}	
\end{align}	
을 만족하는 ${\bf T}_ {p\times p'}$는 ${\bf X}'{\bf X}$의 고유벡터 ${\bf V}_ {p\times p}$에서 **뒷부분에 위치한 덜 중요해보이는 $(p-p')$개의 고유벡터를 삭제한 것**과 같고, 위의 식을 만족하는 ${\bf \Lambda}$는 ${\bf X}'{\bf X}$의 고유벡터 ${\bf D}_ {p\times p}$에서 **뒷 부분에 위치한 덜 중요해보이는 $(p-p')$개의 고유치를 삭제한것**과 같다. 즉 	
\begin{align}	
\begin{cases}	
{\bf T}_ {p \times p'}=cbind({\bf V}_ 1, \dots, {\bf V}_ {p'})  \\\\ 	
{\bf V}_ {p \times p}=cbind({\bf T}_ {p \times p'}, {\bf 0}_ {p\times(p-p')})	
\end{cases}	
\end{align}	
이고	
\begin{align}	
\begin{cases}	
{\bf \Lambda}_ {p' \times p'}=diag(\lambda_ 1, \dots, \lambda_ {p'})  \\\\ 	
{\bf D}_ {p \times p}=diag(\lambda_ 1 , \dots , \lambda_ p)	
\end{cases}	
\end{align}	
이다. 증명은 그냥 ${\bf X}'{\bf X}{\bf T}={\bf T}{\bf \Lambda}$에다가 ${\bf T}_ {p \times p'}=cbind({\bf V}_ 1, \dots, {\bf V}_ {p'})$를 대입하면 쉽게 할 수 있다. (식 ${\bf T}'{\bf T}={\bf I}$는 고유벡터의 정의에 의해서 그냥 성립하므로 체크할 필요도 없다.) 	
\begin{align}	
{\bf X}'{\bf X}{\bf T}&={\bf X}'{\bf X}cbind({\bf V}_ 1, \dots, {\bf V}_ {p'}) \\\\ 
&=cbind({\bf X}'{\bf X}{\bf V}_ 1, \dots, {\bf X}'{\bf X}{\bf V}_ {p'}) = cbind({\bf V}_ 1 \lambda_ 1, \dots, {\bf V}_ {p'}\lambda_{p'}) \\\\ 	
&=cbind({\bf V}_ 1, \dots, {\bf V}_ {p'})diag(\lambda_1,\dots,\lambda_{p'})={\bf T} {\bf \Lambda}	
\end{align}


