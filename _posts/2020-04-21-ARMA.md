---
layout : post 
title : (강의) ARMA 
---

### About this doc 

- 이 문서에서는 ARMA모델에 대하여 다룬다. 

### Wold의 관찰 (교재 5.2.4.)
- Wold는 임의의 정상확률과정이 결정적성분과 비결정적 성분의 합으로 구분가능하다고 주장하였다. 즉 임의의 정상확률과정 $X_t$는 
\begin{align}
\begin{cases}
X_t = \eta_t + Z_t\\\\ \\
Z_t = \epsilon_t+\psi_{1}\epsilon_{t-1}+\psi_{2}\epsilon_{t-2}+\dots =\sum_{j=0}^{\infty}\psi_j \epsilon_{t-j}
\end{cases}
\end{align}
와 같이 표현할 수 있다. 

- 여기에서 $\eta_t$ predictable한 성분을 의한다. 구체적으로는 $x_{t-1},x_{t-2},\dots$이 주어졌을 경우 $\eta_t$의 값을 완벽히 예측할 수 있을때 $\eta_t$를 deterministic하다고 한다. $\eta_t$가 랜덤이 아니라는 의미가 아니라는 것에 유의하자. 

- 여기에서 $\psi_0,\dots,\psi_{\infty}$의 값은 유일하게 결정할 수 있다. 이때 $\psi_0=1$이다. 참고로 이 정리가 거의 사기적인데 사기적인 부분은 **유일하게** 결정할 수 있다는 부분이다. 

- 우리의 수준에서는 임의의 정상시계열 $\\{Z_t\\}$에 대하여 아래의 두 식을 만족하는 계수 $\psi_j$를 선택할 수 있다고 이해할 수 있다. 
\begin{align}
\begin{cases}
Z_t= \epsilon_t+\psi_{1}\epsilon_{t-1}+\psi_{2}\epsilon_{t-2}+\dots =\sum_{j=0}^{\infty}\psi_j \epsilon_{t-j}\\\\ \\
\sum_{j=0}^{\infty}\psi_j^2<\infty
\end{cases}
\end{align}

- 이 정리를 쉽게 생각하면 매우 쉽게 이해할 수 있다. $Z_t=0.5Z_{t-1}+\epsilon_t$에 대하여 
\begin{align}
Z_t=\epsilon_t+0.5\epsilon_{t-1}+0.5^2\epsilon_{t-2}+\dots 
\end{align}
꼴로 정리되는 것은 이미 여러번 보였는데 이것이 그 예라고 할 수 있다. 

- 여기에서 
\begin{align}
Z_t =\epsilon_t+\psi_{1}\epsilon_{t-1}+\psi_{2}\epsilon_{t-2}+\dots =\sum_{j=0}^{\infty}\psi_j \epsilon_{t-j}
\end{align}
의 꼴은 무한MA과정, 즉 MA($\infty$)로 부르기도 한다. 참고로 MA(1)과정은 
\begin{align}
Z_t =\epsilon_t-\theta_{1}\epsilon_{t-1}
\end{align}
으로 표현하며 MA(2)과정은 
\begin{align}
Z_t =\epsilon_t-\theta_{1}\epsilon_{t-1}-\theta_{2}\epsilon_{t-2}
\end{align}
와 같이 표현한다. 

### MA 

- MA과정을 좀 더 이론적으로 파고 들어보자. 

- AR(1) 모델: 
\begin{align}
Z_t=&\phi Z_{t-1}+\epsilon_t= \phi(\phi Z_{t-2}+\epsilon_{t-1})+\epsilon_t\\\\ \\
=&\phi^L Z_{t-L}+\sum_{h=0}^{L-1}\phi^h\epsilon_{t-h}
\end{align}
결국 이런식으로 반복하면 
\begin{align}
Z_t=\sum_{h=0}^{\infty}\phi^h\epsilon_{t-h}
\end{align}
와 같이 쓸 수 있을 것 같은데 엄밀하게 말하면 $\lim_{L\to\infty}\phi^{L}Z_{t-L}=0$ 이 성립해야 한다. 

- 만약에 $Z_{t-L}$가 (아무리 커도 어떠한 고정된) 상수라면 
$\|\phi\|<1$이라는 제약아래에서 
\begin{align}
\lim_{L\to\infty}\phi^{L}Z_{t-L}=0
\end{align}
이 성립하겠다. 고정된 상수가 아니고 $\|Z_{t-L}\|\leq M$임을 알아도 극한이 0임을 주장할 수 있다. 하지만 $Z_{t-L}$이 확률변수이므로 이러한식의 주장을 할 수 없다. 왜냐하면 $\phi_L$가 점점 작아질때마다 우연히 $Z_{t-L}$가 점점 $\phi_L$ 보다 큰 값이 뽑힐 수도 있기 때문이다. (확률변수니까~!) 결국 사실 그럴 확률은 거의 없지 않겠냐? 즉 $Z_{t-L}$가 큰 값이 연속적으로 계속 뽑힐 **확률**이 너무 적지 않겠냐? 라는 식의 논리전개를 해야한다. 따라서 **확률, 평균**적 개념을 넣어야 한다. 결론적으로 말하면 $L\to \infty$일때 아래와 같이 평균적인 개념을 통하여서만 $Z_t=\sum_{h=0}^{\infty}\phi^{h}\epsilon_{t-h}$ 임을 주장할 수 있다. 
\begin{align}
\lim_{L \to \infty} E\left(Z_t-\sum_{h=0}^{L}\phi^h \epsilon_{t-h}\right)^2=0 
\end{align}
참고로 위의 식은 성립한다. 따라서 
\begin{align}
Z_t \overset{L_2}{=} \sum_{h=0}^{\infty}\phi^{h}\epsilon_{t-h}
\end{align}
와 같이 표현할 수 있다. 이때 등호는 **mean square sense** 에서 성립한다고 표현한다. 그런데 일반적으로 간략하게는 
\begin{align}
Z_t = \sum_{h=0}^{\infty}\phi^{h}\epsilon_{t-h}
\end{align}
와 같이 표현한다. 

- AR(1) 모델인데 $\|\phi\|>1$인 경우는 어떻게 될까? 당연히 
\begin{align}
\lim_{L\to\infty}\phi^{L}Z_{t-L}
\end{align}
이 수렴하지 않기 때문에 
\begin{align}
Z_t = \sum_{h=0}^{\infty}\phi^{h}\epsilon_{t-h}
\end{align}
와 같이 표현할 수 없겠다. 그런데 만약에 아래와 같이 $Z_t$를 표현해보면 어떨까?
\begin{align}
Z_{t+1}=\phi Z_t +\epsilon_t\\\\ \\
Z_t=\frac{1}{\phi}Z_{t+1}+\frac{1}{\phi}\epsilon_t
\end{align}
그렇다면 
\begin{align}
Z_t=\frac{1}{\phi^L}Z_{t+L}-\sum_{h=1}^{L-1}\frac{1}{\phi^h}\epsilon_{t+h}
\end{align}
와 같이 표현된다. 일반적인 AR모델은 과거에서부터 순차적으로 미래로 확률변수들을 생성하는데, 이 모델은 미래에서 순차적으로 확률변수들을 생성한다고 생각하면 된다. 이런방식으로 무한한 과거상태로 간다면 이 모델은 정상상태에 있다고 주장할 수 있다. 

- 따라서 어떤사람들은 이 모델을 정상모델이라고 주장하기도 한다. 이러한 모델을 ***stationary future dependent AR(1) model*** 이라고 한다. 이 논리대로 치면 모든 AR(1) 모델은 $\phi=1$인 경우를 제외하고 정상모델이다. 그런데 사실상 $\phi>1$인 경우의 모델은 쓸모가 없는데 미래의 값으로 과거의 값을 예측하는게 의미가 없기 때문이다. 이렇게 미래에서 과거의 값을 예측하는 모델을 ***causal***하지 않다고 말한다. 따라서 *stationary future dependent AR(1) model*을 주장하는 사람들은 정상성외에 ***causality***를 추가적으로 가정한다. 결국 이 부류의 사람들도 관심이 있는 모델은 $\|\phi\|<1$인 경우이다. 

- 참고로 *stationary future dependent AR(1) model*을 인정하는 사람들은 울드의 정리도 아래와 같은 형태로 이해한다. 
\begin{align}
\begin{cases}
Z_t= \epsilon_t+\psi_{1}\epsilon_{t-1}+\psi_{2}\epsilon_{t-2}+\dots =\sum_{j=\-\infty}^{\infty}\psi_j \epsilon_{t-j}\\\\ \\
\sum_{j=-\infty}^{\infty}\|\psi_j\|<\infty
\end{cases}
\end{align}

- 아래와 같은 모델이 있다고 하자. AR(1)에 한정하여 $\|\phi\|>1$인 모델은 그에 대응하는 (그와 분포가 같은) *casaul* 모델로 바꿀 수 있다. 구체적으로 
\begin{align}
Z_t=\phi Z_{t-1}+\epsilon_t, \quad \epsilon_t \sim ~iid~N(0,\sigma^2)
\end{align}
를 만족하는 시계열 $\\{Z_t\\}$ 는 
\begin{align}
Y_t=\frac{1}{\phi} Y_{t-1}+\eta_t, \quad \eta_t \sim ~iid~ N\big(0,(\sigma/\phi)^2\big)
\end{align}
를 만족하는 시계열 $\\{Y_t\\}$와 **동등**하다. 즉 모든 유한개의 샘플에 대하여 분포가 같다. 이러한 개념은 쉽게 고차원으로 확장할 수 있다. 

- *stationary future dependent AR(1) model*의 개념을 받아들이면 편리한점이 있는데 바로 위와 같은 예제이다. 만약에 *stationary future dependent AR(1) model*을 비정상시계열로 받아들인다면 위의 예제에서 $\\{Z_t\\}$는 비정상시계열이고 $\\{Y_t\\}$는 정상시계열인데 두 시계열은 분포가 같다는 이상한 결론이 나온다. *stationary future dependent AR(1) model*을 받아들이면 이러한 개념적인 충돌을 피할 수 있다. 

- 사실 잘 생각해보면 $\phi>\|1\|$인 경우에도 분산구조는 lag에만 의존한다. (평균은 $0$ 이라 가정한다) 이는 정상시계열의 조건에 딱 맞음에도 불구하고 $\phi$의 절대값이 1보다 큰 시계열은 주구장창 비정상시계열이라고 주장했던 이유는 $t\to \infty$ 일수록 분산이 터지기 때문이다. 하지만 $t \to -\infty$ 이라면 분산이 터지지 않는다. 즉 **분산이 터진다** 라는 표현자체가 교묘한데 마치 무한개 더해서 분산이 터지는것처럼 착각을 하게 만들지만 실제로는 더하는 횟수가 무한임이 중요한게 아니고 더하는 방향이 중요한 것이었다. 만약에 미래에서부터 과거로 살아가는 사람이 $\phi>\|1\|$ 인 AR(1)모형을 관찰한다면 이는 점점 안정화되는 정상시계열의 모습을 가진다고 느낄 것이다. 

- 이제 MA모델로 넘어가 보자. 우선 $\\{Z_t\\}$가 아래와 같이 표현되는 모델을 MA(q)라고 한다. 
\begin{align}
Z_t=\epsilon_t + \theta_1 \epsilon_{t-1}+\dots+\theta_q\epsilon_{t-q}
\end{align}
교재에 따라서 아래와 같이 정의하기도 한다. 
\begin{align}
Z_t=\epsilon_t - \theta_1 \epsilon_{t-1}-\dots-\theta_q\epsilon_{t-q}
\end{align}
울드의 정리에 따르면 이것은 항상 정상과정시계열이다. 

- 



