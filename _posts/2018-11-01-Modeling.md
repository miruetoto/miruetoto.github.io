---
layout: post
title: (정리) 모델링 
---
### About this post 
- 본 포스트는 모델링에 대하여 다룬다. 

- 우리가 풀고 싶은 문제는
\begin{align}
y_i=f(x_i)+\epsilon_i
\end{align}
로 표현했을때 어색하지 않은 underlying function $f(x)$를 찾아내는 것이다. 

--- 

#### Basis 선택 

- 가장 기본적인 모델인 1차원 회귀식부터 생각해보자.  
\begin{align}
y_i=f(x_i)+\epsilon_i, ~~~ f(x_i)=\beta_0+x_i\beta_1. 
\end{align}
이런 모델은 2개의 basis를 가진다. 이러한 모델을 파라메트릭 모델이라고 한다. 

- 위의 모델을 좀 더 개신시켜 basis를 $[1,\cos x, \sin x, \cos 2x, \sin 2x, \dots, ]$와 같이 확장해볼 수도 있을 것이다. 이때 각각의 basis는 오소고날하다. 

- 위의 2가지 방법은 각각 어떠한 가정을 내포한다. 가령 
''$f(x)$는 $x$에 선형변환으로 만들어질 수 있다. (즉 $f(x)=\beta_0+\beta_1x$)'' 
라든가 하는식으로 말이다. 이말은 
\begin{align}
y_i=f(x_i)+\epsilon, 
\end{align}
와 같은 모델에서 $f$가 어떠한 형태를 가질것인지 미리 알고 있다고 생각한다는 말과 같다. 이처럼 $f$가 어떤 모양인지 미리 알고 접근하는 방법을 파라메트릭 모델이라고 한다. 그리고 보통 $f$의 모양을 결정하는 과정(즉 적절한 basis를 선택하는 과정)을 모델링이라고 한다. 

- $f$를 어떻게 모델링할지 감이 안 올 수도 있다. 즉 자료를 봤는데 선형의 모양을 가지는지 어떤지 감을 못잡겠는 경우이다. 이것을 바꾸어 말하면 $\\{y_i\\}$가 $\\{x_i\\}$의 어떤 space에 있는지 감을 못 잡겠다는 뜻이다. 혹은 모델링이 귀찮을 수도 있다. 이럴 경우 $f(x)$가 $x$의 어떤 특정스페이스 $\cal A$의 부분공간에 존재한다고 가정하고 그 특정스페이스 $\cal A$를 expansion할 수 있는 베이시스를 선택하여 문제를 풀 수 있다. 가령 예를들면
''$f(x)$가 어떤 공간에 있는지 모르겠는데 최소한 비숍스페이스의 부분공간에 있는것 같아'' 
라고 생각한다면 웨이블릿 베이시스를 선택하여 모델링 하는 것이다. 

- 보통위와 같은 접근법은 무한대의 basis를 활용한다. 많은 수학자들이 
"이런식으로 무한개의 basis를 활용하면 특정공간에 있는 어떠한 함수도 표현할 수 있어요~"
라는 식의 증명을 많이 해놓았는데 이러한 증명결과들을 적극적으로 활용하는 셈이다. 요렇게 $f$를 표현하는데 무한개의 basis를 활용하는 모델링을 semi-parametric이라고 한다. 

- 커널모델을 사용하여 자료를 표현하는 경우는 어떤가? 커널모델은 $f(x)$를 아래와 같이 가정하는 것으로 이해할 수 있다. 
\begin{align}
f(x)=\theta_1K_h(x,x_i)+\dots+\theta_n K_h(x,x_n)
\end{align}
특이한것은 $f(x)$를 설계할때 관측자료 $x_i$를 사용하였다는 점이다. 퓨리에 변환이 $f(x)$를 가정하는 방식
\begin{align}
f(x)=\sum_{k =0}^{\infty}\theta_k e^{i\omega kx}
\end{align}
과 비교하여 보면 퓨리에변환은 $f(x)$를 표현하는데 무한대에 가까운 숫자의 basis를 사용하였지만 입력자료 $x_i$를 basis에 활용하지는 않았다. 이처럼 입력값 $x_i$를 직접 basis를 설계하는데 활용하는 방식을 non-parametric이라고 한다. 

--- 
### Basis 확장

- 지금까지 살펴본 것은 모두 입력변수가 1차원일 경우에 basis를 선택하는 방법이었다. 즉 입력변수의 차원이 $p$라면, 즉 ${\bf x}_ i=cbind(x_ {i1},\dots,x_ {ip})$라면 어떻게 해야하는가? 이와 같은 경우 기저를 어떻게 잡아야 할까? 보통 이런 경우는 **(1)** 일차원 기저를 적당히 확장하여 고차원 기저를 만드는 경우가 있고 **(2)** 커널처럼 자료를 직접활용하여 기저를 만드는 방법이 있다. (1)과 같은 방법은 파라메트릭의 정신을 계승하는 것이고 (semi-parametric 포함) (2)와 같은 방법은 넌파라메트릭의 정신을 계승하는 것이다. 

- 이중에서 (1)의 방법을 살펴보자. (1)의 방법을 주장하는 사람은 크게 다시 2부류로 나누어진다. **(a)** 그 중 하나의 부류는 **멀티플리케이티브 모델**(multiplicative model)을 지지하는 부류이다. 이 사람들은 고차원베이시스를 1차원베이시스들의 product로 표현할 수 있다는 관점이다. 쉽게 말하면 교호작용을 고려하는 모델로 볼 수 있다. 예를들어 입력변수가 4차원이고, 즉 ${\bf x}_ i=cbind(x_{i1},x_{i2},x_{i3},x{i4})$이고 각 차원을 모델링하는데 각각 2개의 파라메터를 쓰고 있다고 생각하자. (additive model) 고차원베이시스를 1차원베이시스의 합으로 표현할 수 있다는 관점임. 
  
- 이때 multiplicative model이 골때린다. 이 모델은 쉽게 말하면 교호작용을 고려하는 모델로 볼 수있다. 예를들어 입력변수 ${\bf x}$가 4차원이고 각 차원을 2개의 파라메터(예를들면 $[1,x]$)로 표현하고 있으면 전체 필요한 파라메터 수는 $2^4$이다. 만약에 입력변수의 차원이 $10$이라고 하면 각 변수당 2개의 파라메터를 쓸때 $2^{10}=1024$개의 파라메터가 필요하다. 한마디로 망한 모델이라는 소리이다. 이처럼 입력변수의 차원에 대하여 파라메터의 수가 지수적으로 증가하는 현상을 차원의 저주라고 부른다. 그래서 일반적으로 Multiplicative model보다 Additive model을 고려하는 것이 좋다. 

- 이제 (2)의 방법은 살펴보자. 이처럼 일반적으로 입력변수가 고차원일때에는 커널과 같은 넌파라메트릭 모델을 고려하는것이 차원의 저주를 피하기에 유리하다. 커널과 같은 경우를 예로 들어보자. $\\{ {\bf x} \\}_ {i=1}^{n}$를 관측하였다면, 
\begin{align}
f({\bf x})=\sum_{i=1}^{n} \theta_i K_h({\bf x},{\bf x}_ i)
\end{align}
와 같이 쓸 수 있다. 이때 
\begin{align}
{\bf x}_ i=[x11_i,x12_i,x21_i,x22_i]
\end{align}
이다. 입력변수의 차원에 상관없이 $n$개의 basis만 가지고서 $f({\bf x})$를 표현 할 수 있다. 

- 커널방법의 또 다른 장점은 $K({\bf x},{\bf x}_ i)$만 잘정의하면 어떠한 입력형태 $\bf x$에서도 동작한다는 것이다. 예를들면 입력 $\bf x$가 문자열, 트리, 그래프등인 경우에도 동작한다. 

- 지금까지 살펴본 모델은 모두 선형모델이다. 선형모델이라는 뜻은 $f(x)$를 아래와 같은 방식으로 표현한다는 의미이다. 
\begin{align}
f(x)=\sum coef \times basis 
\end{align}
여기에서 coef는 parameter로 이해해도 된다. 왜 우리는 선형모델을 가정할까? 선형모델을 가정하면 최소제곱법과 같은 방법을 활용하여 parameter를 매우 쉬운 연산만으로 구할 수 있다는 장점이 있다. 앞에서 살펴본 커널의 경우도 밴드윗을 $h$로 고정하고 중심도 $x_i$로 고정된 상태에서 $\theta_i$를 구하는것은 선형모델이다. (하지만 만약에 커널모델에서 $h$와 $\theta_i$를 동시에 구해야 한다면 이것은 비선형 방정식을 풀어야한다.)  

---

### 선형모델에서 Basis의 Coef를 구하기 
- 회귀분석류의 대부분의 방식은 본질적으로 2가지 단계로 귀결된다. **(1)** 모델링을 한다. 즉 적당한 basis를 선택한다. **(2)** 각 basis의 coef를 구한다. 즉 적합을 시킨다. 
  
- (1) 의 과정이 끝나면 즉 모델링이 끝나면 아래와 같이 자료가 표현될것이다. 
\begin{align}
y_i=f(x_i)+\epsilon_i=\sum coef \times basis +\epsilon_i= \sum_{j=1}^{p} \theta_j B_j(x_i)+\epsilon_i
\end{align}
여기에서 $B_j(x)$는 원래 데이터를 가지고 만든 어떠한 basis이다. 그리고 $\theta_j$는 그러한 basis에 해당하는 coef이다. 귀찮으니까 그냥 앞으로 $B_j(x)=X[,j]$라고 생각하고 $\theta_j=\beta_j$라고 생각하자. 그러면 모든 (선형)모델은 아래와 같이 쓸 수 있다. 
\begin{align}
{\bf y}={\bf X\beta} +{\bf \epsilon}
\end{align}

- 이제 남은것은 (2) 의 과정 즉 coef $\bf \beta$를 구하는 것이다. 보통 LS방법으로 구하면 
\begin{align}
\bf \hat{\beta}=(X'X)^{-1}X'y
\end{align}
와 같이 된다. 

- 가끔 가다가 오차항의 분산이 일정하지 않을수도있다. 예를들어서 $i \in \\{1,\dots,500\\}$에서는 분산이 $1$인 정규분포를 따르고 $i \in \\{501,\dots,1000\\}$에서는 분산이 $2$인 정규분포를 따른다고 하자. 이 경우에도 
\begin{align}
\sum_{i=1}^{1000}(y_i-\hat{y}_ i)^2
\end{align}
을 최소화하는 아이디어는 좀 곤란하다. 기본적으로 $i=1,\dots,500$에서 $E(y_i-\hat{y}_ i)^2$와 $i=501,\dots,1000$에서 $E(y_i-\hat{y}_ i)^2$의 값은 다르기 때문이다. 뒤에 값이 앞의 값보다 2배정도 크기 때문에 뒤의 값의 loss를 우선적으로 줄이는 쪽으로 LSE가 구해질 가능성이 높다. 따라서 $i=501,\dots,1000$에 해당하는 loss를 구할때는 $\frac{1}{2}$씩 곱해보는 것이 더 현명할 수 있다. 이것이 WLS의 핵심아이디어이다. 

- 좀 더 살펴보자. 위의 예제에서 오차항의 분산은 $V({\bf \epsilon})=diag(1,\dots,1,2,\dots,2)$와 같은 형태가 된다. 적당한 행렬 ${\bf P}=diag(1,\dots,1,\frac{1}{\sqrt{2}},\dots,\frac{1}{\sqrt{2}})$을 가져와서 $\bf y=X \beta +\epsilon$의 양변에 곱하면 
\begin{align}
\bf Py=PX\beta+P\epsilon
\end{align}
이 된다. 그럼 이때 $V({\bf P \epsilon})=I$ 이다. 이제 분산이 똑같아 졌으니까 LSE를 써도 괜찮을 것 같다. LSE를 쓰면 
\begin{align}
\bf \hat{\beta}=(X'P'PX)^{-1}X'P'Py  
\end{align}
가 된다. ${\bf P'P}={\bf P^2}=diag(1,\dots,1,\frac{1}{2},\dots,\frac{1}{2})$가 된다. 이는 결국 loss함수를 
\begin{align}
\sum_{i=1}^{500} (y_i-\hat{y}_ i)^2+ \frac{1}{2} \sum_{i=501}^{1000} (y_i-\hat{y}_ i)^2
\end{align}
와 같이 설정하는 것과 동일한 효과를 준다. 

- 좀 더 일반적으로 $V({\bf \epsilon})={\bf \Sigma}$인 경우를 살펴보아도 $\bf y=X\beta + \epsilon$의 양변에 ${\bf \Sigma}^{-\frac{1}{2}}$를 곱해주면 문제가 간단하다. 문제는 ${\bf \Sigma}^{-\frac{1}{2}}$이 존재하느냐는 것이다. 이것은 잘 생각해보면 당연하다는 것을 알 수 있다. 우선 $\Sigma$는 *(real, symm,) positive definite*이다. 이러한 행렬을 편의상 *(real-symm)-pd* 행렬이라고 하자. 아래와 같은 사실이 있다.
<br/><br/>
***모든 (real-symm)-pd 행렬 $A_ {n \times n}$은 고유치가 모두 양수이고 고유벡터가 서로 정규직교하도록 분해할 수 있다. (그리고 역도 성립한다.)***<br/><br/>
이걸 잘 이용하면 1) $\Sigma$의 역행렬 $\Sigma^{-1}$이 존재한다. 2) $\Sigma^{-1}$역시 real-symm-pd이다. 3) $\Sigma^{-1}=\Sigma^{-\frac{1}{2}}\Sigma^{-\frac{1}{2}}$를 만족하는 $\Sigma^{-\frac{1}{2}}$이 존재함을 순차적으로 (매우쉽게) 보일 수 있다. 

- 즉 $\epsilon$의 분산구조가 어떠한 형태를 가지든 간에 그것이 *(real-symm)-pd*이기만 하면 위와 같은 방법으로 LSE를 구할 수 있다는 것을 의미한다.  이때 *(real-symm)*은 분산의 정의상 무조건 성립하는 것이고 가끔씩 $\epsilon$의 공분산행렬이 *pd*가 아니고 *semi_pd*가 되는 경우가 가끔 있는데 ($\epsilon$이 랜덤변수가 아니고 상수라든가 하는 경우) 이러한 경우만 조심하면 된다. 통계학에서는 그냥 분산이 *semi_pd*인 경우는 없다고 생각하는게 정신건강에 좋다. 

- 만약에 $\bf (X'X)^{-1}$가 존재하지 않는다면 어떻게 되는가? 본질적으로 이런 경우는 $\bf X$의 랭크가 $p$보다 작은 경우, 그래서 결국 $\bf X'X$의 랭크가 $p$보다 작게되어 역행렬을 못구하는 경우에 생긴다. 이러한 일은 1) $n<p$ 이거나 2) $\bf X$중에서 선형적으로 종속된 변수들이 있을 경우 자주 발생한다. 
