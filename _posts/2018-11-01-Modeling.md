---
layout: post
title: (정리) 모델링 혹은 모형선택
---

### About this post 

- 본 포스트에서는 다양한 모델링에 대하여 소개한다. 본 포스팅의 내용은 내 주관이 상당히 많이 들어간 내용이며 특정 교재의 내용을 바탕으로 재구성한것은 아니다. 

- (나를 포함하여) 많은 사람들이 선형모델/비선형모델, 파라메트릭/세미파라메트릭/넌파라메트릭, 어디티브/멀티플리케이티브 모델 등과 같은 용어들을 엄밀하게 구분하지 못하는데 이것에 대하여 조금 정리하고 싶어 포스팅을 시작했다. 

--- 

### 파라메트릭 / 세미파라메트릭 / 넌파라메트릭 

- 모델링이란 
\begin{align}
y_i=f(x_i)+\epsilon_i
\end{align}
의 꼴에서 $f$의 모양을 결정하는 과정을 의미한다. 

- $f$의 모양을 결정할때 데이터에 대한 확실한 사전정보가 있는 경우가 있다. 예를들어 
''$f(x)$는 $x$에 선형변환으로 만들어질 수 있다. (즉 $f(x)=\beta_0+\beta_1x$)'' 
라는 사실을 알고 있는 경우이다. 이는 
\begin{align}
y_i=f(x_i)+\epsilon_i, 
\end{align}
와 같은 모델에서 $f$가 어떠한 형태를 가질것인지 미리 알고 있다고 생각한다는 말과 같다. 이처럼 $f$가 어떤 모양인지 미리 알고 접근하는 방법을 **파라메트릭 모델링** 이라고 한다. 

- 사전정보가 없어서 $f$를 어떻게 모델링할지 감이 안 올 수도 있다. 즉 자료를 봤는데 선형의 모양을 가지는지 어떤지 감을 못잡겠는 경우이다. 이것을 바꾸어 말하면 $\\{y_i\\}$가 $\\{x_i\\}$의 어떤 ***space*** 에 있는지 감을 못 잡겠다는 뜻이다. 혹은 모델링이 귀찮을 수도 있다. 이럴 경우 $f(x)$가 $x$의 어떤 특정스페이스 $\cal A$의 부분공간에 존재한다고 가정하고 그 특정스페이스 $\cal A$를 할 수 있는 베이시스를 선택하여 문제를 풀 수 있다. 가령 예를들면
''$f(x)$가 어떤 공간에 있는지 모르겠는데 최소한 비숍스페이스의 부분공간에 있는것 같아'' 
라고 생각한다면 웨이블릿 베이시스를 선택하여 모델링 하는 것이다. 보통 위와 같은 접근법은 무한대의 basis 를 활용한다. 많은 수학자들이 
"이런식으로 무한개의 basis를 활용하면 특정공간에 있는 어떠한 함수도 표현할 수 있어요~"
라는 식의 증명을 많이 해놓았는데 이러한 증명결과들을 적극적으로 활용하는 셈이다. 요렇게 $f$를 표현하는데 무한개의 basis를 활용하는 모델링을 ***semi-parametric modeling*** 이라고 한다. 

- 웨이블릿과 퓨리에변환등으로 $f(x)$를 추론하는 것이 대표적인 세미파라메트릭 모델링이다. 

- 파라메트릭 모델링도 못하겠고 세미파라메트릭 모델링도 못하겠다면 넌파라메트릭 모델링을 할 수 있다. 넌파라메트릭 모델링은 $f(x)$ 에 대한 어떠한 가정도 필요하지 않다. 예를들면 "어떠한 식으로 표현가능하다" 라든가 (요건 파라메트릭 스타일) 혹은 "최소한 어떠한 공간안에 있는것 같다" 라든가 (요건 세미파라메트릭 스타일) 하는 식의 가정이 필요하지 않다. 

- **넌파라메트릭*(non-parametric)*** 은 통계학에서 대충 2개의 의미로 쓰인다. 보통 (1) 자료가 특정한 분포에서 나왔다는 가정이 필요없는 경우 (2) 자료가 특정한 모델 혹은 스트럭처에서 생성된다는 가정이 없는 경우를 의미한다(위키피디아 참고). (1) 과 관련된 용어로는 ***non-prametric statistics***, ***distribution-free***, ***rank***, ***order-statistics*** 등이 있다. (2) 와 관련된 용어는 ***non-prametric regression***, ***non-parametric hierarchical Bayesian models***, ***kernel*** 등이 있다. 여기에서는 모델링과 관련된 내용을 다루므로 (2)의 경우로 한정하자. 

- 넌파라메트릭은 베이시스를 설계할때 입력표본 $x_i$을 사용한다. 따라서 ***given data*** 에 따라 ***basis*** 가 달라진다. 이러한 특징을 ***data adaptive*** 하다 라고 표현한다. 아래는 넌파라메트릭 모델링의 예이다. 
\begin{align}
f(x)=e^{-\frac{\\| x-x_1\\|}{2h^2}}\beta_1+e^{-\frac{\\| x-x_2\\|}{2h^2}} \beta_2+\dots+e^{-\frac{\\| x-x_n\\|}{2h^2} }\beta_n
\end{align}
위의 식을 보면 입력데이터를 그대로 쓰지 않고 **커널(*kernel)*** 을 한번 씌웠는데 이렇게 입력표본에 커널을 씌우는 방식을 커널모델이라고 한다. 

- 넌파라메트릭 방법중에 커널을 쓰는 방법이 매우 많다. 하지만 모든 넌파라메트릭 방법이 반드시 커널을 써야하는 것은 아니다. 예를 들어 ***simple kriging*** 같은 경우는 커널을 쓰지 않는 넌파라메트릭 방법으로 볼 수 있다. 

- 참고로 넌파라메트릭 방법에도 모수가 있다. 구체적으로 위의식에서 $\beta_j$ 가 모수에 해당한다. 넌파라메트릭 방법은 모수가 있냐 없냐가 중요한게 아니라 ***basis*** 가 ***given data*** 의 함수이냐 아니냐가 중요하다. 따라서 내생각에는 넌파라메트릭 모델 보다 ***data-adaptive model*** 이 더 좋은 표현 인것 같다. 즉 다시말하면 파라메트릭 모델 혹은 세미파라메트릭 모델은 $f$의 **스트럭처*(structure)*** 가 모수 $\theta$ 만의 함수이다. 따라서 $y_i=f(x_i)+\epsilon_i)$ 의 꼴에서 $f$ 를 
\begin{align}
f_{\theta}
\end{align}
와 같은 형태로 표현가능하다. 하지만 넌파라메트릭 모델은 $f$의 스트럭처가 모수와 $x_i$에 모두 의존한다. 

- 실제로 아까 살펴본 커널모델을 다시 살펴보자. 커널을 결정하는 것은 사실 (1) 밴드윗 (2) 중심점 (3) 높이 인데 여기에서 
\begin{align}
f(x)=e^{-\frac{\\| x-x_1\\|}{2h^2}}\beta_1+e^{-\frac{\\| x-x_2\\|}{2h^2}} \beta_2+\dots+e^{-\frac{\\| x-x_n\\|}{2h^2} }\beta_n
\end{align}
아까의 모델이 넌파라메트릭인 이유는 (2) 중심점을 표현하는 파라메터가 데이터 자체이기 때문이다. 

### 선형모델 / 비선형모델 

- 모델링이란 
\begin{align}
y_i=f(x_i)+\epsilon_i
\end{align}
의 꼴에서 $f$의 모양을 결정하는 과정을 의미한다. 이때 $f$는 베이시스와 파라메터로 구성된다. 그런데 이때 $f$가 아래와 같이 
\begin{align}
f = \sum basis \times coef
\end{align}
꼴의 형태로 표현할 수 있다면 이 모델을 선형모델이라고 하고 그렇지 않으면 비선형모델이라고 한다. 

- 일반적으로 커널은 ***bandwidth*** 와 ***height*** 에 따라서 결정된다. 예를들어 
\begin{align}
f(x)=e^{-\frac{\\| x-x_1\\|}{2h^2}}\beta_1+e^{-\frac{\\| x-x_2\\|}{2h^2}} \beta_2+\dots+e^{-\frac{\\| x-x_n\\|}{2h^2} }\beta_n
\end{align}
와 같은 모델에서는 밴드윗이 $h$ 이고 높이가 $\beta_i$ 라고 이해할 수 있다. 만약에 $h$ 가 ***known*** 이라 $\beta_i$ 만을 추정해도 된다고 하자. 이 경우는 베이시스와 코이피션트가 선형으로 결합된 형태가 되므로 이 경우에는 커널모델을 선형모델로 해석할 수 있다. 하지만 $h$ 역시 우리가 추정해야 한다면 이 모델은 비선형모델이 된다. 

- 선형모델이 아니지만 적당한 변환을 통하여 선형모델로 바꿀수 있는 모델 역시 넓은범위에서는 선형모델로 친다. 이런 모델을 ***generalized linear model*** 이라고 한다. 로지스틱이나 로그선형모델등이 이러한 범주에 속한다. 

- 퓨리에변환과 웨이블릿 변환도 선형모델이다. 

- 딥러닝 모델의 뼈대가 되는 신경망은 파라메트릭 이면서 비선형 모델이다. 참고로 이 모델은 



- 많은 사람들이 $f(x)$가 $x$에 대하여 선형이면 선형모델이라고 생각하는데 이건 사실이 아니다. 이것은 $f$가 $x$에 대하여 비선형이라도 파라메터 $\beta$에 대하여 선형이라면 선형모델이라고 한다. 중학교 1학년때 배우는 방정식을 보면 기준을 $x$로 보느냐 $y$로 보느냐에 따라 1차식이 되기도 하고 2차식이 되기도 하는 수식을 보았을 것이다. 예를 들어보자. 
\begin{align}
x^2 y= 0
\end{align} 
은 이 1차식인가 2차식인가? 이것은 $x$에 대한 2차식이지만 $y$에 대한 1차식이다. 이와 유사하게 
\begin{align}
f(x)=\beta_0+\beta_1 x + \beta_2 x^2
\end{align}
은 $x$에 대하여는 비선형이지만 $\beta=(\beta_0,\beta_1,\beta_2)'$에 대해서는 선형이다. 

- 선형모델이 반드시 파라메트릭 모델일 필요는 없다. 회귀분석, ploynomial regression, ANOVA, 로지스틱, 로그선형모델 등 GLM류로 정리되는 모델드은 선형모델이면서 파라메트릭모델이다. 또한 퓨리에변환/웨이블릿 변환은 선형모델이지만 세미파라메트릭 모델이다. 

### 파라메트릭 모델링에서 Basis 확장

- 지금까지 살펴본 것은 모두 입력변수가 1차원일 경우에 basis를 선택하는 방법이었다. 즉 입력변수의 차원이 $p$라면, 즉 ${\bf x}_ i=cbind(x_ {i1},\dots,x_ {ip})$라면 어떻게 해야하는가? 이와 같은 경우 기저를 어떻게 잡아야 할까? 보통 이런 경우는 **(1)** 일차원 기저를 적당히 확장하여 고차원 기저를 만드는 경우가 있고 **(2)** 커널처럼 자료를 직접활용하여 기저를 만드는 방법이 있다. (1)과 같은 방법은 파라메트릭의 정신을 계승하는 것이고 (semi-parametric 포함) (2)와 같은 방법은 넌파라메트릭의 정신을 계승하는 것이다. 

- 이중에서 (1)의 방법을 살펴보자. (1)의 방법을 주장하는 사람은 크게 다시 2부류로 나누어진다. **(a)** 그 중 하나의 부류는 **멀티플리케이티브 모델**(multiplicative model)을 지지하는 부류이다. 이 사람들은 고차원베이시스를 1차원베이시스들의 product로 표현할 수 있다는 관점이다. 쉽게 말하면 교호작용을 고려하는 모델로 볼 수 있다. 예를들어 입력변수가 4차원이고, 즉 ${\bf x}_ i=cbind(x_{i1},x_{i2},x_{i3},x{i4})$이고 각 차원을 모델링하는데 각각 2개의 파라메터를 쓰고 있다고 생각하자. (additive model) 고차원베이시스를 1차원베이시스의 합으로 표현할 수 있다는 관점임. 
  
- 이때 multiplicative model이 골때린다. 이 모델은 쉽게 말하면 교호작용을 고려하는 모델로 볼 수있다. 예를들어 입력변수 ${\bf x}$가 4차원이고 각 차원을 2개의 파라메터(예를들면 $[1,x]$)로 표현하고 있으면 전체 필요한 파라메터 수는 $2^4$이다. 만약에 입력변수의 차원이 $10$이라고 하면 각 변수당 2개의 파라메터를 쓸때 $2^{10}=1024$개의 파라메터가 필요하다. 한마디로 망한 모델이라는 소리이다. 이처럼 입력변수의 차원에 대하여 파라메터의 수가 지수적으로 증가하는 현상을 차원의 저주라고 부른다. 그래서 일반적으로 Multiplicative model보다 Additive model을 고려하는 것이 좋다. 

- 이제 (2)의 방법은 살펴보자. 이처럼 일반적으로 입력변수가 고차원일때에는 커널과 같은 넌파라메트릭 모델을 고려하는것이 차원의 저주를 피하기에 유리하다. 커널과 같은 경우를 예로 들어보자. $\\{ {\bf x} \\}_ {i=1}^{n}$를 관측하였다면, 
\begin{align}
f({\bf x})=\sum_{i=1}^{n} \theta_i K_h({\bf x},{\bf x}_ i)
\end{align}
와 같이 쓸 수 있다. 이때 
\begin{align}
{\bf x}_ i=[x11_i,x12_i,x21_i,x22_i]
\end{align}
이다. 입력변수의 차원에 상관없이 $n$개의 basis만 가지고서 $f({\bf x})$를 표현 할 수 있다. 

- 커널방법의 또 다른 장점은 $K({\bf x},{\bf x}_ i)$만 잘정의하면 어떠한 입력형태 $\bf x$에서도 동작한다는 것이다. 예를들면 입력 $\bf x$가 문자열, 트리, 그래프등인 경우에도 동작한다. 

- 지금까지 살펴본 모델은 모두 선형모델이다. 선형모델이라는 뜻은 $f(x)$를 아래와 같은 방식으로 표현한다는 의미이다. 
\begin{align}
f(x)=\sum coef \times basis 
\end{align}
여기에서 coef는 parameter로 이해해도 된다. 왜 우리는 선형모델을 가정할까? 선형모델을 가정하면 최소제곱법과 같은 방법을 활용하여 parameter를 매우 쉬운 연산만으로 구할 수 있다는 장점이 있다. 앞에서 살펴본 커널의 경우도 밴드윗을 $h$로 고정하고 중심도 $x_i$로 고정된 상태에서 $\theta_i$를 구하는것은 선형모델이다. (하지만 만약에 커널모델에서 $h$와 $\theta_i$를 동시에 구해야 한다면 이것은 비선형 방정식을 풀어야한다.)  



