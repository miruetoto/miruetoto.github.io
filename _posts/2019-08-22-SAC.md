---
layout: post
title: (리뷰) SAC 
--- 

### About this doc

- 이 포스팅은 아래 논문의 리뷰이다. <br/>
Haarnoja, T., Zhou, A., Hartikainen, K., Tucker, G., Ha, S., Tan, J., ... & Levine, S. (2018). Soft actor-critic algorithms and applications. arXiv preprint arXiv:1812.05905.

### Preliminaries

- MDP 는 $({\cal S},{\cal A},p,r)$ 이고 state space $\cal S$와 action space $\cal A$는 모두 연속이다. $p$는 state transition probability 이며 현재상태 ${\bf s}_ t \in {\cal S}$ 에서 특정한 액션 ${\bf a}_ t \in {\cal A}$ 를 취해서 다음상태 ${\bf s}_ {t+1} \in {\cal S}$ 로 갈 probability density를 의미한다. 따라서 
\begin{align}
p:{\cal S} \times {\cal S} \times {\cal A} \to [0,\infty) 
\end{align}
이다. 환경은 현재상태 ${\bf s}_ t $ 와 액션 ${\bf a}_ t$에 따라서 보상 $r:{\cal S}\times{\cal A} \to [r_{min}, r_{max}]$ 를 주게 된다. 

- 여기에서 폴리쉬 $\pi({\bf a}_ t | {\bf s}_ t)$ 가 기븐일 경우 아래와 같은 확률을 정의할 수 있는데
\begin{align}
p({\bf s}_ t \| \pi), \quad p({\bf s}_ t,{\bf a}_ t \| \pi) 
\end{align}
이 논문에서는 이를 각각 $\rho_{\pi}({\bf s}_ t)$ 와 $\rho_{\pi}({\bf s}_ t, {\bf a}_ t)$ 로 표시한다. 그리고 이것들을 각각 아래와 같이 부른다. <br/>
$\cdot ~ \rho_{\pi}({\bf s}_ t)$: *state* marginals of the trajectory distribution induced by a polish $\pi({\bf a}_ t | {\bf s}_ t)$ <br/>
$\cdot ~ \rho_{\pi}({\bf s}_ t, {\bf a}_ t)$: *state-action* marginals of the trajectory distribution induced by a polish $\pi({\bf a}_ t | {\bf s}_ t)$

- 통상적으로 우리의 목표는 아래값을 최대로 하는 적절한 $\pi({\bf a}_ t | {\bf s}_ t)$를 학습하는 것이었다.
\begin{align}
\sum_{t} \mathbb{E}_ {({\bf s}_ t, {\bf a}_ t) \sim \rho_\pi} \big[r({\bf s}_ t, {\bf a}_ t) \big] 
\end{align}

- 논문에서는 위의 오브젝티브 함수에 엔트로피개념의 패널티텀을 추가하여 아래와 같은 오브젝트 함수를 최대화 하는 문제를 푸는것에 관심을 가진다. 
\begin{align}
\sum_{t} \mathbb{E}_ {({\bf s}_ t, {\bf a}_ t) \sim \rho_\pi} \big[r({\bf s}_ t, {\bf a}_ t) 
+\alpha {\cal H}\big(\pi(\cdot \| {\bf s}_ t) \big) \big] 
\end{align}
여기에서 ${\cal H}(\pi(\cdot \| {\bf s}_ t))$ 는 분포 $\pi(\cdot \| {\bf s}_ t)$의 정보량이다. 

- (잠깐복습) 원래 큐펑션에 대한 벨만이퀘이션은 아래와 같다. (셔튼교재 p.83) 
\begin{align}
q_{\pi}(s,a)=&\sum_{s',r}p(s',r \| s,a)\big[r+\gamma v_{\pi}(s') \big] \\\\ \\
=&\sum_{r} \sum_{s'} p(s',r \| s,a) \big[r+\gamma v_{\pi}(s') \big] \\\\ 
=&\sum_{r} r \sum_{s'} p(s',r \| s,a) + \sum_{r} \sum_{s'}\gamma v_{\pi}(s') p(s',r \| s,a) \\\\ 
=&r(s,a) + \sum_{s'}\gamma v_{\pi}(s') p(s' \| s,a) \\\\ 
=&r(s,a) + \mathbb{E}_ {s' \sim p} \gamma v_{\pi}(s') \\\\ 
\end{align}
여기에서 $v_{\pi}(s')$ 는 밸류펑션이며 아래의 식을 만족하는 값으로 정의된다. 
\begin{align}
v_{\pi}(s)=&\sum_{a}\pi(a|s)\sum_{s',r}p(s',r|s,a)\Big[r+\gamma v_{\pi}(s')\Big] \\\\ \\ 
=&\sum_{a}\pi(a|s)q_{\pi}(s,a) \\\\ \\ 
=&\mathbb{E}_ {a \sim \pi}q_{\pi}(s,a)
\end{align}
따라서 위의 식을 연립하여 정리하면 
\begin{align}
q_{\pi}(s,a)=r(s,a) +  \gamma \mathbb{E}_ {s' \sim p} &\mathbb{E}_ {a'\sim \pi} q_{\pi}(s',a') \\\\ 
\end{align}
