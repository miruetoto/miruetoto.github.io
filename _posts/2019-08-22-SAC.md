---
layout: post
title: (리뷰) SAC 
--- 

### About this doc

- 이 포스팅은 아래 논문의 리뷰이다. <br/>
Haarnoja, T., Zhou, A., Hartikainen, K., Tucker, G., Ha, S., Tan, J., ... & Levine, S. (2018). Soft actor-critic algorithms and applications. arXiv preprint arXiv:1812.05905.

### Preliminaries

- MDP 는 $({\cal S},{\cal A},p,r)$ 이고 state space $\cal S$와 action space $\cal A$는 모두 연속이다. $p$는 state transition probability 이며 현재상태 ${\bf s}_ t \in {\cal S}$ 에서 특정한 액션 ${\bf a}_ t \in {\cal A}$ 를 취해서 다음상태 ${\bf s}_ {t+1} \in {\cal S}$ 로 갈 probability density를 의미한다. 따라서 
\begin{align}
p:{\cal S} \times {\cal S} \times {\cal A} \to [0,\infty) 
\end{align}
이다. 환경은 현재상태 ${\bf s}_ t $ 와 액션 ${\bf a}_ t$에 따라서 보상 $r:{\cal S}\times{\cal A} \to [r_{min}, r_{max}]$ 를 주게 된다. 

- 여기에서 폴리쉬 $\pi({\bf a}_ t | {\bf s}_ t)$ 가 기븐일 경우 아래와 같은 확률을 정의할 수 있는데
\begin{align}
p({\bf s}_ t \| \pi), \quad p({\bf s}_ t,{\bf a}_ t \| \pi) 
\end{align}
이 논문에서는 이를 각각 $\rho_{\pi}({\bf s}_ t)$ 와 $\rho_{\pi}({\bf s}_ t, {\bf a}_ t)$ 로 표시한다. 그리고 이것들을 각각 아래와 같이 부른다. <br/>
$\cdot ~ \rho_{\pi}({\bf s}_ t)$: *state* marginals of the trajectory distribution induced by a polish $\pi({\bf a}_ t | {\bf s}_ t)$ <br/>
$\cdot ~ \rho_{\pi}({\bf s}_ t, {\bf a}_ t)$: *state-action* marginals of the trajectory distribution induced by a polish $\pi({\bf a}_ t | {\bf s}_ t)$

- 통상적으로 우리의 목표는 아래값을 최대로 하는 적절한 $\pi({\bf a}_ t | {\bf s}_ t)$를 학습하는 것이었다.
\begin{align}
\sum_{t} \mathbb{E}_ {({\bf s}_ t, {\bf a}_ t) \sim \rho_\pi} \big[r({\bf s}_ t, {\bf a}_ t) \big] 
\end{align}

- 논문에서는 위의 오브젝티브 함수에 엔트로피개념의 패널티텀을 추가하여 아래와 같은 오브젝트 함수를 최대화 하는 문제를 푸는것에 관심을 가진다. 
\begin{align}
\sum_{t} \mathbb{E}_ {({\bf s}_ t, {\bf a}_ t) \sim \rho_\pi} \big[r({\bf s}_ t, {\bf a}_ t) 
+\alpha {\cal H}\big(\pi(\cdot \| {\bf s}_ t) \big) \big] 
\end{align}
여기에서 ${\cal H}(\pi(\cdot \| {\bf s}_ t))$ 는 분포 $\pi(\cdot \| {\bf s}_ t)$의 정보량이다. 

### Soft Policy Iteration

- (잠깐복습) 원래 큐펑션에 대한 벨만이퀘이션은 아래와 같다. (셔튼교재 p.83) 
\begin{align}
q_{\pi}(s,a)=&\sum_{s',r}p(s',r \| s,a)\big[r+\gamma v_{\pi}(s') \big] \\\\ \\
=&\sum_{r} \sum_{s'} p(s',r \| s,a) \big[r+\gamma v_{\pi}(s') \big] \\\\ 
=&\sum_{r} r \sum_{s'} p(s',r \| s,a) + \sum_{r} \sum_{s'}\gamma v_{\pi}(s') p(s',r \| s,a) \\\\ 
=&r(s,a) + \sum_{s'}\gamma v_{\pi}(s') p(s' \| s,a) \\\\ 
=&r(s,a) + \mathbb{E}_ {s' \sim p} \gamma v_{\pi}(s') \\\\ 
\end{align}
여기에서 $v_{\pi}(s')$ 는 밸류펑션이며 아래의 식을 만족하는 값으로 정의된다. 
\begin{align}
v_{\pi}(s)=&\sum_{a}\pi(a|s)\sum_{s',r}p(s',r|s,a)\Big[r+\gamma v_{\pi}(s')\Big] \\\\ \\ 
=&\sum_{a}\pi(a|s)q_{\pi}(s,a) \\\\ \\ 
=& ~ \mathbb{E}_ {a \sim \pi}q_{\pi}(s,a)
\end{align}
따라서 위의 식을 연립하여 정리하면 
\begin{align}
q_{\pi}(s,a)=r(s,a) +  \gamma \mathbb{E}_ {s' \sim p} &\mathbb{E}_ {a'\sim \pi} q_{\pi}(s',a') 
\end{align}
와 같이 되고 특정한 정책 $\pi$에 대하여 $q_{\pi}$를 구하는 방법은 수렴할때 까지 아래를 반복하는 것이다. 
\begin{align}
q_{\pi}^{(k+1)}(s,a) = r(s,a) +  \gamma \mathbb{E}_ {s' \sim p} &\mathbb{E}_ {a'\sim \pi} q_{\pi}^{(k)}(s',a') 
\end{align}
이때 ***backup operator*** ${\cal T}^{\pi}$ 라는걸 도입하여 위의 식을 아래와 같이 재 표현할 수 있다. 
\begin{align}
{\cal T}^{\pi} q_{\pi}^{(k)}(s,a) = r(s,a) +  \gamma \mathbb{E}_ {s' \sim p} &\mathbb{E}_ {a'\sim \pi} q_{\pi}^{(k)}(s',a')
\end{align}
보는것처럼 ${\cal T}^{\pi} q_{\pi}^{(k)}(s,a)=q_{\pi}^{(k+1)}(s,a)$ 을 만족한다. 그런데 만약에 $q_{\pi}^{(k)}$ 가 정책 $\pi$에 대한 큐펑션이 맞다면(근사치가 아니라 정확한 값이라면) ${\cal T}^{\pi} q_{\pi}^{(k)}(s,a)=q_{\pi}^{(k)}(s,a)$ 가 성립하게 된다. 즉 정확한 큐펑션은 백업오퍼레이터의 ***fixed point*** 이다. 따라서 벨만이퀘이션
\begin{align}
q_{\pi}(s,a) = r(s,a) +  \gamma \mathbb{E}_ {s' \sim p} &\mathbb{E}_ {a'\sim \pi} q_{\pi}(s',a') 
\end{align}
은 아래와 같이 표현할 수 있다. 
\begin{align}
{\cal T}^{\pi} q_{\pi}(s,a) = r(s,a) +  \gamma \mathbb{E}_ {s' \sim p} &\mathbb{E}_ {a'\sim \pi} q_{\pi}(s',a').
\end{align}

- 그런데 논문에서는 아래와 같이 정의되는 ***soft state value function*** 을 정의한다. 
\begin{align}
v_{\pi}(s)= \mathbb{E}_ {a \sim \pi} \big[ q_{\pi}(s,a) -\alpha \log \pi(a|s) \big]
\end{align}
그리고 이런 소프트-스테이트-밸류펑션을 대입하여 ***soft q-value*** 라는 것을 아래와 같이 정의한다. 
\begin{align}
{\cal T}^{\pi} q_{\pi}(s,a) = r(s,a) +  \gamma \mathbb{E}_ {s' \sim p} \mathbb{E}_ {a'\sim \pi} \big[  q_{\pi}(s',a')-\alpha \log \pi(a' \| s') \big].
\end{align}
논문식으로 표현하면 아래와 같다. (논문의 수식 (2)-(3)) 
\begin{align}
{\cal T}^{\pi} Q_{\pi} ({\bf s}_ t,{\bf a}_ t)= r({\bf s}_ t,{\bf a}_ t) + \gamma \mathbb{E}_ { {\bf s}_ {t+1} \sim p} \mathbb{E}_ { {\bf a}_ {t+1} \sim \pi} \big[  Q_{\pi}( {\bf s}_ {t+1}, {\bf a}_ {t+1} )-\alpha \log \pi( {\bf a}_ {t+1} \| {\bf s}_ {t+1}) \big].
\end{align}

- 소프트-큐펑션을 구하는 방식도 일반적인 큐함수를 구하는 방식과 동일하게 임의의 $Q^{(0)}$ 로 시작하여 아래식을 반복하여 구할 수 있음이 알려져 있다. (논문의 레마1) 
\begin{align}
{\cal T}^{\pi} Q_{\pi}({\bf s}_ t,{\bf a}_ t) = r({\bf s}_ t,{\bf a}_ t) +  \gamma \mathbb{E}_ { {\bf s}_ {t+1} \sim p} \mathbb{E}_ { {\bf a}_ {t+1} \sim \pi} \big[ Q_{\pi}({\bf s}_ t,{\bf a}_ t)-\alpha \log \pi({\bf a}_ t \| {\bf s}_ t) \big].
\end{align}
이 결과는 직관적인 편이라 받아들이기 쉽다. 

- 정책을 업데이트 하는 방법에 대하여 논의하자. 논문에서는 모든 정책을 고려하지 않고 특정 정책들의 집합 ${\bf \Pi}$ 에서 최적의 정책을 찾는 방식을 제안한다. 일반적으로 폴리쉬는 현재 정책 $\pi_{old}$ 보다 $q$-펑션을 약간이라도 인푸르브할 수 있는 새로운 $\tilde \pi$를 찾음을 반복함으로써 최적값을 구할수 있는데 여기에서는 우리가 생각한 약간의 인푸르브된 정책 $\tilde \pi$ 가 ${\bf \Pi}$ 에 없을 수도 있다는 문제점이 생긴다. 그래서 ${\bf \Pi}$ 의 원소중에서 (이 원소들을 편의상 $\pi'$라고 하자. 즉 $\pi' \in {\bf \Pi}$ 임)  $\tilde \pi$ 와 가장 가까운 아이를 고르자는 생각을 하게 된다. 그래서 쿨백라이블러 괴리도를 도입하게 되어 $\tilde \pi$ 와 $\pi '$ 의 거리비슷한것을 측정하고 그것을 최소화 하는 크라이테리온을 아래와 같이 만든다. 
\begin{align}
\pi_{new} = \underset{\pi' \in \Pi}{\operatorname{argmin}} D_{KL} \bigg( \pi'(\cdot \| {\bf s}_ {t}) \bigg\\| \frac{\exp\big( \frac{1}{\alpha}Q_{\pi_{old} } ({\bf s}_ t, \cdot) \big)}{Z_{\pi_ {old} }({\bf s}_ t)} \bigg).
\end{align}
여기에서 아래의 term
\begin{align}
\frac{\exp\big( \frac{1}{\alpha}Q_{\pi_{old} } ({\bf s}_ t, \cdot) \big)}{Z_{\pi_ {old} }({\bf s}_ t)}
\end{align}
이 $\tilde \pi$ 의 역할을 한다고 해석하면 된다. 생긴건 저래도 쿨백라이블러 안에 들어가 있는걸 보면 저것도 나름 분포인 모양이다. 아니면 분포를 만들기 위해서 어거지로 $Z_{\pi_ {old} } ({\bf s}_ t)$ 를 짜맞췄던가. 

- 아무튼 레마2에 의하면 위의 크라이테리온으로 업데이트한 $\pi_{new}$ 가 아래의 식을 만족한다고 한다. 
\begin{align}
Q_{\pi_{new} } ({\bf s}_ t, {\bf a}_ t) \geq Q_{\pi_ {old} } ({\bf s}_ t, {\bf a}_ t)
\end{align}
또한 Thm1 에 의하면 이런식으로 계속 폴리쉬를 업데이트하다보면 최적의 폴리쉬 $\pi^* $ 를 찾을 수 있다고 한다. 논문의 언급을 보면 $\pi^* $ 는 반드시 $\bf \Pi$ 의 원소일 필요는 없는것 같다. 

### Soft Actor-Crititc 

- 이번에는 ***parameterized soft q-fucntion*** $Q_{\theta}({\bf s}_ t, {\bf a}_ t)$ 와 ***tractable policy*** $\pi_{\phi}({\bf a}_ t \| {\bf s}_ t)$ 를 논의하겠다. 파라메터화된 소프트-$q$펑션은 아무래도 큐함수를 어떠한 파라메터 $\theta$ 로 표현한 것이고 (따라서 $Q^{(k)}$를 업데이트하는 것이 아니라 $\theta^{(k)}$를 업데이트 할 생각임) 트랙터블-폴리쉬는 파라메터 $\phi$ 로 표현가능한 어떠한 폴리쉬인데 아마 이런 폴리쉬가 다루기가 손쉬워서 트랙터블이라는 이름이 붙은것 같다.  

- 먼저 파라메터화된 소프트-$q$ 펑션의 파라메터 $\theta$를 훈련시키는 방법에 대하여 알아보자. 모수 $\theta$ 에 대한 훈련이 잘되어있을수록 아래식이 성립함을 관찰하자. 
\begin{align}
Q_{\pi,\theta} ({\bf s}_ t,{\bf a}_ t) \approx r({\bf s}_ t,{\bf a}_ t) + \gamma \mathbb{E}_ { {\bf s}_ {t+1} \sim p} \mathbb{E}_ { {\bf a}_ {t+1} \sim \pi} \big[  Q_{\pi,\theta}( {\bf s}_ {t+1}, {\bf a}_ {t+1} )-\alpha \log \pi_{\phi}( {\bf a}_ {t+1} \| {\bf s}_ {t+1}) \big].
\end{align}
여기에서 데이터를 잘보고 적당히 $\mathbb{E}_ { {\bf s}_ {t+1} \sim p} \mathbb{E}_ { {\bf a}_ {t+1} \sim \pi}$ 에 대한 평균들을 계산할수 있다. 따라서 아래와 같이 근사할수 있다. 
\begin{align}
Q_{\pi,\theta} ({\bf s}_ t,{\bf a}_ t) \approx r({\bf s}_ t,{\bf a}_ t) + \gamma \big[  Q_{\pi,\bar \theta}( {\bf s}_ {t+1}, {\bf a}_ {t+1} )-\alpha \log \pi_{\phi}( {\bf a}_ {t+1} \| {\bf s}_ {t+1}) \big].
\end{align}
여기에서 $\bar \theta$ 는 ***exponentially moving average of the soft $Q$-function weights*** 로 계산한 값이다. 구체적으로 멀 어떻게 구했다는 것인지 모르겠지만 대충 데이터를 보고 적당한 방식으로 $\theta$를 추정해낸 초기값인것 같다. 아무튼 $\theta$를 구하기 위해서 아래와 같은 크라이테리온을 만든다. 


