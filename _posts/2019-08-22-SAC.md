---
layout: post
title: (리뷰) SAC 
--- 

### About this doc

- 이 포스팅은 아래 논문의 리뷰이다. <br/>
Haarnoja, T., Zhou, A., Hartikainen, K., Tucker, G., Ha, S., Tan, J., ... & Levine, S. (2018). Soft actor-critic algorithms and applications. arXiv preprint arXiv:1812.05905.



### Preliminaries

- MDP 는 $({\cal S},{\cal A},p,r)$ 이고 state space $\cal S$와 action space $\cal A$는 모두 연속이다. $p$는 state transition probability 이며 현재상태 ${\bf s}_ t \in {\cal S}$ 에서 특정한 액션 ${\bf a}_ t \in {\cal A}$ 를 취해서 다음상태 ${\bf s}_ {t+1} \in {\cal S}$ 로 갈 probability density를 의미한다. 따라서 
\begin{align}
p:{\cal S} \times {\cal S} \times {\cal A} \to [0,\infty) 
\end{align}
이다. 환경은 현재상태 ${\bf s}_ t $ 와 액션 ${\bf a}_ t$에 따라서 보상 $r:{\cal S}\times{\cal A} \to [r_{min}, r_{max}]$를 주게 된다. 

- 여기에서 폴리쉬 $\pi({\bf a}_ t | {\bf s}_ t)$ 가 기븐일 경우 아래와 같은 확률을 정의할 수 있는데
\begin{align}
p({\bf s}_ t \| \pi) 
\end{align}
\begin{align}
p({\bf s}_ t,{\bf a}_ t \| \pi) 
\end{align}
이 논문에서는 이를 각각 $\rho_{\pi}({\bf s}_ t)$ 와 $\rho_{\pi}({\bf s}_ t, {\bf a}_ t)$ 로 표시한다. 그리고 이것들을 각각 ***state* marginals of the trajectory distribution induced by a polish $\pi({\bf a}_ t | {\bf s}_ t)$, *state-action* marginals of the trajectory distribution induced by a polish $\pi({\bf a}_ t | {\bf s}_ t)$** 라고 부른다. 

- 우리의 목표는 아래값을 최대로 하는 적절한 $\pi({\bf a}_ t | {\bf s}_ t)$를 학습하는 것이다.
\begin{align}
\sum_{t} \mathbb{E}_ {({\bf s}_ t, {\bf a}_ t) \sim \rho_\pi} \big[r({\bf s}_ t, {\bf a}_ t) \big] 
\end{align}
논문에서는 위의 오브젝티브 함수에 엔트로피개념의 패널티 텀을 추가하여 아래와 같은 오브젝트 함수를 최대화 하는 문제를 푸는것에 관심을 가진다. 
\begin{align}
\sum_{t} \mathbb{E}_ {({\bf s}_ t, {\bf a}_ t) \sim \rho_\pi} \big[r({\bf s}_ t, {\bf a}_ t) 
+\alpha {\cal H}\big(\pi(\cdot \| {\bf s}_ t) \big) \big] 
\end{align}
