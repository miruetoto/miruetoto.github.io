---
layout: post
title: (리뷰) SAC 
--- 

### About this doc

- 이 포스팅은 아래 논문의 리뷰이다. 


### Preliminaries

- MDP 는 $({\cal S},{\cal A},p,r)$ 이고 state space $\cal S$와 action space $\cal A$는 모두 연속이다. $p$는 state transition probability 이며 현재상태 ${\bf s}_ t \in {\cal S}$ 에서 특정한 액션 ${\bf a}_ t \in {\cal A}$ 를 취해서 다음상태 ${\bf s}_ {t+1} \in {\cal S}$ 로 갈 probability density를 의미한다. 따라서 
\begin{align}
p:{\cal S} \times {\cal S} \times {\cal A} \to [0,\infty) 
\end{align}
이다. 환경은 현재상태 ${\bf s}_ t $ 와 액션 ${\bf a}_ t$에 따라서 보상 $r:{\cal S}\times{\cal A} \to [r_{min}, r_{max}]$를 주게 된다. 그리고 폴리쉬 $\pi({\bf a}_ t | {\bf s}_ t)$에 따른 밸류펑션 $\rho_{\pi}({\bf s}_ t)$ 와 $q$-펑션 $\rho_{\pi}({\bf s}_ t, {\bf a}_ t)$를 정의한다. 

- 우리의 목표는 아래값을 최대로 하는 적절한 $\pi({\bf a}_ t | {\bf s}_ t)$를 학습하는 것이다.
\begin{align}
\sum_{t} \mathbb{E}_ {({\bf s}_ t, {\bf a}_ t) \sim \rho_\pi} \big[r({\bf s}_ t, {\bf a}_ t) \big] 
\end{align}
