---
layout: post
title: (리뷰) SAC 
--- 

### About this doc

- 이 포스팅은 아래 논문의 리뷰이다. <br/>
Haarnoja, T., Zhou, A., Hartikainen, K., Tucker, G., Ha, S., Tan, J., ... & Levine, S. (2018). Soft actor-critic algorithms and applications. arXiv preprint arXiv:1812.05905.

### Preliminaries

- MDP 는 $({\cal S},{\cal A},p,r)$ 이고 state space $\cal S$와 action space $\cal A$는 모두 연속이다. $p$는 state transition probability 이며 현재상태 ${\bf s}_ t \in {\cal S}$ 에서 특정한 액션 ${\bf a}_ t \in {\cal A}$ 를 취해서 다음상태 ${\bf s}_ {t+1} \in {\cal S}$ 로 갈 probability density를 의미한다. 따라서 
\begin{align}
p:{\cal S} \times {\cal S} \times {\cal A} \to [0,\infty) 
\end{align}
이다. 환경은 현재상태 ${\bf s}_ t $ 와 액션 ${\bf a}_ t$에 따라서 보상 $r:{\cal S}\times{\cal A} \to [r_{min}, r_{max}]$ 를 주게 된다. 

- 여기에서 폴리쉬 $\pi({\bf a}_ t | {\bf s}_ t)$ 가 기븐일 경우 아래와 같은 확률을 정의할 수 있는데
\begin{align}
p({\bf s}_ t \| \pi), \quad p({\bf s}_ t,{\bf a}_ t \| \pi) 
\end{align}
이 논문에서는 이를 각각 $\rho_{\pi}({\bf s}_ t)$ 와 $\rho_{\pi}({\bf s}_ t, {\bf a}_ t)$ 로 표시한다. 그리고 이것들을 각각 아래와 같이 부른다. <br/>
$\cdot ~ \rho_{\pi}({\bf s}_ t)$: *state* marginals of the trajectory distribution induced by a polish $\pi({\bf a}_ t | {\bf s}_ t)$ <br/>
$\cdot ~ \rho_{\pi}({\bf s}_ t, {\bf a}_ t)$: *state-action* marginals of the trajectory distribution induced by a polish $\pi({\bf a}_ t | {\bf s}_ t)$

- 통상적으로 우리의 목표는 아래값을 최대로 하는 적절한 $\pi({\bf a}_ t | {\bf s}_ t)$를 학습하는 것이었다.
\begin{align}
\sum_{t} \mathbb{E}_ {({\bf s}_ t, {\bf a}_ t) \sim \rho_\pi} \big[r({\bf s}_ t, {\bf a}_ t) \big] 
\end{align}

- 논문에서는 위의 오브젝티브 함수에 엔트로피개념의 패널티텀을 추가하여 아래와 같은 오브젝트 함수를 최대화 하는 문제를 푸는것에 관심을 가진다. 
\begin{align}
\sum_{t} \mathbb{E}_ {({\bf s}_ t, {\bf a}_ t) \sim \rho_\pi} \big[r({\bf s}_ t, {\bf a}_ t) 
+\alpha {\cal H}\big(\pi(\cdot \| {\bf s}_ t) \big) \big] 
\end{align}
여기에서 ${\cal H}(\pi(\cdot \| {\bf s}_ t))$ 는 분포 $\pi(\cdot \| {\bf s}_ t)$의 정보량이다. 

- (잠깐복습) 원래 큐펑션에 대한 벨만이퀘이션은 아래와 같다. (셔튼교재 p.83) 
\begin{align}
q_{\pi}(s,a)=&\sum_{s',r}p(s',r \| s,a)\big[r+\gamma v_{\pi}(s') \big] \\\\ \\
=&\sum_{r} \sum_{s'} p(s',r \| s,a) \big[r+\gamma v_{\pi}(s') \big] \\\\ 
=&\sum_{r} r \sum_{s'} p(s',r \| s,a) + \sum_{r} \sum_{s'}\gamma v_{\pi}(s') p(s',r \| s,a) \\\\ 
=&r(s,a) + \sum_{s'}\gamma v_{\pi}(s') p(s' \| s,a) \\\\ 
=&r(s,a) + \mathbb{E}_ {s' \sim p} \gamma v_{\pi}(s') \\\\ 
\end{align}
여기에서 $v_{\pi}(s')$ 는 밸류펑션이며 아래의 식을 만족하는 값으로 정의된다. 
\begin{align}
v_{\pi}(s)=&\sum_{a}\pi(a|s)\sum_{s',r}p(s',r|s,a)\Big[r+\gamma v_{\pi}(s')\Big] \\\\ \\ 
=&\sum_{a}\pi(a|s)q_{\pi}(s,a) \\\\ \\ 
=& ~ \mathbb{E}_ {a \sim \pi}q_{\pi}(s,a)
\end{align}
따라서 위의 식을 연립하여 정리하면 
\begin{align}
q_{\pi}(s,a)=r(s,a) +  \gamma \mathbb{E}_ {s' \sim p} &\mathbb{E}_ {a'\sim \pi} q_{\pi}(s',a') 
\end{align}
와 같이 되고 특정한 정책 $\pi$에 대하여 $q_{\pi}$를 구하는 방법은 수렴할때 까지 아래를 반복하는 것이다. 
\begin{align}
q_{\pi}^{(k+1)}(s,a) = r(s,a) +  \gamma \mathbb{E}_ {s' \sim p} &\mathbb{E}_ {a'\sim \pi} q_{\pi}^{(k)}(s',a') 
\end{align}
이때 ***backup operator*** ${\cal T}^{\pi}$ 라는걸 도입하여 위의 식을 아래와 같이 재 표현할 수 있다. 
\begin{align}
{\cal T}^{\pi} q_{\pi}^{(k)}(s,a) = r(s,a) +  \gamma \mathbb{E}_ {s' \sim p} &\mathbb{E}_ {a'\sim \pi} q_{\pi}^{(k)}(s',a')
\end{align}
보는것처럼 ${\cal T}^{\pi} q_{\pi}^{(k)}(s,a)=q_{\pi}^{(k+1)}(s,a)$ 을 만족한다. 그런데 만약에 $q_{\pi}^{(k)}$ 가 정책 $\pi$에 대한 큐펑션이 맞다면(근사치가 아니라 정확한 값이라면) ${\cal T}^{\pi} q_{\pi}^{(k)}(s,a)=q_{\pi}^{(k)}(s,a)$ 가 성립하게 된다. 즉 정확한 큐펑션은 백업오퍼레이터의 ***fixed point*** 이다. 따라서 벨만이퀘이션
\begin{align}
q_{\pi}(s,a) = r(s,a) +  \gamma \mathbb{E}_ {s' \sim p} &\mathbb{E}_ {a'\sim \pi} q_{\pi}(s',a') 
\end{align}
은 아래와 같이 표현할 수 있다. 
\begin{align}
{\cal T}^{\pi} q_{\pi}(s,a) = r(s,a) +  \gamma \mathbb{E}_ {s' \sim p} &\mathbb{E}_ {a'\sim \pi} q_{\pi}(s',a').
\end{align}

- 그런데 논문에서는 아래와 같이 정의되는 ***soft state value function*** 을 정의한다. 
\begin{align}
v_{\pi}(s)= \mathbb{E}_ {a \sim \pi} \big[ q_{\pi}(s,a) -\alpha \log \pi(a|s) \big]
\end{align}
그리고 이런 소프트-스테이트-밸류펑션을 대입하여 ***soft q-value*** 라는 것을 아래와 같이 정의한다. 
\begin{align}
{\cal T}^{\pi} q_{\pi}(s,a) = r(s,a) +  \gamma \mathbb{E}_ {s' \sim p} \mathbb{E}_ {a'\sim \pi} \big[  q_{\pi}(s',a')-\alpha \log \pi(a' \| s') \big].
\end{align}
논문식으로 표현하면 아래와 같다. (논문의 수식 (2)-(3)) 
\begin{align}
{\cal T}^{\pi} Q_{\pi} ({\bf s}_ t,{\bf a}_ t)= r({\bf s}_ t,{\bf a}_ t) + \gamma \mathbb{E}_ { {\bf s}_ {t+1} \sim p} \mathbb{E}_ { {\bf a}_ {t+1} \sim \pi} \big[  q_{\pi}( {\bf s}_ {t+1}, {\bf a}_ {t+1} )-\alpha \log \pi( {\bf a}_ {t+1} \| {\bf s}_ {t+1}) \big].
\end{align}

- 소프트-큐펑션을 구하는 방식도 일반적인 큐함수를 구하는 방식과 동일하게 임의의 $Q^{(0)}$ 로 시작하여 아래식을 반복하여 구할 수 있음이 알려져 있다. (논문의 레마1) 
\begin{align}
{\cal T}^{\pi} Q_{\pi}({\bf s}_ t,{\bf a}_ t) = r({\bf s}_ t,{\bf a}_ t) +  \gamma \mathbb{E}_ { {\bf s}_ {t+1} \sim p} \mathbb{E}_ { {\bf a}_ {t+1} \sim \pi} \big[ Q_{\pi}({\bf s}_ t,{\bf a}_ t)-\alpha \log \pi({\bf a}_ t \| {\bf s}_ t) \big].
\end{align}
이 결과는 직관적인 편이라 받아들이기 쉽다. 

- 정책을 업데이트 하는 방법에 대하여 논의하자. 논문에서는 모든 정책을 고려하지 않고 특정 정책들의 집합 ${\bf \Pi}$ 에서 최적의 정책을 찾는 방식을 제안한다. 일반적으로 폴리쉬는 현재 정책 $\pi_{old}$ 보다 $q$-펑션을 약간이라도 인푸르브할 수 있는 새로운 $\tilde \pi$를 찾음을 반복함으로써 최적값을 구할수 있는데 여기에서는 우리가 생각한 약간의 인푸르브된 정책 $\tilde \pi$ 가 ${\bf \Pi}$ 에 없을 수도 있다는 문제점이 생긴다. 그래서 ${\bf \Pi}$ 의 원소중에서 (이 원소들을 편의상 $\pi'$라고 하자. 즉 $\pi' \in {\bf \Pi}$ 임)  $\tilde \pi$ 와 가장 가까운 아이를 고르자는 생각을 하게 된다. 그래서 쿨백라이블러 괴리도를 도입하게 되어 $\tilde \pi$ 와 $\pi '$ 의 거리비슷한것을 측정하고 그것을 최소화 하는 크라이테리온을 아래와 같이 만든다. 
\begin{align}
\pi_{new} = \underset{\pi' \in \Pi}{\operatorname{argmin}} D_{KL} \bigg(\pi'(\cdot \| {\bf s}_ {t} \\| \frac{1}{2} \bigg) 
\end{align} 




