---
layout: post
title: (정리) EM 알고리즘
--- 

### About this doc 

- 여기에서는 EM 알고리즘에 대하여 다룬다. 이 포스팅은 임요한 교수님의 강의노트를 위주로 요약하였다. 

### EXAMPLE 1: Mixture model 

- 이번예제에서는 예제를 푸는것과 동시에 EM-알고리즘에 대한 대략적인 설명을 곁들인다. 

- 아래와 같은 자료를 고려하자. 
\begin{align}
Z_i=W_iX_i + (1-W_i)Y_i, \quad i=1,\dots,n 
\end{align}
아래를 가정하자. <br/><br/>
**(1)** 모든 관측치는 서로 독립이다. <br/>
**(2)** $W_i$ 는 확률이 $p$인 베르누이 분포에서 생성되었다. <br/>
**(3)** $X_i$ 와 $Y_i$ 는 각각 평균이 $\lambda_1$, $\lambda_2$ 인 포아송분포에서 생성되었다. 

- 관측된 자료는 $\bf z$ 이고 관측하고 싶었던 자료, 즉 완전한 자료는 $\bf (z,w)$이다. 완전한 자료가 $\bf (x,y,w)$ 가 아님을 유의하자. (물론 $\bf (x,y,w)$ 가 완전한 자료 맞다. 하지만 완전한 자료를 $\bf (x,y,w)$ 로 두면 우리가 관측한 자료 $\bf z$를 하나도 이용할 수 없다.) 우리가 알고 싶은 것은 $(\lambda_1,\lambda_2,p)$ 이므로 MLE를 구하기 위해서 $(\lambda_1,\lambda_2,p)$ 의 우도함수 $L(\lambda_1,\lambda_2,p)$ 를 생각하자. 그런데 **자료가 모두 관측되었다고 가정하면** $(\lambda_1,\lambda_2,p)$ 의 우도함수는 $\bf(z,w)$의 **조인트-pdf** 이므로 아래와 같이 쓸 수 있다. 
\begin{align}
L(\lambda_1,\lambda_2,p)=f({\bf z,w})=f({\bf w}) f({\bf z | \bf w})=\prod_{i=1}^{n}f(w_i)f(z_i|w_i)
\end{align}
이제 $f(w_i)$ 와 $f(z_i|w_i)$ 를 구해야 한다. 각각은 아래와 같이 풀 수 있다. <br/><br/>
**(1)** $f(w_i)=p^{w_i}+(1-p)^{1-w_i}$ <br/>
**(2)** $f(z_i|w_i)=
\begin{cases} 
\frac{e^{-\lambda_1} \lambda_1^{z_i}}{z_i !} & w_i=1 \\\\ 
\frac{e^{-\lambda_2} \lambda_2^{z_i}}{z_i !} & w_i=0
\end{cases}=\left(\frac{e^{-\lambda_1} \lambda_1^{z_i}}{z_i !} \right)^{w_i} \left(\frac{e^{-\lambda_2} \lambda_2^{z_i}}{z_i !} \right)^{1-w_i}$ <br/><br/> 
여기에서 $f(z_i|w_i)$ 는 다소 직관적으로 구했다. **~~원래 믹스처모델은 대충해도 된다.~~**
아무튼 이걸 정리하면 아래와 같이 된다. 
\begin{align}
L(\lambda_1,\lambda_2,p)=\prod_{i=1}^{n}\left(\frac{pe^{-\lambda_1} \lambda_1^{z_i}}{z_i !} \right)^{w_i} \left(\frac{(1-p)e^{-\lambda_2} \lambda_2^{z_i}}{z_i !} \right)^{1-w_i}
\end{align}

- 사실 완전한 자료 $\bf (z,w)$ 를 관측했다면 위에서 구한 $L(\lambda_1,\lambda_2,p)$ 를 최소화하는 $(\lambda_1,\lambda_2,p)$를 구하면 끝난다. 즉 아래를 연립해서 풀면 끝난다. (물론 $L(\lambda_1,\lambda_2,p)$ 이 covex임도 보여야 함) 
\begin{align}
\begin{cases}
\frac{\partial L(\lambda_1,\lambda_2,p)}{\partial \lambda_1} = 0 \\\
\frac{\partial L(\lambda_1,\lambda_2,p)}{\partial \lambda_2} = 0 \\\
\frac{\partial L(\lambda_1,\lambda_2,p)}{\partial p} =0 
\end{cases}
\end{align}
그런데 골치 아픈 것은 우리가 관측한 자료가 사실 $\bf (z,w)$가 아니라 $\bf z$ 라는 것이다. 그래서 $L(\lambda_1,\lambda_2,p)$ 는 우도함수가 아니라 **랜덤변수**가 된다. 즉  
\begin{align}
L(\lambda_1,\lambda_2,p)=\prod_{i=1}^{n}\left(\frac{pe^{-\lambda_1} \lambda_1^{z_i}}{z_i !} \right)^{W_i} \left(\frac{(1-p)e^{-\lambda_2} \lambda_2^{z_i}}{z_i !} \right)^{1-W_i}
\end{align}
이다. 위의 식은 $W_i$가 포함되어있으므로 랜덤이 된다. 즉 $L(\lambda_1,\lambda_2,p)$ 는 $W_i$의 값들에 의해서 좌우하는 **"function of random variable"** 이라고 볼 수 있다. 굳이 설명하면 아래와 같은 꼴이다. 
\begin{align}
L(\lambda_1,\lambda_2,p)=g(W_1,\dots,W_n)
\end{align}
--- 
그런데 사실 $W_i$는 독립이라는 가정이 있으므로 
$
L(\lambda_1,\lambda_2,p)=\prod_{i=1}^{n}g(W_i)
$
와 같이 보는 것도 가능하다. 
---
아무튼 우리는 $W_i$에 대한 값을 모르므로 $L(\lambda_1,\lambda_2,p)$ 는 랜덤이 되고 따라서 이를 최적화할 수는 없다. 하지만 
\begin{align}
E\left[L(\lambda_1,\lambda_2,p)|\right]
\end{align}
는 랜덤이 아니므로 이것을 최적화 할 수는 있다. 여기에서 당연히 평균은 ${\bf z}=c(z_i,\dots,z_n)$가 기븐되었을 경우 ${\bf W}=c(W_1,\dots,W_n)$의 평균이다. 즉 엄밀하게는 
\begin{align}
E_{\bf W}\left[L(\lambda_1,\lambda_2,p)|{\bf z}\right]
\end{align}
이다. 머 굳이 이렇게 까지 쓸 필요는 없고 $E\left[L(\lambda_1,\lambda_2,p)|{\bf z}\right]$ 정도만 써도 의미는 통한다. 아무튼 중요한것은 우리가
\begin{align}
\begin{cases}
\frac{\partial L(\lambda_1,\lambda_2,p)}{\partial \lambda_1} = 0 \\\
\frac{\partial L(\lambda_1,\lambda_2,p)}{\partial \lambda_2} = 0 \\\
\frac{\partial L(\lambda_1,\lambda_2,p)}{\partial p} =0 
\end{cases}
\end{align}
를 푸는것이 아니라 
\begin{align}
\begin{cases}
\frac{\partial E\left[L(\lambda_1,\lambda_2,p) | {\bf z} \right]}{\partial \lambda_1} = 0 \\\
\frac{\partial E\left[L(\lambda_1,\lambda_2,p) | {\bf z} \right]}{\partial \lambda_2} = 0 \\\
\frac{\partial E\left[L(\lambda_1,\lambda_2,p) | {\bf z} \right]}{\partial p} =0 
\end{cases}
\end{align}
를 풀기만 하면 된다는 것이다. 이때 위의 방정식을 푸는 과정을 **M-step** 이라고 하고 $E\left[L(\lambda_1,\lambda_2,p) | {\bf z} \right]$ 를 계산하는 과정을 **E-step** 이라고 한다. 

- 이제 문제는 
\begin{align}
\begin{cases}
\frac{\partial E\left[L(\lambda_1,\lambda_2,p)\right | {\bf z} ]}{\partial \lambda_1} = 0 \\\
\frac{\partial E\left[L(\lambda_1,\lambda_2,p)\right | {\bf z} ]}{\partial \lambda_2} = 0 \\\
\frac{\partial E\left[L(\lambda_1,\lambda_2,p)\right | {\bf z} ]}{\partial p} =0 
\end{cases}
\end{align}
를 푼 것의 솔루션이 
\begin{align}
\begin{cases}
\frac{\partial L(\lambda_1,\lambda_2,p)}{\partial \lambda_1} = 0 \\\
\frac{\partial L(\lambda_1,\lambda_2,p)}{\partial \lambda_2} = 0 \\\
\frac{\partial L(\lambda_1,\lambda_2,p)}{\partial p} =0 
\end{cases}
\end{align}
를 푼것의 솔루션과 같냐는 것이다. 결론부터 이야기하면 같지않다. **즉 EM-알고리즘으로 찾은 모수의 수렴값은 MLE가 아니다.** 요건 상당히 아쉬운 부분이다. 하지만 그럼에도 불구하고 EM-알고리즘이 많이 쓰이는 까닭은 **iteration** 을 반복할수록 **EM-알고리즘** 의 수렴값이 **MLE** 와 가까워 지기는 하기 때문이다. 

- 아무튼 각설하고 이제 **E-step**을 수행하자. 그런데 **M-step**에서 
\begin{align}
\begin{cases}
\frac{\partial E\left[L(\lambda_1,\lambda_2,p)\right]}{\partial \lambda_1} = 0 \\\
\frac{\partial E\left[L(\lambda_1,\lambda_2,p)\right]}{\partial \lambda_2} = 0 \\\
\frac{\partial E\left[L(\lambda_1,\lambda_2,p)\right]}{\partial p} =0 
\end{cases}
\end{align}
를 풀지 않고
\begin{align}
\begin{cases}
\frac{\partial E\left[\log L(\lambda_1,\lambda_2,p) | {\bf z} \right]}{\partial \lambda_1} = 0 \\\
\frac{\partial E\left[\log L(\lambda_1,\lambda_2,p) | {\bf z} \right]}{\partial \lambda_2} = 0 \\\
\frac{\partial E\left[\log L(\lambda_1,\lambda_2,p) | {\bf z} \right]}{\partial p} =0 
\end{cases}
\end{align}
를 풀어도 되니까 
\begin{align}
E\left[\log L(\lambda_1,\lambda_2,p)\right|{\bf z}]
\end{align}
를 구하자. 특별한 생각 없이 쭉쭉 전개하면 아래처럼 된다. 
\begin{align}
E& \left[\log L(\lambda_1,\lambda_2,p) \left| {\bf z} \right. \right] \\\\ 
&=E \left[ \sum_{i=1}^{n}\left( W_i\log\frac{pe^{-\lambda_1} \lambda_1^{z_i}}{z_i !}+(1-W_i)\log\frac{(1-p)e^{-\lambda_2} \lambda_2^{z_i}}{z_i !} \right) \left| {\bf z} \right. \right]  \\\\ \\\\
&=E \left[ \sum_{i=1}^{n}\left( W_i\log\bigcirc +(1-W_i)\log\bigtriangleup \right)  \left| {\bf z} \right. \right]  \\\\ \\\\
&=E \left[ \sum_{i=1}^{n}\left( W_i\log\frac{\bigcirc}{\bigtriangleup}+\log\bigtriangleup \right) \left| {\bf z} \right.\right] \\\\ \\\\
&=\sum_{i=1}^{n}\left( E(W_i|{\bf z})\log\frac{\bigcirc}{\bigtriangleup}+\log\bigtriangleup \right)
\end{align}

