---
layout: post
title: (정리) EM 알고리즘
--- 

### About this doc 

- 여기에서는 EM 알고리즘에 대하여 다룬다. 이 포스팅은 임요한 교수님의 강의노트를 위주로 요약하였다. 

- EM 알고리즘을 이해하는데는 많은 센스가 필요하다. 원래 MLE 관련된 이론은 계산이 더러운 경우가 많기 때문에 통계적 센스와 통찰이 필요하다. (안 그러면 계산하다가 개고생한다.) 

### EM 알고리즘은? 

- 우선 현재까지 내가 이해하기로는 EM 알고리즘은 래이턴트-베리어블이 있을 경우 MLE 를 구하는 문제에 쓰인다. 즉 통계학에서 흔히 말하는 **미싱-프라블럼**(missing problem)을 푸는데 사용된다. 사실 여기에만 쓰이는게 아니고 다양하게 쓰일것 같은데 현재까지 내가 찾아본바로는 여기에만 쓰이는것 같다.

- **미싱-프라블럼** 이란 우리는 사실 $\bf (Z,W)$를 관측하고 싶었는데 어떠한 이유로 $\bf Z$만 관측한 상황을 의미한다. 여기에서 ${\bf Z}=c(Z_1,\dots,Z_n)$ 이고 ${\bf W}=c(W_1,\dots, W)$ 이다. 이때 보통 ${\bf Z}$와 ${\bf W}$의 각 관측치는 독립이라고 가정하는것 같다. 즉 $Z_1,\dots,Z_n$은 서로 독립이고 $W_1,\dots,W_n$도 서로 독립이라고 가정한다. 

- 랜덤벡터 $\bf (Z,W)$ 를 생성하는 어떠한 모수를 $\boldsymbol \theta$ 라고 하자. 우리가 관심있는 것은 $\boldsymbol \theta$ 의 우도함수를 $\boldsymbol \theta$ 를 추론하는 것이다. 그런데 우리는 ${\bf W}$를 관측하지 못했으니까 아래를 구할 수 없다. 
\begin{align}
L({\boldsymbol \theta}| {\bf Z}={\bf z},{\bf W}={\bf w})=f({\bf z},{\bf w} | {\boldsymbol\theta})
\end{align}
그래서 ${\boldsymbol \theta}$ 의 MLE를 구할 수 없다. 그러면 아래를 구해서 ${\boldsymbol \theta}$ 의 MLE를 구하면 어떨까? 
\begin{align}
L({\boldsymbol \theta}| {\bf Z}={\bf z})=\int f({\bf z},{\bf w} | {\boldsymbol\theta}) d{\bf w}
\end{align}
가능할것 같지만 일단 위를 계산하기 매우 어렵다. ~~참고로 위와 같이 **조인트-pdf** 를 가지고 **마지날-pdf** 를 구한다는 논리는 거의다 **입통계** 다. 계산 거의 못한다. (적분을 어케 할거임..)~~  EM 알고리즘의 목표는 $L({\boldsymbol \theta}| {\bf Z}={\bf z})$를 직접계산해서 최대화 못하겠으니까 적당한 트릭을 사용하여 $\boldsymbol \theta$의 MLE를 이터레이티브하게 구하자는 것이다. 

- 보통 통계적 방법은 데이터에 대한 정보를 거의 다 알아낸다는 의미이고 이것은 **새로운 데이터를 생성할 수 있느냐** 에 대한 의미와 일맥상 통한다. 예를들어 확률밀도함수, 모델과 같은 도구들도 **새로운 데이터를 생성** 하기 위한 수단으로 이해할 수 있다. 그래서 종종 어떠한 방법을 이해할때 "그래서 이 방법을 활용하면 새로운 데이터를 어떻게 만든다는것이지?" 라는 관점으로 이해하면 이해가 잘 될 경우가 있다. 참고로 전통적인(=피셔파) 통계추론은 아래와 같이 이루어 진다. <br/><br/>
**(1)** given data를 활용하여 알고싶은 모수인 $\theta$ 에 대한 우도함수를 구한다. 
**(2)** 우도함수를 최대화하는 $\theta$를 추정한다. 
**(3)** 추정된 $\theta$를 가지고 

--- 

### EXAMPLE 1: Mixture model 

- 이번 예제에서는 예제를 푸는것과 동시에 EM-알고리즘에 대한 대략적인 설명을 곁들인다. 

- 아래와 같은 자료를 고려하자. 
\begin{align}
Z_i=W_iX_i + (1-W_i)Y_i, \quad i=1,\dots,n 
\end{align}
아래를 가정하자. <br/><br/>
**(1)** 모든 관측치는 서로 독립이다. <br/>
**(2)** $W_i$ 는 확률이 $p$인 베르누이 분포에서 생성되었다. <br/>
**(3)** $X_i$ 와 $Y_i$ 는 각각 평균이 $\lambda_1$, $\lambda_2$ 인 포아송분포에서 생성되었다. 

- 관측된 자료는 $\bf z$ 이고 관측하고 싶었던 자료, 즉 완전한 자료는 $\bf (z,w)$이다. 완전한 자료를 $\bf (x,y,w)$ 로 두지 않음을 유의하자. (물론 $\bf (x,y,w)$ 가 완전한 자료 맞다. 하지만 완전한 자료를 $\bf (x,y,w)$ 로 두면 우리가 관측한 자료 $\bf z$를 하나도 이용할 수 없다. 완전한 자료를 $\bf (x,y,w)$ 로 두면  이건 통계적센스에 문제가 있는 사람이다.) 

- 우리가 알고 싶은 것은 $(\lambda_1,\lambda_2,p)$ 이므로 MLE를 구하기 위해서 $(\lambda_1,\lambda_2,p)$ 의 우도함수 $L(\lambda_1,\lambda_2,p)$ 를 생각하자. 그런데 **자료가 모두 관측되었다고 가정하면** $(\lambda_1,\lambda_2,p)$ 의 우도함수는 $\bf(z,w)$의 **조인트-pdf** 이므로 아래와 같이 쓸 수 있다. 
\begin{align}
L(\lambda_1,\lambda_2,p)=f({\bf z,w})=f({\bf w}) f({\bf z | \bf w})=\prod_{i=1}^{n}f(w_i)f(z_i|w_i)
\end{align}
이제 $f(w_i)$ 와 $f(z_i|w_i)$ 를 구해야 한다. 각각은 아래와 같이 풀 수 있다. <br/><br/>
**(1)** $f(w_i)=p^{w_i}+(1-p)^{1-w_i}$ <br/>
**(2)** $f(z_i|w_i)=
\begin{cases} 
\frac{e^{-\lambda_1} \lambda_1^{z_i}}{z_i !} & w_i=1 \\\\ 
\frac{e^{-\lambda_2} \lambda_2^{z_i}}{z_i !} & w_i=0
\end{cases}=\left(\frac{e^{-\lambda_1} \lambda_1^{z_i}}{z_i !} \right)^{w_i} \left(\frac{e^{-\lambda_2} \lambda_2^{z_i}}{z_i !} \right)^{1-w_i}$ <br/><br/> 
여기에서 $f(z_i|w_i)$ 는 다소 직관적으로 구했다. **~~원래 믹스처모델은 대충해도 된다.~~**
아무튼 이걸 정리하면 아래와 같이 된다. 
\begin{align}
L(\lambda_1,\lambda_2,p)=\prod_{i=1}^{n}\left(\frac{pe^{-\lambda_1} \lambda_1^{z_i}}{z_i !} \right)^{w_i} \left(\frac{(1-p)e^{-\lambda_2} \lambda_2^{z_i}}{z_i !} \right)^{1-w_i}
\end{align}

- 사실 완전한 자료 $\bf (z,w)$ 를 관측했다면 위에서 구한 $L(\lambda_1,\lambda_2,p)$ 를 최소화하는 $(\lambda_1,\lambda_2,p)$를 구하면 끝난다. 즉 아래를 연립해서 풀면 끝난다. (물론 $L(\lambda_1,\lambda_2,p)$ 이 covex임도 보여야 함) 
\begin{align}
\begin{cases}
\frac{\partial L(\lambda_1,\lambda_2,p)}{\partial \lambda_1} = 0 \\\
\frac{\partial L(\lambda_1,\lambda_2,p)}{\partial \lambda_2} = 0 \\\
\frac{\partial L(\lambda_1,\lambda_2,p)}{\partial p} =0 
\end{cases}
\end{align}
그런데 골치 아픈 것은 우리가 관측한 자료가 사실 $\bf (z,w)$가 아니라 $\bf z$ 라는 것이다. 그래서 $L(\lambda_1,\lambda_2,p)$ 는 우도함수가 아니라 **랜덤변수** 가 된다. 왜냐하면
\begin{align}
L(\lambda_1,\lambda_2,p)=\prod_{i=1}^{n}\left(\frac{pe^{-\lambda_1} \lambda_1^{z_i}}{z_i !} \right)^{W_i} \left(\frac{(1-p)e^{-\lambda_2} \lambda_2^{z_i}}{z_i !} \right)^{1-W_i}
\end{align}
이라 볼 수 있는데 위의 식은 $W_i$가 포함되어있으므로 랜덤이 된다. 따라서 이를 최적화할 수는 없다. 하지만 
\begin{align}
E\left[L(\lambda_1,\lambda_2,p)\right]
\end{align}
는 랜덤이 아니므로 이것을 최적화 할 수는 있을것 **같다.** 여기에서 당연히 $E$는 ${\bf W}$ 에 대한 평균이다. 그런데 사실  $E\left[L(\lambda_1,\lambda_2,p)\right]$ 를 구하면 될 것 같다고 생각하는 사람은 통계센스가 절망적인 사람이다. (나처럼..) 왜냐하면
위를 계산하려면 $p$ 를 알아야 하는데 우리가 지금 하고 있는게 사실 $p$를 알기 위한 것임을 생각해보면 이 과정에서 **순환논리의 모순**이 생기기 때문이다. <br/>
> $p$를 알고 싶음 -> $p$를 구하기 위해서는 ${\bf W}$를 관측해야 함. ->  그런데 ${\bf W}$를 관측못한 상태에서 $p$를 구하는 방법이 있다고 하는데 그건 ${\bf W}$의 관측값들을 평균처리하는 기법이라고 함. -> 그런데 ${\bf W}$의 관측값들을 평균처리하려면 $p$를 알아야함. -> $p$를 알고싶음 ... ??????????????

- 가만히 생각해보면 우리가 지금까지 풀어오던 문제는 <br/><br/>
**(1)** 자료 ${\bf w}$를 관측한 경우: 자료 ${\bf w}$를 가지고 모수 $p$ 를 추론함 <br/>
**(2)** 모수 $p$를 아는 경우: 모수 $p$를 가지고 랜덤변수 ${\bf W}=c(W_1,\dots,W_n)$의 평균따위등을 계산함 <br/><br/>
과 같은 경우였는데 이건 자료도 관측못했고 모수도 모르는 **완전 노답인 상황인 것이다.** 이걸 해결하는 방법은 우리가 알고있는 유일한 값 ${\bf z}$를 이용하여 어떻게든 $p$를 추정하고 이것을 이용하여 $E\left[L(\lambda_1,\lambda_2,p) | {\bf z} \right]$ 를 계산하는 것이다. 이게 계산이 될지는 몰겠지만 아무튼 계산을 했다고 치자. 그럼 이제 우리는 
\begin{align}
\begin{cases}
\frac{\partial L(\lambda_1,\lambda_2,p)}{\partial \lambda_1} = 0 \\\
\frac{\partial L(\lambda_1,\lambda_2,p)}{\partial \lambda_2} = 0 \\\
\frac{\partial L(\lambda_1,\lambda_2,p)}{\partial p} =0 
\end{cases}
\end{align}
를 푸는것이 아니라 
\begin{align}
\begin{cases}
\frac{\partial E\left[L(\lambda_1,\lambda_2,p) | {\bf z} \right]}{\partial \lambda_1} = 0 \\\
\frac{\partial E\left[L(\lambda_1,\lambda_2,p) | {\bf z} \right]}{\partial \lambda_2} = 0 \\\
\frac{\partial E\left[L(\lambda_1,\lambda_2,p) | {\bf z} \right]}{\partial p} =0 
\end{cases}
\end{align}
를 풀기만 하면 된다. 이때 위의 방정식을 푸는 과정을 **M-step** 이라고 하고 $E\left[L(\lambda_1,\lambda_2,p) | {\bf z} \right]$ 를 계산하는 과정을 **E-step** 이라고 한다. 

- 이제 문제는 
\begin{align}
\begin{cases}
\frac{\partial E\left[L(\lambda_1,\lambda_2,p)\right | {\bf z} ]}{\partial \lambda_1} = 0 \\\
\frac{\partial E\left[L(\lambda_1,\lambda_2,p)\right | {\bf z} ]}{\partial \lambda_2} = 0 \\\
\frac{\partial E\left[L(\lambda_1,\lambda_2,p)\right | {\bf z} ]}{\partial p} =0 
\end{cases}
\end{align}
를 푼 것의 솔루션이 
\begin{align}
\begin{cases}
\frac{\partial L(\lambda_1,\lambda_2,p)}{\partial \lambda_1} = 0 \\\
\frac{\partial L(\lambda_1,\lambda_2,p)}{\partial \lambda_2} = 0 \\\
\frac{\partial L(\lambda_1,\lambda_2,p)}{\partial p} =0 
\end{cases}
\end{align}
를 푼것의 솔루션과 같냐는 것이다. 결론부터 이야기하면 같지않다. **즉 EM-알고리즘으로 찾은 모수의 수렴값은 MLE가 아니다.** 요건 상당히 아쉬운 부분이다. 하지만 그럼에도 불구하고 EM-알고리즘이 많이 쓰이는 까닭은 **iteration** 을 반복할수록 **EM-알고리즘** 의 수렴값이 **MLE** 와 가까워 지기는 하기 때문이다. 

- 아무튼 각설하고 이제 **E-step**을 수행하자. 그런데 **M-step**에서 
\begin{align}
\begin{cases}
\frac{\partial E\left[L(\lambda_1,\lambda_2,p)\right]}{\partial \lambda_1} = 0 \\\
\frac{\partial E\left[L(\lambda_1,\lambda_2,p)\right]}{\partial \lambda_2} = 0 \\\
\frac{\partial E\left[L(\lambda_1,\lambda_2,p)\right]}{\partial p} =0 
\end{cases}
\end{align}
를 풀지 않고
\begin{align}
\begin{cases}
\frac{\partial E\left[\log L(\lambda_1,\lambda_2,p) | {\bf z} \right]}{\partial \lambda_1} = 0 \\\
\frac{\partial E\left[\log L(\lambda_1,\lambda_2,p) | {\bf z} \right]}{\partial \lambda_2} = 0 \\\
\frac{\partial E\left[\log L(\lambda_1,\lambda_2,p) | {\bf z} \right]}{\partial p} =0 
\end{cases}
\end{align}
를 풀어도 되니까 
\begin{align}
E\left[\log L(\lambda_1,\lambda_2,p)\right|{\bf z}]
\end{align}
를 구하자. 특별한 생각 없이 쭉쭉 전개하면 아래처럼 된다. 
\begin{align}
E& \left[\log L(\lambda_1,\lambda_2,p) | {\bf z} \right] \\\\ 
&=E \left[ \sum_{i=1}^{n}\left( W_i\log\frac{pe^{-\lambda_1} \lambda_1^{z_i}}{z_i !}+(1-W_i)\log\frac{(1-p)e^{-\lambda_2} \lambda_2^{z_i}}{z_i !} \right)  ~ ~ \Bigg|~ ~ {\bf Z=z} \right]  \\\\ \\\\
&=\sum_{i=1}^{n}E \left[\left( W_i\log\frac{pe^{-\lambda_1} \lambda_1^{z_i}}{z_i !}+(1-W_i)\log\frac{(1-p)e^{-\lambda_2} \lambda_2^{z_i}}{z_i !} \right)  ~ ~ \Bigg|~ ~ Z_i=z_i \right]  \\\\ \\\\
&=\sum_{i=1}^{n}E \left[\left( W_i\log\bigcirc +(1-W_i)\log\bigtriangleup \right)   ~ ~ \Bigg| ~ ~ Z_i=z_i  \right]  \\\\ \\\\
&=\sum_{i=1}^{n}E \left[\left( W_i\log\frac{\bigcirc}{\bigtriangleup}+\log\bigtriangleup \right)  ~ ~ \Bigg| ~ ~ Z_i=z_i \right] \\\\ \\\\
&=\sum_{i=1}^{n}\left( E(W_i|Z_i=z_i)\log\frac{\bigcirc}{\bigtriangleup}+\log\bigtriangleup \right)
\end{align}
여기에서 $E(W_i|Z_i=z_i)$는 아래와 같이 구한다. ~~(그냥 때려맞추면 됨)~~
\begin{align}
E(W_i|Z_i=z_i)=\frac{pe^{-\lambda_1}\lambda_1^{z_i}}{pe^{-\lambda_1}\lambda_1^{z_i}+(1-p)e^{-\lambda_2}\lambda_2^{z_i}}
\end{align}

- 이제 **M-step**을 계산하면 된다. 즉 아래를 풀면 된다. 
\begin{align}
\begin{cases}
\frac{\partial E\left[\log L(\lambda_1,\lambda_2,p) | {\bf z} \right]}{\partial \lambda_1} = 0 \\\
\frac{\partial E\left[\log L(\lambda_1,\lambda_2,p) | {\bf z} \right]}{\partial \lambda_2} = 0 \\\
\frac{\partial E\left[\log L(\lambda_1,\lambda_2,p) | {\bf z} \right]}{\partial p} =0 
\end{cases}
\end{align}

--- 

### EXAMPLE 2: 태양광자료, AR-term은 제외

- 아래와 같은 자료를 고려하자. 
\begin{align}
Z_i=W_i y_i + (1-W_i){\tilde y}_ i, \quad i=1,\dots,n 
\end{align}
아래를 가정하자. <br/><br/>
**(1)** 모든 관측치는 서로 독립이다. <br/>
**(2)** $W$ 는 확률이 $p$인 베르누이 분포에서 생성되었다. 여기에서 $p=\frac{\exp(\gamma+{\bf X}{\boldsymbol\beta})}{\exp(\gamma+{\bf X}{\boldsymbol\beta})+1}$. <br/>
**(3)** ${\bf y}={\bf X}{\boldsymbol \beta} +{\boldsymbol \epsilon}$ 이다. <br/>
**(4)** ${\bf \tilde y}=0$ 이다. 

- 관측한 자료는 $\bf z$이고, 완전한 자료는 $\bf (w,z)$이다. 그리고 ${\boldsymbol\theta}=(\gamma,\boldsymbol{\beta})$이다. 완전한 자료를 모두 관측하였다고 가정하였을 경우 우도함수는 아래와 같이 표현가능하다. 
\begin{align}
L({\boldsymbol\theta})=\prod_{i=1}^{n} f(z_i|w_i)f(w_i)  
\end{align}
여기에서 $f(w_i)$와 $f(z_i|w_i)$는 각각 아래와 같이 표현된다. 
\begin{align}
f(w_i)=\Bigg[\frac{\exp(\gamma+{\bf x}_ i {\boldsymbol\beta})}{\exp(\gamma+{\bf x}_ i{\boldsymbol\beta})+1}\Bigg]^{w_i}\Bigg[1-\frac{\exp(\gamma+{\bf x}_ i{\boldsymbol\beta})}{\exp(\gamma+{\bf x}_ i{\boldsymbol\beta})+1}\Bigg]^{1-w_i}
\end{align}
\begin{align}
f(z_i|w_i)=
\Bigg[ \frac{1}{\sigma\sqrt{2\pi}}\exp\left(-\frac{(z_i-{\bf x}_ i {\boldsymbol\beta})^2}{2\sigma^2} \right) \Bigg]^{w_i} \Bigg[\delta(z_i)  \Bigg]^{1-w_i}
\end{align}
그리고 이때 $E(W_i \| z_i)=I(z_i>0)$와 같이 구할 수 있다. 이제부터 아래와 같이 놓고 생각없이 쭉쭉 풀면(=미분하면) 된다. 
\begin{align}
E\big[\log L({\boldsymbol\theta}) \| {\bf z} \big]
=\sum_{i=1}^{n}\Bigg(E\big[\log f(z_i|w_i) \| z_i \big]+ E\big[\log f(w_i) \| z_i \big]\Bigg)
\end{align}

- $\gamma$로 먼저 미분해보자. 
\begin{align}
\frac{\partial}{\partial\gamma}E\big[\log f(z_i|w_i) \| z_i \big]=0 
\end{align}
\begin{align}
\frac{\partial}{\partial\gamma}E\big[\log f(w_i) \| z_i \big] 
=I(z_i>0)\frac{1}{e^{\gamma+{\bf x}_ i {\boldsymbol\beta}}+1}+ I(z_i=0)\frac{-e^{\gamma+{\bf x}_ i {\boldsymbol \beta}}}{e^{\gamma+{\bf x}_ i {\boldsymbol\beta}}+1}
\end{align}
따라서
\begin{align}
\frac{\partial}{\partial\gamma}E\big[\log L({\boldsymbol\theta}) \| {\bf z} \big]=0 
\end{align}
을 만족하는 방정식을 풀면 
\begin{align}
m = \sum_{i \in \\{i:z_i=0\\}} e^{\gamma+{\bf x}_ i \boldsymbol\beta}
\end{align}
이 된다. 여기에서 $m=\sum_{i=1}^{n} I(z_i>0)$ 이다. 이걸 $\gamma$에 대하여 정리하면 아래와 같이 된다. 
\begin{align}
\gamma=\log \Bigg(\frac{m}{\sum_{i \in \\{i:z_i=0\\}} e^{ {\bf x}_ i \boldsymbol\beta}}\Bigg) 
\end{align}

- $\boldsymbol\beta$로 미분해보자. (*벡터미분이 생소하다고 $\beta_j$로 미분할생각하면 진짜 엄청난 계산이 기다린다. 개고생만 하다가 결국에는 반드시 공책을 찢게 될 것이다. 방금 공책찢고와서 너무 한이 맺혀서 쓰는 글이다. 벡터나 행렬에서 **엘리먼트-와이즈 연산** 은 남들이 해놓은거 이해하기는 쉽지만 본인이 하려면 엄청 힘들고 특히 머리가 겁나 좋아야 한다는걸 깨달았다. 이인석 교수님 급으로 머리 돌아가는거 아니면 그냥 포기하는게 낫다. 잘 생각해보면 애초에 **엘리먼트-와이즈** 연산을 잘한다는 것은 벡터미분의 다양한 공식(?)들을 다 바로바로 유도하면서 쓴다는 소리이니 이것이 쉬울리가 없다. 다시는 이렇게 풀 생각하지 말자. 머리 나쁘면 그냥 미리 정리해둔 공식 떠올리면서 벡터미분하는게 정신건강에 좋다.*)
\begin{align}
\frac{\partial}{\partial\boldsymbol\beta}E\big[\log f(z_i|w_i) \| z_i \big]=I(z_i>0)\frac{(z_i-{\bf x}_ i \boldsymbol\beta){\bf x}_ i'}{\sigma^2} = I(z_i>0) \frac{ {\bf x}_ i' z_i - {\bf x}_ i' {\bf x}_ i \boldsymbol\beta}{\sigma^2}
\end{align}
여기에서 
\begin{align}
\sum_{i=1}^{n} {\bf x}_ i' {\bf x}_ i = {\bf X}'{\bf X}, \quad \sum_{i=1}^{n} {\bf x}_ i' z_ i = {\bf X}'{\bf z}
\end{align}
임을 이용하면 아래와 같이 정리할 수 있다. 
\begin{align}
\sum_{i=1}^{n}\left( \frac{\partial}{\partial\boldsymbol\beta}E\big[\log f(z_i|w_i) \| z_i \big]\right)
=\frac{1}{\sigma^2}\bf \Bigg(\tilde X' z -\tilde X'\tilde X {\boldsymbom\beta}  \Bigg)
\end{align}


---
이제 $\beta_j$로 미분해보자. 
\begin{align}
\frac{\partial}{\partial\beta_j}E\big[\log f(z_i|w_i) \| z_i \big]=I(z_i>0)\frac{(z_i-{\bf x}_ i \boldsymbol\beta)x_{ij}}{\sigma^2}
\end{align}
\begin{align}
\frac{\partial}{\partial\beta_j}E\big[\log f(w_i) \| z_i \big] 
=I(z_i>0)\frac{x_{ij}}{e^{\gamma+{\bf x}_ i {\boldsymbol\beta}}+1}+ I(z_i=0)\frac{-e^{\gamma+{\bf x}_ i {\boldsymbol \beta}} x_{ij}}{e^{\gamma+{\bf x}_ i {\boldsymbol\beta}}+1}
\end{align}
따라서
\begin{align}
\frac{\partial}{\partial\beta_j}E\big[\log L({\boldsymbol\theta}) \| {\bf z} \big]=0 
\end{align}
을 만족하는 방정식은 아래와 같이 정리된다. 
---
