---
title: (리뷰) RSSA
layout: post
---

### About this doc
- 이 포스트는 아래논문의 리뷰이다. <br/>
[1] Golyandina, N., Korobeynikov, A., Shlemov, A., \& Usevich, K. (2013). Multivariate and 2D extensions of singular spectrum analysis with the Rssa package. arXiv preprint arXiv:1309.5050.

### General scheme of SSA 
- Th SSA-like algorithms decompose the given data ${\mathbb X}$ into a sum of different components: 
\begin{align}
{\mathbb X}={\mathbb X}_ 1 + \dots + {\mathbb X}_ m
\end{align}

- The input data ${\mathbb X}$ can be a time series, a multivariate time series, or a digital image. 

- Thus each componets, i.e., ${\mathbb X}_ 1, \dots, {\mathbb X}_ m$, should be also a time series, a multivariate time series, or a digital image. Note that ${\mathbb X}_ 1, \dots, {\mathbb X}_ m$ have same dimension with ${\mathbb X}$.

- The algorithm is divided into four steps.
	1. **(Embedding)** Make trajectory matrix and denote it ${\bf X}={\cal T}(\mathbb{X})$. 
	2. **(SVD)** Alppy SVD (or other method) to ${\bf X}$ and express ${\bf X}$ as ${\bf X}={\bf X}_ 1+ {\bf X}_ 2+\dots +{\bf X}_ d$. In here, $d=\max\\{j:\lambda_j>0\\}$. 
	3. **(Grouping)** Grouping each component, i.e., partitions $\\{1,2,\dots,d\\}$ to $m < d$ disjoint subsets $I_1,\dots,I_m$. Thus ${\bf X}$ can be expressed as ${\bf X}={\bf X}_ {I_1}+\dots+{\bf X}_ {I_m}$. The grouping with $I_j=\\{j\\}$ is called elementary. 
	4. **(Reconstruction)** Express ${\mathbb X}$ as ${\mathbb X}=\tilde{\mathbb X}_ 1 + \dots \tilde{\mathbb X}_ m$. 

### Step 1. Embedding (detail)

- Suppose that ${\mathbb X}=(x_1,\dots,x_{100})^T$. Choose the length of window as 5, i.e., $L=5$. Then ${\cal T}$ maps $\mathbb{R}^{100}$ to space of Hankel matrix with dimension $5\times 96$. That is
\begin{align}
{\cal T}(\mathbb{X})=\begin{pmatrix} 
x_1& x_2& x_3& \dots & x_{96}\\\\ \\
x_2& x_3& x_4& \dots & x_{97}\\\\ \\
x_3& x_4& x_5& \dots & x_{98}\\\\ \\
x_4& x_5& x_6& \dots & x_{99}\\\\ \\
x_5& x_6& x_7& \dots & x_{100}
\end{pmatrix}.
\end{align}

- Donote ${\cal M}_ {p,q}$ the set of $p\times q$ real matrix. Let ${\cal M}$ be the linear space of all possible ${\boldsymbol X}$. Then ${\cal T}$ is a one-to-one mapping from ${\cal M}$ to ${\cal M}_ {5,96}^{(H)}\subset {\cal M}_ {5, 96}$, where ${\cal M}_ {5,96}^{(H)}$ is the set of matrices with Hankel-like structure. In target paper, denote $K$ as $N-L+1=96$ where $N$ is length of data. 

- Let ${\cal A}_ k$ be the set of indices such that 
\begin{align}
\forall (i,j) \in {\cal A}_ k : \quad {\bf X}_ {ij}=x_k.
\end{align}
Thus, ${\cal A}_ 4 =\\{(4,1),(3,2),(2,3),(1,4)\\}$ in above Example. Note that ${\cal A}_ k$ consists of the positions of the elements on the k-th anti-diagonal of ${\bf X}$.

- Let $\mathbb{E}_ k=E_k\in {\cal M}$ be the object with the $k$th element equal to 1 and all the other elements equal to zero. Then 
\begin{align}
{\cal A}_ k=\\{(i,j): \big({\cal T}(\mathbb{E}_ k)\big)_ {ij}=1\\}
\end{align}
where $\big({\cal T}(\mathbb{E}_ k)\big)_ {ij}$ is $(i,j)$-th element of ${\cal T}(\mathbb{E}_ k)$. 

### Step 4. Reconstruction (detail)

- Note that the 
\begin{align}
{\bf X}={\cal T}({\mathbb X})
\end{align}
is in ${\cal M}_ {L,K}^{(H)}$ but $\hat{\bf X}_ 1, \dots , \hat{\bf X}_ m$ such that 
\begin{align}
{\bf X}={\bf X}_ {I_1}+\dots+{\bf X}_ {I_m}=\hat{\bf X}_ 1+\dots+\hat{\bf X}_ m
\end{align}
is not in ${\cal M}_ {L,K}^{(H)}$. The components of ${\bf X}$, i.e., $\hat{\bf X}_ 1, \dots , \hat{\bf X}_ m$, reside on ${\cal M}_ {L,K}$. 

- Thus we need to introduce something new transform which maps element of ${\cal M}_ {L,K}$ to ${\cal M}_ {L,K}^{(H)}$. To do this, they introduce a matrix $\Pi^{(H)}$ in [1]. Denote $\Pi^{(H)}:{\cal M}_ {L,K} \to {\cal M}_ {L,K}^{(H)}$ the orthogonal projection on ${\cal M}_ {L,K}^{(H)}$ in Frobenius norm. It can transfer the element of ${\cal M}_ {L,K}$ to space of ${\cal M}_ {L,K}^{(H)}$. Thus I interprete $\Pi^{(H)}$ as hat-matrix which project ${\bf X}_ 1, \dots, \hat{\bf X}_ m$ onto space of ${\cal M}_ {L,K}^{(H)}$. 

- Thus basic process of reconstruction is as follows: (1) for all $k=1,\dots, m$, get ${\bf \tilde X}_ k = \Pi^{(H)}{\bf \hat X}_ k$, (2) apply inverse transform of imbedding in $\bf{\tilde X_k}$, i.e., get 
\begin{align}
\tilde{\mathbb X}_ k={\cal T}^{-1} (\bf{\tilde X_k}).
\end{align}

- Then the question is that how could we properly define $\Pi_ {L,K}^{(H)}$ in fixed $(L,K)$. Denote ${\bf Y}$ arbitrary matrix which reside on ${\cal M}_ {L,K}$. Simply, just consider ${\bf Y}$ as ${\bf \hat X}_ 1$. Then our goal becomes more clear: we want to find ${\mathbb Y}\in {\cal M}$ such that 
\begin{align}
\\|{\bf Y}-{\cal T}(\tilde\mathbb{Y}) \\|_ F. 
\end{align}
Thus we choose $\Pi^(H)$ which minimize 
\begin{align}
\\|{\bf Y}-{\cal T}({\cal T}^{-1}\Pi^{(H)}Y) \\|_ F. 
\end{align}
