---
title: (정리) MLE의 점근적 성질 
layout: post
---

### About this doc

- 이번 포스팅에서는 MLE의 점근적 성질에 대하여 다룬다. 

- 참고한 교재는 김우철의 수리통계학, 호그 5판이다. 


### MME를 왜 안 쓰는가? 

- 보통 MLE를 설명하기 전에 적률추정량(method of moments estimator, MME)이란걸 배운다. 적률추정량이란 모집단의 적률의 추정할때 표본적률을 사용한다는 단순한 추정량이다. 이를 이용하면 우리가 관심있는 모수를 적률의 함수로 표현하기만 하면(내 생각엔 앵간한 모수는 거의 다 적률로 표현할 수는 있을 것 같다) 바로 추정량을 얻을 수 있다. 가령 우리가 어떤 확률변수 $X$의 평균과 분산에 관심있다고 하자. 그런데 평균은 일차적률 그 자체이고 분산은 이차적률에서 일차적률제곱을 뺀 형태로 표현가능하다. 즉 
\begin{align}
V(X)=EX^2-(EX)^2 
\end{align}
와 같이 표현할 수 있다. 따라서 적률추정법을 이용하면 평균과 분산에 대한 estimator를 아래와 같이 구할 수 있다. 
\begin{align}
\mbox{estimator of }EX=&\bar{X} \\\\ \\
\mbox{estimator of }V(X)=&\frac{1}{n}\sum_{i=1}^{n}X^2-\bar{X}^2.
\end{align}
MME는 정말 구하기 쉽다. MLE를 구하기 위해서는 우도함수를 구해서 미분을 하다보면 MME가 얼마나 쉽게 얻어질 수 있는지 체감될 것이다. 

- 적률추정량의 또 다른 장점은 LLN(law of large number)덕에 일치성을 보이기가 매우 쉽다는 것이다. 예를 들어 분산의 MME의 경우 $E\|X^2\|<\infty$임을 가정하기만 하면 쉽게 
\begin{align}
\frac{1}{n}\sum_{i=1}^{n}X^2-\bar{X}^2 \quad \overset{p}{\operatorname \rightarrow} \quad V(X)
\end{align}
임을 보일 수 있다. 이따가 보일 MLE의 일치성을 증명하는 과정을 따라가다 보면 이것이 얼마나 사기적인 장점인지 체감할 것이다. 

- 이러한 장점들에도 불구하고 (엄청 쉽게 estimator를 얻을 수 있는데, 심지어 얻어진 estimator가 거의 반자동으로 일치추정량임) MME를 쓰지 않는 이유는 무엇일까? 처음에는 MME가 유일하지 않기 때문이라 생각했다. (왜 MME가 유일하지 않은지 모르겠다면 포아송분포를 생각해보자.) 그런데 MLE도 유일하지 않은 경우가 있다. (김우철 수리통계 p.265) 그래서 나름대로 MLE를 쓰는 이유를 생각해봤는데 다음과 같은 이유들이 있을것 같다. **(1)** 점근적 성질에 대한 이론적 토대가 탄탄하다. (피셔의 정보량등..) **(2)** LSE, 베이즈추정량 같은 estimator도 MLE와 밀접한 연관이 있다. **(3)** EM-알고리즘등 매우 유용한 응용들이 MLE를 중심으로 개발되어 있다(즉 MLE가 이론적으로 탄탄해서 이를 활용한 다양한 방법을 개발하기 쉬운모양). 

- 이런 이유로 이제 MME는 손절하고 MLE로 넘어가자. 

### MLE의 일치성 

- MLE의 일치성을 만족하기 위해서 어떠한 조건이 필요한지 직관적으로 생각해보자. 우선 아래와 같은 함수를 가정하자. 
\begin{align}
l(\theta; {\boldsymbol X}):=l(\theta; X_1,\dots,X_n)=\sum_{i=1}^{n}\log f(X_i;\theta)
\end{align}
일반적인 likelihood와 다르게 이것은 관측치 $x_1,\dots,x_n$가 아니라 랜덤표본 $X_1,\dots,X_n$에 의해서 정의된다는 점이다. 이를 강조하기 위해서 ${\boldsymbol X}$를 따로 표시하였다. (김우철 교수님 교재 p.278에서는 일반적인 likelihood $l(\theta)$와 구분하기 위하여 $l_n(\theta)$라는 기호를 썼다.) 아무튼 이 함수를 정의하면 $\theta$의 MLE는 아래와 같이 정의할 수 있다.
\begin{align}
\mbox{MLE of }\theta = \hat{\theta}^{MLE}=\underset{\theta \in \Theta}{\operatorname{ argmax} }~l(\theta;{\boldsymbol X}).
\end{align}
그런데 아래들 자체를 하나의 랜덤변수로 볼 수 있다. 
\begin{align}
\log f(X_1;\theta), \dots, \log f(X_n;\theta). 
\end{align}
이런논리면 $l(\theta;\{\boldsymbol X})$는 $n$개의 서로독립이고 동일한 분포를 따르는 확률변수의 합으로 볼 수 있다. 따라서 LLN을 쓰면 아래가 성립한다. 
\begin{align}
\frac{1}{n} \sum_{i=1}^{n} \log f(X_i;\theta)\quad \overset{p}{\operatorname \rightarrow} \quad E \log f(X_1;\theta).
\end{align}
따라서 다음과 같은 추측을 할 수 있다. 
\begin{align}
\underset{\theta \in \Theta}{\operatorname{ argmax} }\frac{1}{n}\sum_{i=1}^{n} \log f(X_i; \theta)\quad \overset{p}{\operatorname \rightarrow} \quad \underset{\theta \in \Theta}{\operatorname{argmax} } E \log f(X_1;\theta)~?
\end{align}

- 이 추측에서 좌변은 아래와 같이 정리된다. 
\begin{align}
\underset{\theta \in \Theta}{\operatorname{ argmax} }\frac{1}{n}\sum_{i=1}^{n} \log f(X_i; \theta)=\underset{\theta \in \Theta}{\operatorname{ argmax} }\sum_{i=1}^{n} \log f(X_i; \theta)=\hat{\theta}^{MLE}
\end{align}

- 이 추측에서 우변은 아래와 같이 정리된다. 
\begin{align}
\underset{\theta \in \Theta}{\operatorname{argmax} }E \log f(X_1;\theta)= \theta
\end{align}
이게 성립하는 이유는 쿨백라이블러 괴리도의 성질 중 아래를 이용한 것이다. 
\begin{align}
KL(\theta,\theta^*):=-E\big( \log\frac{f(X;\theta)}{f(X;\theta^*)} \big)=0 
\end{align}

- 따라서 우리의 추측은 아래로 요약된다. 
\begin{align}
\hat{\theta}^{MLE} \overset{p}{\rightarrow} \theta
\end{align}
