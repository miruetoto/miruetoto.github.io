---
title: (정리) 베이지안 
layout: post
---

### About this doc

- 이 문서에서는 베이지안 대하여 다룬다. 베이지안 추론법이 아니라 베이지안을 다룬다고 한 이유는 그들의 방법을 이해하기 위함이 아니라 그들의 철학 혹은 사고방식을 이해하기 위함이다. 

- 수준은 학부수준, 교재는 예전에 내가 공부했던 Hogg 5판. 


### 사후분포를 구하는 두 가지 방법

- 가볍게 예제로 시작하자. $x_1,\dots,x_n$는 모수가 $\lambda$인 포아송분포에서 추출한 ***i.i.d.*** 샘플이라고 하자. (좀더 엄밀하게 쓰면 $X$는 모수가 $\lambda$인 포아송분포를 따르고 $x_1,\dots,x_n$ 은 $X$의 ***i.i.d. realization*** 이라고 말해야 한다.) 그리고 $\lambda$는 확률변수 $\Lambda$의 ***realization*** 이며 확률변수 $\Lambda$는 모수가 $\alpha$, $\beta$ 인 감마분포를 따른다고 하자. 이러한 상황을 보통 간단하게 아래와 같이 표현한다. 
\begin{align}
\begin{cases}
\lambda \sim \Gamma(\alpha,\beta) \\\\ \\
x_1,\dots,x_n \sim ~ i.i.d.~ P(\lambda)
\end{cases}
\end{align}

- 여기에서 많은 사람들이 헷갈려하는 점을 짚고 넘어가겠다. 혼돈은 $\lambda$를 파라메터로 부르는 것이 맞나? 라는 의문에서 시작된다. 이미 언급했지만  $\lambda$는 파라메터가 아니라 확률변수의 realization 을 의미하고 오히려 $\alpha,\beta$ 를 파라메터라고 부르는 것이 맞다고 생각한다. 하지만 이런 관점은 빈도주의적 관점이다. 베이지안들은 $\alpha,\beta$ 를 하이퍼파라메터라고 하고 $\theta$를 그냥 파라메터라고 부른다. 그리고 베이지안들은 파라메터를 랜덤변수의 realization 으로 생각한다. 정리하면 아래와 같다. 빈도주의자의 입장에서 정리하였다. (괄호는 베이지안들의 입장) 
  - $\alpha$, $\beta$ : parameter (hyper-parameter)
  - $\lambda$ : realization of $\Lambda$ (parameter)
  - $x_1,\dots,x_n$: ***(i.i.d.)*** realizations of $X$. 


- 아무튼 이러한 상황에서 우리가 구할것은 $f(\lambda \| x_1,\dots,x_n)$ 이다. 이를 $\lambda$ 의 사후분포라고 한다. 내친김에 관련된 용어들을 정리하여 보자. 
  - $f(\lambda)$ : 사전분포, ***prior***.
  - $f(x_1,\dots,x_n\|\lambda)$ : 우도, ***likelihood***. 
  - $f(\lambda \| x_1,\dots,x_n)$ : 사후분포, ***posterior***.
  - $f(x_1,\dots,x_n,\lambda)$: 결합분포, ***joint***. 
  - $f(x_1,\dots,x_n)$: 주변분포, ***marginal***. 
  
- 여기에서 또 사소한 헷갈림이 발생한다. 일반적으로 빈도주의자들이 말하는 우도함수는 $L(\lambda)=f(x_1,\dots,x_n ; \lambda)$ 와 같이 표현한다. 이때 빈도주의자들은 $f(x_1,\dots,x_n ; \lambda)$ 대신에 간략하게 $f(x_1,\dots,x_n)$ 와 같이 쓰기도 하는데 이는 베이지안들이 생각하는 마지널의 표기법과 겹친다. 즉 
\begin{align}
f(x_1,\dots,x_n)
\end{align}
이 빈도주의자들에게는 우도함수로 보이고 베이지안에게는 마지날로 보인다. 베이지안들은 절대로 우도함수를 위와같이 쓰지 않는다. 철저하게 
\begin{align}
f(x_1,\dots,x_n \| \lambda)
\end{align}
와 같이 쓴다. 대신 빈도주의자들은 우도함수를 $f(x_1,\dots,x_n ; \lambda)$ 혹은 $L(\lambda)$ 혹은 $f(x_1,\dots,x_n)$ 와 같이 쓴다. 

- 다시 우리의 문제로 돌아와서 사후분포 즉 $f(\lambda \| x_1,\dots,x_n)$ 을 구하는 방법에 대하여 알아보자. 

- **(풀이 1: 미련함)** 정석적인 풀이를 해보자. 아마 좀 어려울거다. 조건부 ***pdf*** 의 정의에 의해서 아래식이 성립한다. 
\begin{align}
f(\lambda \| x_1,\dots,x_n) = \frac{f(x_1,\dots,x_n,\lambda)}{f(x_1,\dots,x_n)}
\end{align}
따라서 위를 계산하려면 $f(x_1,\dots,x_n,\lambda)$ 와 $f(x_1,\dots,x_n)$ 를 각각 계산하면 된다. 여기에서 $f(x_1,\dots,x_n,\lambda)$는 아래와 같이 쓸 수 있다. 
\begin{align}
f(x_1,\dots,x_n,\lambda)= \left( \prod_{i=1}^{n} \frac{\lambda^{x_i} e^{-\lambda} }{x_i !} \right) \frac{\lambda^{\alpha-1}e^{-\lambda/\beta} }{\Gamma(\alpha)\beta^{\alpha} }
\end{align}
이제 $f(x_1,\dots,x_n)$을 계산하면 되는데 이는 $f(x_1,\dots,x_n,\lambda)$을 적분해서 얻을 수 있다. 즉
\begin{align}
f(x_1,\dots,x_n)=\int f(x_1,\dots,x_n,\lambda) d\lambda 
\end{align}
을 계산하면 된다. 적분을 항상 손으로 계산할 수 있을지는 모르지만 위의 경우는 계산할 수 있다. 아무튼 계산해보면 아래와 같다. (이번 한번만 계산할것이다) 
\begin{align}
f(x_1,\dots,x_n)&=\int f(x_1,\dots,x_n,\lambda) d\lambda  \\\\ \\
&= \int_0^{\infty} \left( \prod_{i=1}^{n} \frac{\lambda^{x_i} e^{-\lambda} }{x_i !} \right) \frac{\lambda^{\alpha-1}e^{-\lambda/\beta} }{\Gamma(\alpha)\beta^{\alpha} } d\lambda \\\\ \\
&= \frac{1}{\prod x_i! \Gamma(\alpha) \beta^{\alpha} } \int_0^{\infty}\lambda^{\sum x_i + \alpha-1 } e^{-\lambda \big(n+\frac{1}{\beta}\big)} d\lambda
\end{align}
감마함수의 정의에 따라서 아래식이 성립한다. 
\begin{align}
\Gamma(n):=\int_{0}^{\infty} y^{n-1} e^{-y} dy 
\end{align}
요걸 이용하면 아래를 얻을 수 있다. (A4 용지 반페이지 정도 투자해서 치환적분해보면 풀린다) 
\begin{align}
f(x_1,\dots,x_n)
=\frac{\Gamma(\sum x_i+\alpha)}{x_1! \dots x_n! \Gamma(\alpha) \beta^{\alpha} \big(n+\frac{1}{\beta}\big)^{\sum x_i+\alpha} }
\end{align}
여기까지 결과를 정리하면 아래와 같다. 
\begin{align}
\begin{cases}
f(x_1,\dots,x_n,\lambda)= \left( \prod_{i=1}^{n} \frac{\lambda^{x_i} e^{-\lambda} }{x_i !} \right) \frac{\lambda^{\alpha-1}e^{-\lambda/\beta} }{\Gamma(\alpha)\beta^{\alpha} } \\\\ \\
f(x_1,\dots,x_n)
=\frac{\Gamma(\sum x_i+\alpha)}{\prod x_i! \Gamma(\alpha) \beta^{\alpha} \big(n+\frac{1}{\beta}\big)^{\sum x_i+\alpha} }
\end{cases}
\end{align}
이제 (드디어) 아래식만 계산하면 풀이가 끝난다. 
\begin{align}
f(\lambda \| x_1,\dots,x_n) = \frac{f(x_1,\dots,x_n,\lambda)}{f(x_1,\dots,x_n)}
\end{align}
계산해보면 아래와 같이 정리된다. 
\begin{align}
f(\lambda \| x_1,\dots,x_n)=\frac{\lambda^{\sum x_i \alpha-1} e^{-\lambda\big(n+\frac{1}{\beta}\big)} }{\Gamma(\sum x_i+\alpha)\big(n+\frac{1}{\beta} \big)^{- \sum x_i - \alpha} }
\end{align}
따라서 $\lambda$의 사후분포는 모수가 $\big(\sum x_i+\alpha,\big(n+\frac{1}{\beta}\big)^{-1} \big) $ 인 감마분포를 따른다. 

- **(풀이 2: 똑똑함)** : 좀 더 쉽게 푸는 방법을 알려주겠다. 이는 아래와 같은 **베이즈정리**를 이용하는 것이다. 
\begin{align}
f(\lambda | x_1,\dots,x_n) \propto f(x_1,\dots,x_n | \lambda) f(\lambda) 
\end{align}
위의 식을 아래와 같이 외우는 사람도 있다. 
\begin{align}
\mbox{ posterior } \propto \mbox{ likelihood } \times \mbox{ prior } 
\end{align}
뭐 저렇게만 보면 $\mbox{likelihood} \times \mbox{prior}$ 가 엄청난 의미를 가지고 있는것 같은데 그냥 $({\boldsymbol x},\theta)$의 ***joint pdf*** 란 의미다. 여기에서 ${\boldsymbol x}=(x_1,\dot,x_n)'$ 이다. 
참고로 위에서 $\propto$ 의 의미는 잘 아는것처럼 **비례**라는 의미이다. 즉 $f(y) \propto g(y)$ 라는 의미는 적당한 상수 $k$ 가 존재하여 $f(y)=k \times g(y)$ 가 성립한다는 의미이다. 이때 $k$는 진짜 $0.2$ 같은 상수란 의미가 아니라 $y$와 상관없는 어떤 수식이라는 의미이다. 베이즈정리의 증명은 그렇게 어렵지 않다. 
\begin{align}
f(\lambda | x_1,\dots,x_n) =& \frac{f(\lambda,x_1,\dots,x_n)}{f(x_1,\dots,x_n)} 
= \frac{f(\lambda)f(x_1,\dots,x_n | \lambda)}{f(x_1,\dots,x_n)} \\\\ \\
=& \mbox{const with regard to $\lambda$} \times f(x_1,\dots,x_n | \lambda) f(\lambda) 
\end{align}
결국 아래를 위우면 된다. 
\begin{align}
\mbox{ posterior } =& \mbox{ inverse of marginal } \times \mbox{ joint } \\\\ \\
=& \mbox{ inverse of marginal } \times \mbox{ likelihood } \times \mbox{ prior } \\\\ \\
\propto& \mbox{ likelihood } \times \mbox{ prior }
\end{align}
베이즈정리를 잘 생각해보면 사후분포는 다음의 과정으로 구할 수 있음을 알 수 있다. **(1)** join를($=\mbox{likelihood} \times \mbox{prior}$)를 쓰고 **(2)** $\{x_1,\dots,x_n\}$ 와 하이퍼파라메터에 관련된텀은 모두 무시하고 $\lambda$와 관련된텀만 남겨 간추린모양을 만든 뒤 **(3)** 간추린모양을 보고 ***pdf*** 를 역추론해 때려맞춘다. 위의 예제로 치면 아래와 같이 joint를 쓴뒤 
\begin{align}
f(x_1,\dots,x_n,\lambda)= \left( \prod_{i=1}^{n} \frac{\lambda^{x_i} e^{-\lambda} }{x_i !} \right) \frac{\lambda^{\alpha-1}e^{-\lambda/\beta} }{\Gamma(\alpha)\beta^{\alpha} }
\end{align}
정리해서 아래와 같이 **간추린모양** 를 만든다. 
\begin{align}
f(x_1,\dots,x_n,\lambda) \propto \lambda^{\sum x_i+\alpha -1} e^{-\lambda\big(n+\frac{1}{\beta}\big)}
\end{align}
이는 모수가 $\big(\sum x_i+\alpha,\big(n+\frac{1}{\beta}\big)^{-1} \big) $  인 감마분포의 **간추린모양**이다. 따라서 사후분포는 모수가 $\big(\sum x_i+\alpha,\big(n+\frac{1}{\beta}\big)^{-1} \big) $ 인 감마분포이다. 

- 켤레를 외워두면 편하다. <br/>
|parameter / (which is random realization in Bayesian) | Prior distribution | Posterior distribution |
|:---------------------------------------------------|:-------------------|:-----------------------|
|$\lambda$ in Poisson | Gamma | Gamma |

### Bayes estimator

- 여기서는 베이즈추정량에 대하여 다룬다. 이를 정의하기 위해서 ***loss, MSE, risk, Bayes risk*** 와 같은 용어들이 나온다. 참고로 이 챕터는 엄청 헷갈린다. 정신 똑바로 차리자. 우선 가장 헷갈리는 MSE부터 정리하자. 

- MSE는 크게 2가지 부류로 나뉜다. (https://en.wikipedia.org/wiki/Mean_squared_error)
\begin{align}
\begin{cases}
\sum_{i=1}^{n}(y_i-\hat{y}_ i)^2 \\\\ \\
\mathbb{E}(\lambda-\hat\lambda)^2
\end{cases}
\end{align}
위의식은 predictor에 대한 MSE의 정의이고 아래식은 estimator에 대한 MSE의 정의이다. 다소 헷갈리지만 그냥 **회귀식정의**와 **수통식정의** 2개 있다고 생각하는 것이 속 편하다.  

- 회귀식 MSE는 잔차 즉 $e_i=y_i-\hat{y}_ i$의 average이다. 수통식 MSE는 손실함수 $(\lambda-\hat{\lambda})^2$ 의 mean이다. 여기에서 미치겠는것은 머신러닝쪽에서는 회귀식 MSE 그 자체를 손실함수라고 표현하기도 한다는 점이다. (요건 최적화쪽에서 따온 용어인데 최적화하는 사람들은 objective function 을 그냥 손실함수라고 부른다) 우선 여기서는 이런용어는 무시하자. 그리고 우리는 수통식으로 정의한 MSE에만 관심을 가지자. 

- 수통식정의의 황당한 점은 또 빈도주의자와 베이지안이 생각하는 정의가 다르다는 것이다. 우선 빈도주의자들은 MSE를 아래와 같이 정의한다. (https://en.wikipedia.org/wiki/Loss_function#Expected_loss)
\begin{align}
\mathbb{E}(\lambda-\hat\lambda)^2:=\int \dots \int (\lambda-\hat{\lambda})^2 f(x_1,\dots,x_n ; \lambda) dx_1,\dots dx_n 
\end{align}
반면에 베이지안은 MSE를 아래와 같이 생각한다. (https://en.wikipedia.org/wiki/Bayes_estimator#cite_note-1)
\begin{align}
\mathbb{E}(\lambda-\hat\lambda)^2:=\int \dots \int \int (\lambda-\hat{\lambda})^2 f(\lambda,x_1,\dots,x_n) dx_1,\dots dx_n d\lambda
\end{align}
즉 빈도주의자들은 우도함수에 대한 평균을 취하고 베이지안은 조인트에 대하여 평균을 취한다. 그리고 빈도주의자와 베이지안 모두 이를 리스크라고 부른다. 이건 그래도 이해할만 한것이 빈도주의자들에게 $\lambda$는 단순히 파라메터이지만 베이지안에게 $\lambda$ 는 단순한 파라메터가 아니라 랜덤변수의 realization 이기 때문에 $\lambda$ 에 대한 적분을 한번 더하는 것이 타당하다. 이는 빈도주의자들의 관점을 베이지안에서 확장한것이라 생각할 수 있다. 

- 베이지안들은 리스크이외에 베이지리스크라는 것을 새롭게 정의한다. 이것은 아래와 같이 정의한다. 
\begin{align}
\mbox{Bayes risk}:=\int (\lambda-\hat{\lambda})^2 f(\lambda \|x_1,\dots,x_n)d\lambda
\end{align}
정리하면 아래와 같이 된다. 
\begin{align}
\begin{cases}
\mbox{(빈도주의) MSE(=risk)}&= \int \dots \int (\lambda-\hat{\lambda})^2 f(x_1,\dots,x_n ; \lambda) dx_1,\dots dx_n \\\\ \\
\mbox{(베이지안) MSE(=risk)}&= \int \dots \int \int (\lambda-\hat{\lambda})^2 f(\lambda,x_1,\dots,x_n) dx_1,\dots dx_n d\lambda \\\\ \\
\mbox{(베이지안) Bayes risk}&= \int (\lambda-\hat{\lambda})^2 f(\lambda \|x_1,\dots,x_n)d\lambda 
\end{cases}
\end{align}




