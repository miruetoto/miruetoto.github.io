---
title: (정리) 베이지안 
layout: post
---

### About this doc

- 이 문서에서는 베이지안 대하여 다룬다. 베이지안 추론법이 아니라 베이지안을 다룬다고 한 이유는 그들의 방법을 이해하기 위함이 아니라 그들의 철학 혹은 사고방식을 이해하기 위함이다. 

- 수준은 학부수준, 교재는 예전에 내가 공부했던 Hogg 5판. 


### 사후분포를 구하는 두 가지 방법

- 가볍게 예제로 시작하자. $x_1,\dots,x_n$는 모수가 $\lambda$인 포아송분포에서 추출한 ***i.i.d.*** 샘플이라고 하자. (좀더 엄밀하게 쓰면 $X$는 모수가 $\lambda$인 포아송분포를 따르고 $x_1,\dots,x_n$ 은 $X$의 ***i.i.d. realization*** 이라고 말해야 한다.) 그리고 $\lambda$는 확률변수 $\Lambda$의 ***realization*** 이며 확률변수 $\Lambda$는 모수가 $\alpha$, $\beta$ 인 감마분포를 따른다고 하자. 이러한 상황을 보통 간단하게 아래와 같이 표현한다. 
\begin{align}
\begin{cases}
\lambda \sim \Gamma(\alpha,\beta) \\\\ \\
x_1,\dots,x_n \sim ~ i.i.d.~ P(\lambda)
\end{cases}
\end{align}

- 위와 같은 간략한 표현은 혼동의 여지가 있다. 이미 언급했지만 정확하게는 위에서 $\alpha,\beta$는 그냥 파라메터이고 $\lambda$는 파라메터가 아니라 확률변수의 realization을 의미한다($\alpha,\beta$를 ***hyper-parameter***라고 부르는 사람도 있다). 흔한 착각은 크게 2부류인데 첫번째는 $\lambda$ 를 그냥 일반적인 파라메터라고 생각하는 사람이다. 이런 사람들은 베이지안 방법자체를 이해 못하는 경우라 할 수 있다. 또 다른 부류는 $\lambda$ 자체를 확률변수라고 이해하는 사람이다. 이런 사람들은 베이지안 추벙법을 잘못 이해하고 있는 경우이다(혹은 랜덤변수와 랜덤변수의 realization을 구분 못하든가). 아무튼 정리하면 아래와 같다. 
  - $\alpha$, $\beta$ : (hyper) parameter
  - $\lambda$ : realization of $\Lambda$. 
  - $x_1,\dots,x_n$: ***(i.i.d.)*** realizations of $X$. 

- 아무튼 이러한 상황에서 우리가 구할것은 $f(\lambda \| x_1,\dots,x_n)$ 이다. 이를 $\lambda$ 의 사후분포라고 한다. 이것을 구하는 방법은 호그책에 소개된 바로는 3가지이다. 가장 미련하고 어려운 풀이부터 시작하겠다. 

- **(풀이 1: 미련함)** 정석적인 풀이를 해보자. 아마 좀 어려울거다. 조건부 ***pdf*** 의 정의에 의해서 아래식이 성립한다. 
\begin{align}
f(\lambda \| x_1,\dots,x_n) = \frac{f(x_1,\dots,x_n,\lambda)}{f(x_1,\dots,x_n)}
\end{align}
따라서 위를 계산하려면 $f(x_1,\dots,x_n,\lambda)$ 와 $f(x_1,\dots,x_n)$ 를 각각 계산하면 된다. 여기에서 $f(x_1,\dots,x_n,\lambda)$는 아래와 같이 쓸 수 있다. 
\begin{align}
f(x_1,\dots,x_n,\lambda)= \left( \prod_{i=1}^{n} \frac{\lambda^{x_i} e^{-\lambda} }{x_i !} \right) \frac{\lambda^{\alpha-1}e^{-\lambda/\beta} }{\Gamma(\alpha)\beta^{\alpha} }
\end{align}
이제 $f(x_1,\dots,x_n)$을 계산하면 되는데 이는 $f(x_1,\dots,x_n,\lambda)$을 적분해서 얻을 수 있다. 즉
\begin{align}
f(x_1,\dots,x_n)=\int f(x_1,\dots,x_n,\lambda) d\lambda 
\end{align}
을 계산하면 된다. 적분을 항상 손으로 계산할 수 있을지는 모르지만 위의 경우는 계산할 수 있다. 아무튼 계산해보면 아래와 같다. (이번 한번만 계산할것이다) 
\begin{align}
f(x_1,\dots,x_n)&=\int f(x_1,\dots,x_n,\lambda) d\lambda  \\\\ \\
&= \int_0^{\infty} \left( \prod_{i=1}^{n} \frac{\lambda^{x_i} e^{-\lambda} }{x_i !} \right) \frac{\lambda^{\alpha-1}e^{-\lambda/\beta} }{\Gamma(\alpha)\beta^{\alpha} } d\lambda \\\\ \\
&= \frac{1}{\prod x_i! \Gamma(\alpha) \beta^{\alpha} } \int_0^{\infty}\lambda^{\sum x_i + \alpha-1 } e^{-\lambda \big(n+\frac{1}{\beta}\big)} d\lambda
\end{align}
감마함수의 정의에 따라서 아래식이 성립한다. 
\begin{align}
\Gamma(n):=\int_{0}^{\infty} y^{n-1} e^{-y} dy 
\end{align}
요걸 이용하면 아래를 얻을 수 있다. (A4 용지 반페이지 정도 투자해서 치환적분해보면 풀린다) 
\begin{align}
f(x_1,\dots,x_n)
=\frac{\Gamma(\sum x_i+\alpha)}{x_1! \dots x_n! \Gamma(\alpha) \beta^{\alpha} \big(n+\frac{1}{\beta}\big)^{\sum x_i+\alpha} }
\end{align}
여기까지 결과를 정리하면 아래와 같다. 
\begin{align}
\begin{cases}
f(x_1,\dots,x_n,\lambda)= \left( \prod_{i=1}^{n} \frac{\lambda^{x_i} e^{-\lambda} }{x_i !} \right) \frac{\lambda^{\alpha-1}e^{-\lambda/\beta} }{\Gamma(\alpha)\beta^{\alpha} } \\\\ \\
f(x_1,\dots,x_n)
=\frac{\Gamma(\sum x_i+\alpha)}{\prod x_i! \Gamma(\alpha) \beta^{\alpha} \big(n+\frac{1}{\beta}\big)^{\sum x_i+\alpha} }
\end{cases}
\end{align}
이제 (드디어) 아래식만 계산하면 풀이가 끝난다. 
\begin{align}
f(\lambda \| x_1,\dots,x_n) = \frac{f(x_1,\dots,x_n,\lambda)}{f(x_1,\dots,x_n)}
\end{align}
계산해보면 아래와 같이 정리된다. 
\begin{align}
f(\lambda \| x_1,\dots,x_n)=\frac{\lambda^{\sum x_i \alpha-1} e^{-\lambda\big(n+\frac{1}{\beta}\big)} }{\Gamma(\sum x_i+\alpha)\big(n+\frac{1}{\beta} \big)^{- \sum x_i - \alpha} }
\end{align}
따라서 $\lambda$의 사후분포는 모수가 $\big(\sum x_i+\alpha,\big(n+\frac{1}{\beta}\big)^{-1} \big) $ 인 감마분포를 따른다. 

- **(풀이 2: 똑똑함)** : 좀 더 쉽게 푸는 방법을 알려주겠다. 이는 아래와 같은 **베이즈정리**를 이용하는 것이다. 
\begin{align}
f(\lambda | x_1,\dots,x_n) \propto f(x_1,\dots,x_n | \lambda) f(\lambda) 
\end{align}
위의 식을 아래와 같이 외우는 사람도 있다. 
\begin{align}
\mbox{ posterior } \propto \mbox{ likelihood } \times \mbox{ prior } 
\end{align}
뭐 저렇게만 보면 $\mbox{likelihood} \times \mbox{prior}$ 가 엄청난 의미를 가지고 있는것 같은데 그냥 $({\boldsymbol x},\theta)$의 ***joint pdf*** 란 의미다. 여기에서 ${\boldsymbol x}=(x_1,\dot,x_n)'$ 이다. 
참고로 위에서 $\propto$ 의 의미는 잘 아는것처럼 **비례**라는 의미이다. 즉 $f(y) \propto g(y)$ 라는 의미는 적당한 상수 $k$ 가 존재하여 $f(y)=k \times g(y)$ 가 성립한다는 의미이다. 이때 $k$는 진짜 $0.2$ 같은 상수란 의미가 아니라 $y$와 상관없는 어떤 수식이라는 의미이다. 베이즈정리의 증명은 그렇게 어렵지 않다. 
\begin{align}
f(\lambda | x_1,\dots,x_n) =& \frac{f(\lambda,x_1,\dots,x_n)}{f(x_1,\dots,x_n)} 
= \frac{f(\lambda)f(x_1,\dots,x_n | \lambda)}{f(x_1,\dots,x_n)} \\\\ \\
=& \mbox{const with regard to $\lambda$} \times f(x_1,\dots,x_n | \lambda) f(\lambda) 
\end{align}
결국 아래를 위우면 된다. 
\begin{align}
\mbox{ posterior } =& \mbox{ inverse of marginal } \times \mbox{ joint } \\\\ \\
=& \mbox{ inverse of marginal } \times \mbox{ likelihood } \times \mbox{ prior } \\\\ \\
\propto& \mbox{ likelihood } \times \mbox{ prior }
\end{align}
베이즈정리를 잘 생각해보면 사후분포는 다음의 과정으로 구할 수 있음을 알 수 있다. **(1)** join를($=\mbox{likelihood} \times \mbox{prior}$)를 쓰고 **(2)** $\{x_1,\dots,x_n\}$ 와 하이퍼파라메터에 관련된텀은 모두 무시하고 $\lambda$와 관련된텀만 남겨 간추린모양을 만든 뒤 **(3)** 간추린모양을 보고 ***pdf*** 를 역추론해 때려맞춘다. 위의 예제로 치면 아래와 같이 joint를 쓴뒤 
\begin{align}
f(x_1,\dots,x_n,\lambda)= \left( \prod_{i=1}^{n} \frac{\lambda^{x_i} e^{-\lambda} }{x_i !} \right) \frac{\lambda^{\alpha-1}e^{-\lambda/\beta} }{\Gamma(\alpha)\beta^{\alpha} }
\end{align}
정리해서 아래와 같이 **간추린모양** 를 만든다. 
\begin{align}
f(x_1,\dots,x_n,\lambda) \propto \lambda^{\sum x_i+\alpha -1} e^{-\lambda\big(n+\frac{1}{\beta}\big)}
\end{align}
이는 모수가 $\big(\sum x_i+\alpha,\big(n+\frac{1}{\beta}\big)^{-1} \big) $  인 감마분포의 **간추린모양**이다. 따라서 사후분포는 모수가 $\big(\sum x_i+\alpha,\big(n+\frac{1}{\beta}\big)^{-1} \big) $ 인 감마분포이다. 

- 켤레를 외워두면 편하다. <br/>
parameter (which is random realization in Bayesian) | Prior distribution | Posterior distribution 
----------------------------------------------------|--------------------|------------------------
$\lambda$ in Poisson | Gamma | Gamma 

### Expected loss

- 여기서는 ***loss, MSE, risk, Bayesian risk*** 에 대하여 알아보자. 참고로 이 챕터는 엄청 헷갈린다. 정신 똑바로 차리자. 

#### MSE

- MSE는 크게 2가지 부류로 나뉜다. 
\begin{align}
\begin{cases}
\sum_{i=1}^{n}(y_i-\hat{y}_ i)^2 \\\\ \\
\mathbb{E}(\lambda-\hat\lambda)^2:=\int (\lambda-\hat{\lambda})^2 f(x_1,\dots,x_n ; \lambda) dx
\end{cases}
\end{align}

- 
이중에서 손실함수, 위험함수는 빈도주의자가 즐겨쓰는 표현이며 손실함수와 베이즈위험은 베이지안이 즐겨쓰는 표현이다. 사실 손실함수 위험함수 베이즈위험은 각각 연관성이 있는데 빈도주의자들은 세상에 손실함수, 위험함수만 있다는 마인드라서 요쪽세계에서는 베이즈위험 이라는 용어자체가 없다. 반면에 베이지안들은 위험함수라는 말을 잘 쓰지 않는다. 이쪽은 오히려 손실함수와 베이즈위험함수만 있다는 마인드다. 그래서 엄청 헷갈린다. 정신 또바로 차려야 한다. 

- 우선 양쪽모두에서 이견이 없는 손실함수 $L$을 정의하도록 하자. 
\begin{align}

ㅁㄴㅇㄹ 





