---
layout: post
title: 야매 강화학습
---

### 간단한 그리드 

- $4\times 4$개의 격자가 있는 세계를 상상하자.

- 로봇이 움직일수 있는 16개의 상태를 아래와 같의 정의하자.<br/><br/>
```
s<-1:16
```
편의상 ${\cal S}:=\\{s[1],\dots,s[16]\\}$이라고 정의하자. 

- 액션을 정의하자. 로봇은 동서남북으로 움직일수 있으므로 4가지 행동을 취할 수 있다.<br/><br/>
```
a<-c("e","w","s","n")
```
편의상 ${\cal A}:=\\{a[1],\dots,a[4]\\}$라고 정의하자. 

- 특정 상태 $s\in {\cal S}$에서 다른 특정 상태 $s'\in {\cal S}$로 이동할 확률을 모두 정의한 매트릭스 $P$를 만들자. 이것을 transition matrix라고 하자. 이때 $P:{\cal S}\rightarrow {\cal S}$이다. 가령 예를 들면 각 상태로 이동할 확률이 모두 동일하다면 $P$를 아래와 같이 설정한다.<br/><br/>
```
P<-rep(1/16,16*16)
dim(P)<-c(16,16)
```
여기에서 $P[s,s']=P(s'|s)$이다. 

- 강화학습은 보통 MDP에서 정의되므로 트랜지션 매트릭스 $P$는 액션을 고려하여 다음과 같은 함수가 되어야 함이 마땅하다.
\begin{align}
P:{\cal S}\times{\cal A} \rightarrow {\cal S}
\end{align}  
즉 이전상태와 이전상태에서의 액션이 기븐되어야 다음상태를 알 수 있다. 따라서 아래와 같이 트랜지션 매트릭스를 정의한다.<br/><br/>
```
P<-rep(0,16*4*16)
dim(P)<-c(16,4,16)
P[1,1,1]<- somevalue
...
P[16,4,16]<- somevalue
```  
여기에서 $P[16,4,16]$은 상태 $s[16]$에서 적당한 액션 $a[4]$를 취했을때 상태 $s[16]$으로 전환될 확률을 의미한다. 그나마 게임은 룰이 명확하기 때문에 $P$를 정의할 수 있다. 이때 $P[s,a,s']=P(s'|s,a)$이다. 
  
- 보상함수에 대하여 정의하자. 가령 예를들면 보상함수를 아래와 같이 정의할 수 있다. 
  1. 상태 $s[1]$혹은 상태 $s[16]$에 도달하면 5점의 점수를 받는다.  

  2. 그리드 밖으로 나가면 -1점의 점수를 얻는다. (그리고 다음상태는 이전상태와 동일하다)  
  
  3. 그외에 경우에는 0점의 점수를 받는다.  


- 보상은 16개의 상태에서 4개의 행동을 하는 경우에서 모두 정의할 수 있다. 따라서 보상 $r(s,a)$은 아래와 같이 쓸 수 있다.<br/><br/>
```
r<-rep(0,16*4)
dim(r)<-c(16,4)
```
그런데 같은상태에서 같은행동을 취해도 다른보상을 줄 수 있다. (될놈될..) 따라서 위에 정의된 $r[s,a]$들은 사실 보상들의 평균이라고 보는 것이 옳은 해석이다. 예컨데 $r[s,a]$는 상태 $s$에서 행동 $a$를 취하였을때 얻으리라 기대되는 평균적인 보상값이다. 보통 $r[s,a]$는 모두 클리어하게 정의할 수 있는데 이것은 우리가 MDP를 가정하기 때문이다. 

- 이때 보상 $r$이 현재상태와 현재상태의액션에 대한 함수이지 다음상태에 대한 함수는 아님을 기억하자. 즉 나중상태의 정보만으로 보상을 완벽히 정의할 수 없다는 의미이다. 예를 들어서 로봇이 $s'=(3,4)$의 위치에 있다고 하자. 1) $s=(2,4)$이고 $a=a[1]$이어서 $s'=(3,4)$가 된 경우에는 보상이 0이다. 2) 하지만 $s=(3,4)$였는데 $a=a[4]$와 같이 되어서 그리드밖으로 튀어나가 다음상태가 $s'=(3,4)$가 된 경우는 보상이 -1이다. 다시 말하면 다음상태 $s'=(3,4)$가 같다고 해서 항상 그 보상이 같은 것은 아니다. 

- 특정 상태 $s \in {\cal S}$가 기븐되었을 경우 특정 액션 $a \in {\cal A}$을 선택할 확률을 정의한 매트릭스 $\pi$를 만들자. 이를 정책이라고 하자. 예를들어 각 상태 16개에서 선택할 수 있는 액션의 확률이 모두 동일한 정책 $\pi_1(a|s)$은 아래와 같이 정의할 수 있다.<br/><br/>
```
pi_1<-rep(1/4,16*4)
dim(pi_1)<-c(16,4) 
```  
이때 $\pi_1[s,a]=\pi_1(a|s)=P(a|s)$이다. 

- 특정 상태 $s \in {\cal S}$에 대하여 정책 $\pi_1(a|s)$가 좋은 정책인지 나쁜 정책인지 평가할 수 있다. 이는 특정상태에서 정책 $\pi_1(a|s)$을 썼을때 얻게되는 보상의 기대값으로 정의할 수 있다. 이런식으로 모든 상태 $s \in {\cal S}$에 대하여 정책 $\pi_1(a|s)$를 썼을때 보상의 기대값을 계산할 수 있는데 이를 정책 $\pi_1(a|s)$대한 가치함수라고 하고 기호로는 $v_{\pi_1}:{\cal S}\rightarrow \mathbb{R}$로 쓴다. 
\begin{align}
v(\pi_1,s):=v_{\pi_1}(s)=\sum_{a\in {\cal A}}\pi_1(a\vert s)\left(r(s,a)+\gamma \sum_{s' \in {\cal S}}P(s' \vert s,a)v_{\pi_1}(s')\right)
\end{align}

- 특정정책 $\pi_1(a \vert s)$이 고정되면 가치함수를 계산할 수 있다. 최적가치함수는 최적의정책 $\hat{\pi}(a \vert s)$로 부터 계산되는 가치함수 $v_{\pi}(s)$이다. 즉 최적의정책을 알고 있다면 최적의가치함수를 계산할 수 있다. 반대로 최적의가치함수값을 알고있으면 그것을 통하여 최적의정책을 찾을 수 있다. 따라서 결국 강화학습의 문제는 결국 ***최적의 value function을 찾아보자!*** 로 요약된다. 

- 최적의가치함수를 찾는 과정으로 **polish iteration 알고리즘**이 있다. 이것은 초기에 임의의 정책을 초기화하고 그담에 그것을 바탕으로 가치함수를 계산하고 계산된 가치함수를 활용하여 정책을 업그레이드하고 다시 그것으로 가치함수를 계산하는 식으로 진행한다. 반복이 진행될수록 정책은 점점 최적정책 $\hat{\pi}(a \vert s)$로 수렴하고 가치함수는 점점 최적가치함수 $v_{\hat{\pi}}(s)$로 수렴할 것이라 기대된다. 

- **polish iteration 알고리즘**은 매우 시간이 많이 걸린다. 사실 특정한 정책이 결정되었을 경우 가치함수를 계산하는 것이 말처럼 쉬운일이 아니다. 왜냐하면 특정 정책이 결정되었을 경우 가치함수를 계산하려면 순환식을 풀어야 하기 때문이다. 이런식으로 풀면 먼저 목표지점에 인접한 셀들의 가치함수가 먼저 계산되고 이 값들이 바깥의 상태로 전파되면서 값들이 결정된다. 

- 가령 예를 들어서 모든 셀에서 동서남북 랜덤으로 움직이는 정책 쓴다고 하자. 모든셀의 가치는 처음에 $0$으로 초기화 하자. 목표지점은 $(1,1)$과 $(4,4)$ 2군데 이다. 이 정책에 의해서 상태$(1,2)$가 가지는 평균가치는 $\frac{5+0+0-1}{4}=1$으로 계산된다. 요런식으로 모든 셀에 대한 가치를 구할 수 있다. 

- 좀 더 스마트하게 가치를 계산할 수 있다. 현재 상황은 $P(s' \vert s,a)$를 완벽하게 알고 있는 상황이기 때문에 현재상태 $(1,2)$에서 어떤 액션을 취해야 셀 $(1,2)$의 가치가 최적화 되는지 알 수 있다. (***남쪽으로 가야한다!!***) 어떻게 행동해야 최적인지 이미 계산가능한 상황에서 정책 $\pi_1$를 수동적으로 따르는 것은 어리석으므로 $\pi_1$에서 $(1,2)$에 해당하는 부분만 바꿔서 수정하고 그거에 따른 가치함수를 계산한다. 즉 $\pi_1(a \vert s=(1,2))$를 기존의 동서남북모두에 $1/4$의 확률을 주는것에서 남쪽으로 가는 액션에만 $1$의 확률을 주게 바꾼다. 그리고 바뀐정책을 기반하여 가치함수를 계산한다. (계산값은 $5$이다.) 즉 가치함수를 아래의 식으로 업데이트한다. 
\begin{align}
v(s)^{(t+1)}=\max_{a \in {\cal A}^{(t)}(s)}\pi(a \vert s)\left(r(s,a)+\gamma \sum_{s' \in {\cal S}}P(s' \vert s,a)v^{(t)}(s')\right)
\end{align}

- 여기에서 
\begin{align}
r(s,a)+\gamma \sum_{s' \in {\cal S}}P(s' \vert s,a)v^{(t)}(s')
\end{align}  
는 상태 $s$에서 행동$a$가 결정되었을 경우 얻게 되는 보상의 기대값으로 정의될 수 있다. 이를 보통 $q_{\pi}(s,a)$라고 정의하고 **큐함수**라고 부른다. 큐함수는 단기보상과 장기보상으로 나눌수 있다. 즉 위의 식에서 $r(s,a)$는 단기보상이 되고, $\gamma \sum_{s' \in {\cal S}}P(s' \vert s,a)v^{(t)}(s')$는 장기보상이 된다. 이때 장기보상을 구하기 위해서 정책정보 $\pi$가 필요하므로 $q_{\pi}(s,a)$는 $\pi$에 의존하는 함수가 된다. 

- 큐함수는 value function과 동등한 역할을 한다. 강화학습의 목표는 최적의 value function을 찾거나 최적의 큐함수를 찾거나 둘중에 하나만 이루면 된다. 

- 이때 ${\cal A}^{(t)}(s)$는 현재시점 $t$에서 정책을 고려하였을때 상태 $s$에서 선택가능한 액션들의 집합을 의미한다. 식
\begin{align}
\max_{a \in {\cal A}^{(t)}(s)}\pi(a \vert s)q_{\pi}(s,a)
\end{align}  
을 계산하여 $v(s)^{(t+1)}$를 업데이트 할 때 ${\cal A}^{(t)}(s)$의 값도 ${\cal A}^{(t+1)}(s)$로 업데이트한다. 위의 예제로 예를들면 ${\cal A}^{(0)}=\\{a[1],a[2],a[3],a[4]\\}$에서 ${\cal A}^{(1)}=\\{a[3]\\}$으로 업데이트 한다. (남쪽으로 가야한다!!) 이러한 방식으로 정책을 부분적으로 수정하면서 가치함수를 계속 업데이트 하는 방식을 **value iteration 알고리즘**이라고 한다. 

- **polish iteration 알고리즘**은 최적의정책과 최적가치함수를 동시에 개선하는 방식이지만 **value iteration 알고리즘**은 최적가치함수를 먼저 찾고 그러부터 최적의정책을 찾는 방식이다. 따라서 **value iteration 알고리즘**이 더 빠르다. 

- 지금까지는 $P(s' \vert s,a)$를 완벽하게 알고 있다고 가정하고 문제를 푸는 방법들이었다. 이걸 모르면 어떻게 할 것인가? 그냥 특정한 정책 $\pi$에서 에피소드 $e$를 생성한다. 마코프가정을 만족하니까 하나의 에피소드 $e$에서 여러개의 ***상태샘플*** $z \in Z(s)$ 혹은 ***상태행동샘플*** $z \in Z(s,a)$가 수집할 수 있다. (알파고의 경우 16만개의 기보(=에피소드 $e$)에서 3천만개의 샘플 $z$를 수집했다고 함). 각 샘플에 대하여 보상함수 $r(z)$를 구한다. 이것을 가지고 아래와 같이 $v_{\pi}(s)$ 혹은 $q_{\pi}(s,a)$를 근사시킨다. 즉 
\begin{align}
v_{\pi}(s)=\frac{1}{|Z(s)|}\sum_{z \in Z(s)} r(z)
\end{align}
혹은 
\begin{align}
q_{\pi}(s,a)=\frac{1}{|Z(s,a)|}\sum_{z \in Z(s,a)} r(z)
\end{align}
와 같이 한다. 

- 요런식으로 샘플을 만들어서 하면 환경모델이 없어도 된다는 장점이 있다. 또한 특정상태만 골라서 그것에 대해서만 가치함수를 계산하여 계산량을 줄일수도 있다. 또 마코프성질을 크게 벗어나는 경우에도 성능저하가 심하지 않다는 장점이 있다. 이러한 방식을 몬테카를로 방식이라고 한다. 

- 하지만 이러한 방식은 하나의 에피소드가 끝나서 보상이 결정되기 전까지는 업데이트가 이루어지지 않는다. 유한에피소드이고 에피소드가 빨리 끝나면 다행이지만 그렇지 않으면 어떻게하는가? 에피소드가 끝나기 전에 업데이트를 하면 좋겠다. 이게 템포랄-디퍼런스의(temporal-differencce)의 핵심이다. 

- 템포랄-디퍼런스 방식은 살사와 Q러닝이 있다. 

- 살사는 어떻게 동작하는가? 
1) 먼저 특정상태 $s$를 랜덤하게 생성한다. 2) $\max_{a \in {\cal A}} q(s,a)$를 만족하는 액션 $a$를 선택한다. 3) 이 선택에 따라서 $r(s,a)$를 받고 다음상태 $s'$로 이동한다. 4) $s'$에서도 $\max_{a' \in {\cal A}} q(s',a')$를 만족하는 액션 $a'$를 선택한다. 5) 샘플 $(s,a,r(s,a),s',a')$을 모으면 이것을 이용하여 아래와 같이 업데이트 한다. 
\begin{align}
q(s_t,a_t) \leftarrow (1-\rho) q(s_t,a_t) + \rho \left(r(s_t,a_t) + \gamma q(s_{t+1},a_{t+1})\right)
\end{align}

- 살사의 약점은 무엇인가? $a_t$를 매우 적절하게 선택하여도 (탐험에 의해서) $a_{t+1}$가 잘못선택되면 $q(s_t,a_t)$의 값도 같이 낮아진다는 것이다. 따라서 $q(s_t,a_t)$의 값이 낮은 원인이 1) $a_t$가 잘못되었는지 2) $a_{t+1}$이 잘못되었는지 알 수 없다는 것이다. 이 한계를 극복한게 Q러닝인데 Q러닝은 아래의 식을 통하여 $q(s_t,a_t)$를 업데이트한다. 
\begin{align}
q(s_t,a_t) \leftarrow (1-\rho) q(s_t,a_t) + \rho \left(r(s_t,a_t) + \gamma \max \\{q(s_{t+1},a[1]),\dots,q(s_{t+1},a[4])\\}\right)
\end{align}
이렇게 되면 $s_{t+1}$에서의 잘못된 선택 $a_{t+1}$에 의해서 $q(s,a)$가 낮아질 일이 없다. 즉 $q(s,a)$는 오로지 $s_t$시점에서 행동한 액션 $a_t$에 대한 패널티 혹은 상이된다. 요런방식을 off-polish방식이라고 한다. 

--- 

### 복잡한 그리드월드 

- 강화학습의 문제는 $q(s,a)$를 잘 구하면 게임이 끝난다고 하였다. 그리드 세계에서는 $(s,a)$의 조합수가 유한하기 때문에 모든 조합수에 대한 단+장기보상을 조사하면 되었다. 가장 쉽게 생각하면 $(s,a)$의 모든 조합수를 테이블로 만들어놓고 에피소드가 끝날때마다 그 테이블의 값을 업데이트하면 된다는 의미이다. (템포랄 디퍼런스 러닝 이딴거 일단 생각안함) 

- 그런데 $(s,a)$의 조합수가 무궁무진하다면? 가령 예를 들면 로봇이 $(0,0)$, $(0,1)$이런곳에만 위치할 수 있는 것이 아니고 $(0,3.45)$와 같은 곳에도 위치할 수 있다면?? $(s,a)$의 모든 조합수를 표시할 테이블을 만들수도 없거니와 설령 만든다 해도 그 테이블에 있는 모든상태에 대해서 에피소드별로 테이블의 값을 업데이트 하는건 너무나 멍청한 짓이다. 하지만 모든 $s \in [0,1] \times [0,1]$에 대하여 보상값을 기록하겠다는 생각을 버리면 의외로 문제가 간단하다. 바로 함수의 근사를 이용하는 것이다. 

- 우선 이해를 위해서 살사알고리즘을 복습해보자. ${\cal S}=\\{1,2,3,4\\}$이라고 하자. ${\cal A}=\\{1,-1\\}$이라고 하자. 각 상태는 1~4까지 1차원으로 있으며 각 상태에서 로봇은 오른쪽으로 1만큼($a=1$) 혹은 왼쪽으로 1만큼($a=-1$) 움직일 수 있다. 상태 3에 가면 1만큼 보상을 받는다. $s=1$에서 왼쪽으로 가는 선택을 하면 그리드 밖으로 나가게 되고 이경우 $-1$의 보상을 받는다. 나머지는 모두 $0$의 보상을 받는다. $q(s,a)$의 도메인은 $3\times 2$ 개의 셀을 가진 테이블로 표현가능하다. 살사알고리즘을 사용하여 $q(s,a)$를 어떻게 업데이트 하는지 생각해보자. $q(s,a)$는 초기에 모두 0으로 셋팅한다. 1) 처음에 상태 $s$를 생성한다. 2) $\epsilon$탐욕에 의해서 $q^{old}(s,a)$로 부터 행동 $a$를 선택한다. 그런데 초기에는 모두 $q(s,a)$이 0이므로 랜덤한 행동을 선택할 것이다. 선택된 행동이 $a=-1$이다. 그러면 $-1$의 보상을 받고 종료된다. 이떼 $q(s,a)$는 아래와 같은 식으로 업데이트 된다. 
\begin{align}
q^{new}(s,a) \leftarrow (1-\rho)q^{old}(s,a) + \rho \left( r(s,a)+\gamma q^{old}(s',a') \right) 
\end{align}
따라서 $q(1,-1)= -1$로 업데이트가 된다. 

- 이제 다시 1) 상태 $s$를 생성한다. 생성된 상태는 또 $s=1$이다. 2) $q(1,-1)=-1$이고 $q(1,1)=0$이므로 탐욕에 의해서 $a=1$인 행동을 선택한다. 3) 상태 2로 가고 보상으로 0을 받는다. 4) 상태2에서는 $q(2,1)=q(2,-1)=0$이므로 또 아무행동이나 선택한다. $a'=1$을 선택하였다. 5) 여기까지 진행하면 샘플 $(s,a,r,s',a')=(1,1,0,2,1)$이 수집된다. $(s,a)=(1,1)$에서의 업데이트를 하자. 
\begin{align}
q^{new}(1,1) \leftarrow (1-\rho)q^{old}(1,1)+ \rho \left( r(1,1)+\gamma q^{old}(2,1) \right)
\end{align}
여기에서 $q^{old}(1,1)=0$, $r(1,1)=0$, $q^{old}(2,1)=0$이므로 $q^{new}(1,1)=0$으로 된다. (그니까 여기서는 업데이트는 일어나지 않았음) 

- 아직 에피소드가 끝난것이 아니다. $(s,a)=(2,1)$에 대한 업데이트도 해야한다. 보상은 $r(2,1)=0$을 받을 것이고 편의상 $s'=3$에서 (운좋게) $a'=1$을 선택했다고 하자. 샘플은 $(s,a,r,s',a')=(2,1,0,3,1)$이다. $q^{new}(2,1)=0$이 될 것이다. 그리고 $q^{new}(3,1)=1$이 된다. 이제 $q(s,a)$는 아래와 같다. <br/>
  1. $q(1,-1)=-1$, $q(1,1)=0$. 
  2. $q(2,-1)=0$, $q(2,1)=0$. 
  3. $q(3,-1)=0$, $q(3,1)=1$. <br/>

- 에피소드를 더 만들어보자. $s=2$를 만들면 $(s,a)=(2,-1)$ 혹은 $(s,a)=(2,1)$중에 하나의 행동을 할것이다. 만약에 $(s,a)=(2,-1)$을 한다면 보상으로 0을 받고 $(s',a')=(1,1)$이 만들어져서 다시 $s=2$상태로 올 것이다. 이때 $q(2,-1)=0$으로 업데이트된다. 만약에 $(s,a)=(2,1)$을 선택하면 보상으로 0을 받고 $(s',a')=(3,1)$이 만들어진다. $q(3,1)=1$이므로 $q(2,1)=\gamma$로 업데이트 된다. 따라서 최종적인 수렴결과는 아래와 같다. <\br>
  1. $q(1,-1)=-1$, $q(1,1)=0$.
  2. $q(2,-1)=0$, $q(2,1)=\gamma$.
  3. $q(3,-1)=0$, $q(3,1)=1$. <br/>


- 살사의 알고리즘을 다시 생각해보자. 1) 상태 $s$를 생성함. 2) $\epsilon$탐욕에 의해서 $q^{old}(s,a)$로 부터 행동 $a$를 선택함. 3) 상태 $s'$와 보상 $r(s,a)$을 받음. 4) $\epsilon$탐욕에 의해서 $q^{old}(s',a)$로 부터 행동 $a'$을 선택함. 5) 새로얻은 샘플 $(s,a,r,s',a')$을 바탕으로 $q^{old}(s,a)$의 값을 업데이트해 $q^{new}(s,a)$로 바꾼다. 이때 5)번의 과정 즉 큐함수를 어떻게 업데이트 하는가를 다시 생각해보자. 업데이트를 하는 식은 아래와 같다. 
\begin{align}
q(s_t,a_t) \leftarrow (1-\rho) q(s_t,a_t) + \rho \left(r(s_t,a_t) + \gamma q(s_{t+1},a_{t+1})\right)
\end{align}
본질적으로 업데이트가 안되는 상태가 학습이 잘된상태라는것을 눈치챈다면 
\begin{align}
q(s_t,a_t) \approx r(s_t,a_t)+\gamma q(s_{t+1},a_{t+1})
\end{align}
일수록 학습이 잘 된 상태라는 것을 알 수 있을 것이다. 상식적으로 생각해도 위의 식이 성립하는 큐함수가 이상적인 $q$함수 상태라는 것을 알 수 있다. (현재 $(s,a)$에서의 가치는 $s$에서 행동 $a$를 했을 경우 얻는 단기가치와 $s'$상태로갔을때의 잠재가치의 합이어야 하므로) 

- 우리는 
