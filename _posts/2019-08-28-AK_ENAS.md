---
title: (리뷰) AK, ENAS with NM
layout: post 
--- 

### About this doc

- 이 포스트는 아래 논문의 리뷰이다. <br/>
Jin, H., Song, Q., & Hu, X. (2018). Auto-keras: Efficient neural architecture search with network morphism. arXiv preprint arXiv:1806.10282.


### 결론 및 요약

- 이 논문의 최적의 아키텍처를 찾는 방법을 다룬 논문이다. 

- 아키텍처는 방향성 그래프(DAG)로 표현가능한 어떠한 아키텍처도 상관이 없다. 

- 논문은 특정한 아키텍처를 처음부터 만들지는 않는다는 점에서 NAS와 차별성을 가진다. (NAS는 처음부터 아키텍처를 RNN으로 컨스트럭트함) 대신에 논문은 이미 알고 있는 아키텍처를 조금 변형하여(논문에서는 이걸 모핑이라고 한다) 새로운 아키텍처를 생성해 낸다. 

- 기존의 아키텍처 A와 모핑된 아키텍처 A'은 서로 어 느정도는 비슷해야 한다. (완전히 쌩뚱맞은 아키텍처를 뱉어낼 수 없음.) 여기에서 아키텍처간의 '비슷함' 을 측정하기 위해서 저자들은 새로운 거리를 제안한다. 그리고 그 거리의 단점을 보완하기 위한 커널도 새롭게 제안한다. 이는 논문의 수식 (2) 와 (3) 에 제시되어 있으며 이 값들을 계산하여 A 에서 A' 을 만들어내는 방법을 제안하는 기초를 마련하였다. 

- 논문에서는 보상값이 랜덤이라는 점에서도 NAS와 차이점을 가진다. 나스에서는 엔바이런먼트가 주는 보상값이 고정된 값인데 여기서는 랜덤값이라고 생각한다. 

- 보상값이 랜덤이므로 그것의 평균과 분산이 존재한다. 따라서 새롭게 생성되는 아키텍처들의 모임 각각에 대한 코스트의 평균과 분산을 마치 베이지안 옵티마이제이션에 잘나오는 그림 마냥 그릴 수 있다. 

- 당연히 보상값이 최대가 되는 아키텍처를 고르는것이 유리한데 여기에서 특이한 점은 평균뿐만 아니라 분산까지 고려하여 탐험의 비율을 조절할 수 있게 하였다는 점이다. 

- 베이지안 옵티마이징과 차이점은 다음과 같다. (1) 베이지안 옵티마이징에서 관심이 있는 타겟펑션은 실수에서 실수로 가는 함수이지만 여기에서 관심을 가지는 타겟펑션은 아키텍처에서 실수로 가는 함수이다. (2) 베이지안 옵티마이징에서는 타겟펑션의 정의역이 변화하지 않지만 여기에서는 변화한다. (3) 베이지안 옵티마이징에서는 정의역이 연속이지만 여기에서는 디스크릭하다. (따라서 최소값 찾는건 겁나쉽다) (4) 베이지안 옵티마이징에서는 타겟펑션의 모양 자체를 학습하는데 초점을 맞추지만 (최저값을 찾는건 부수적인 문제) 여기에서는 최소값을 찾는거 자체가 고민이다. (5) (4)의 이유로 베이지안 옵티마이징에서는 우선 포스테리어의 분산이 맥시마이즈되는 지점을 "선택"하여 탐사한다. 하지만 논문의 방법에서는 "지정"된 지점을 탐사하며 평균이 작고 분산이 큰 지점을 탐사한다. (평균과 분산의 관계는 베타로 조정한다) 

- 논문의 알고리즘은 (1) 생성 (2) 평가 (3) 업데이트 의 과정을 반복하면서 그런데 후반부로 갈수록 업데이트가 되는 비율이 점점 낮아진다. 따라서 알고리즘 1은 항상 수렴한다. 

- 위와 같은 이유로 알고리즘이 수렴하지 않다가 갑자기 어느 순간 잠재력이 터지면서 갑자기 수렴하기 시작하는 현상은 관측되지 않는다. 대부분 초반 몇번의 이터레이션에 성능평가가 날것이다. 

- 아키텍처에서 모핑된 새로운 후보들이 시원치 않을경우 업데이트를 안할수도 있다. 

- 모핑을 하는 방식은  ***deep, wide, add, concat*** 이며 이 방식은 추가되거나 수정될 수 있다. 

- 이 방식이 RNN 에서 효과적일것 같지는 않다. 

- 이론적이지 않지만 실용적으로 보인다. 












