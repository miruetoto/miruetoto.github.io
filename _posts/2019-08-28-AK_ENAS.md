---
title: (리뷰) AK, ENAS with NM
layout: post 
--- 

### About this doc

- 이 포스트는 아래논문의 리뷰이다. <br/>
Jin, H., Song, Q., & Hu, X. (2018). Auto-keras: Efficient neural architecture search with network morphism. arXiv preprint arXiv:1806.10282.

### AK: ENAS with NM

- 기븐 neural architecture search space 를 ${\cal F}$ 라고 하자. 입력자료를 $X$ 라고 하고 cost metric 을 $Cost(\cdot)$ 이라고 하자. 우리의 목표는 최적의 뉴럴네트워크 $f^* \in {\cal F}$ 를 찾는것이다. 이때 네트워크 $f $ 의 훈련파라메터는 $\theta_f $라고 한다. 따라서 아래를 푸는것에 관심을 가진다. 
\begin{align}
f^* = \underset{f \in {\cal F} }{\operatorname {argmin} } \min_ {\theta_f } Cost(f(X; \theta_f))
\end{align}
여기에서 $\theta_f \in \mathbb{R}^{w(f)}$ 이고 $w(f)$는 $f$ 에 속하는 파라메터의 갯수이다. 

- 네트워크 $f$의 구조를 directed acyclic graph $G_f=(V_f,E_f)$ 로 표현하자. 임의의 두 노드 $v,u \in V_f$ 사이에서 
\begin{align}
u \prec v 
\end{align}
가 성립한다는 것은 노드 $v$에서 적당히 에지를 따라서 흘러가면 $u$에 도달(reachable)할 수 있다는 의미이다. 그리고 이 논문에서는 모든노드에 대하여 $u \prec v$ 이거나 $v \prec u$ 임을 가정하자. (즉 모든 노드가 연결되어있다는 의미임.) 

- 2개의 뉴럴네트워크 $f_a$, $f_b$ 가 있다고 하자. 아래와 같은 커널을 생각하자. 
\begin{align}
\kappa(f_a, f_b)=e^{-\rho^2 (d(f_a,f_b))
\end{alig}
여기에서 $d$는 사용자가 정의하기 나름인 거리함수이고 $\rho$ 는 (결과를 잘나오게끔 조절가능한) 적당한 mapping 함수이다. 

- 논문에서 제안하는 방법은 결국 아래와 같다. 
\begin{align}
d(f_a,f_b)=D_l(L_a,L_b)+\lambda D_s(S_a,S_b).
\end{align}
여기에서 $L_a, L_b$ 는 각각 $f_a$ 와 $f_b$ 에 포함된 네트워크를 레이어 별로 묶은 것이다. 즉 $L_a=\\{l_a^{(1)},l_a^{(2)},\dots,\\}$ 이고 $L_b=\\{l_b^{(1)},l_b^{(2)},\dots,\\}$ 이다. 

- 











