---
layout: post 
title: (리뷰) NAS
--- 

### About this doc 

- 이번 포스팅에서는 아래의 논문을 정리하겠다. <br/>
Zoph, B., & Le, Q. V. (2016). Neural architecture search with reinforcement learning. arXiv preprint arXiv:1611.01578.

- 개인적으로 석연치 않은 논문이다. 내가 이해를 잘 못하는 것인지 아니면 모르겠지만 저자들이 사용한 방법에 근본적인 문제점이 있는것 같다. (1) 먼저 RNN 방식을 왜 사용해야 하는지 명확하게 설명하지 않았다. 적어도 나에게는 그 이유가 명확하게 와닿지 않았다. (2) 저자들의 논문은 AI를 설계하는 AI를 만든다는 법을 설명한다고 했는데 이건 단순히 AI 아키텍처들의 여러조합 중 가장 성능을 잘 내는 조합을 골라내는 것 뿐이다. 심지어 아키텍처의 조합도 유한개이다. (3) 저자들이 제시한 방법은 결국 아키텍처를 설계하는 하이퍼파라미터를 옵티마이징 하는것인데 다른방법에 비하여 얼마나 효율적으로 옵티마이징 하는지 비교하지 않았다. 


### Toy Example 

- 이해를 돕기 위해서 가벼운 예제를 도입하였다. 아래와 같은 데이터셋 $(x_i,y_i)$ 이 있다고 가정하자. 
\begin{align}
y_ i=2 x_ i + \epsilon_i
\end{align}
여기에서 $\epsilon_i$ 는 에러텀이다. 위와 같은 자료에서 우리가 학습하고 싶은 것은 
\begin{align}
\sum_{i=1}^{n}\big(y_i-f(x_i)\big)^2
\end{align}
을 최소화 하는 어떠한 함수 $f(x)$ 이다. 그리고 당연히 이때 $f(x)=2x$ 이다. 

- $f(x)$를 학습하기 위해서 each observation $i$에 대하여 아래와 같은 구조를 가지는 MLP 를 쌓았다고 가정하자. 
\begin{align}
y_i = g\big(h(x_i u) v\big) 
\end{align}
여기에서 $u$, $v$는 weight 이고 $g$ 와 $h$ 는 activation function 이다. $g\big(h(x_i u) v\big)=2x $ 가 되는것이 최적이므로 $g$ 와 $h$ 를 모두 linear activation 으로 설정하고 $uv=2 $ 가 되도록 하는 weight 를 설정하면 된다. 

- 이 상황에서 우리는 activation 의 선택지를 sigmoid 와 linear 로 주고 적절한 activation 을 선택하게끔 만드는 NAS를 구성할 것이다. 즉 우리의 예제에서 NAS가 선택해야 하는 조합의 수는 아래의 4가지이다. <br/><br/>
**(1)** $h(x)=x$, $g(x)=x$ <br/>
**(2)** $h(x)=x$, $g(x)=\frac{e^x}{1+e^x}$ <br/>
**(3)** $h(x)=\frac{e^x}{1+e^x}$, $g(x)=x$ <br/>
**(4)** $h(x)=\frac{e^x}{1+e^x}$, $g(x)=\frac{e^x}{1+e^x}$ <br/> <br/> 
참고로 최선의 선택은 당연히 **(1)** 이다. 

- NAS는 여기에서 2개의 action을 취하게 된다. 하나는 $h$를 고르는 action이고 하나는 $g$를 고르는 action이다. 편의상 이 action을 각각 $A_1$ 과 $A_2$ 라고 하자. (대문자인 이유는 확률변수이기 때문임). 즉 ${\cal A}=\\{\mbox{"linear"}, \mbox{"sigmoid"} \\}$ 이고 $A_1, A_2 \in {\cal A}$ 이다. 편의상 각각의 action에서 linear를 고를 확률을 $p_1$ 과 $p_2$ 라고 하자. 즉 
\begin{align}
\begin{cases}
p_1 = \mathbb{P} ~ \big(A_1 = \mbox{"linear"} \big) \\\\ 
p_2 = \mathbb{P} ~ \big(A_2 = \mbox{"linear"} ~ \big\| ~ a_1 \big) 
\end{cases}
\end{align}
이다. 편의상 $(p_1,p_2)$ 를 $\boldsymbol\theta_c$ 라고 하자. 즉 $\boldsymbol\theta_c=(p_1,p_2)'$ 이다. 

- 이제 **에이전트(=컨트롤러)** 와 **엔바이러먼트(혹은 신)** 을 정의하자. 여기에서 **에이전트**는 적절한 $h$와 $g$를 고르는 주체이고 **신**은 에이전트의 선택이 적절한지 적절하지 않은지 보상을 주는 존재이다. 참고로 여기에서 에이전트가 $(h,g)$를 고른다는 말은 적절한 $(a_1,a_2)$를 고른다는 말과 같고 이말은 $f(x)$를 추론하기 위한 적절한 아키텍처를 고른다는 말과 같음을 상기하자. **신**이 주는 보상은 에이전트의 아키텍처에 대한 MSE값 즉 
\begin{align}
\sum_{i=1}^{n} \big(y_i-\hat{f}(x)\big)^2
\end{align}
값이다. 에이전트가 고를 수 있는 아키텍처는 4개이므로 신이 줄수 있는 보상도 아래의 4개이다. 
**(1)** $r_1$: 에이전트가 $h(x)=x$, $g(x)=x$ 를 선택하였을 경우 즉 $(a_1,a_2)=(\mbox{linear},\mbox{linear})$ 인 경우의 보상. <br/>
**(2)** $r_2$: 에이전트가 $h(x)=x$, $g(x)=\frac{e^x}{1+e^x}$ 를 선택하였을 경우 즉 $(a_1,a_2)=(\mbox{linear},\mbox{sigmoid})$ 인 경우의 보상. <br/>
**(3)** $r_3$: 에이전트가 $h(x)=\frac{e^x}{1+e^x}$, $g(x)=x$ 를 선택하였을 경우 즉 $(a_1,a_2)=(\mbox{sigmoid},\mbox{linear})$ 인 경우의 보상. <br/>
**(4)** $r_4$: 에이전트가 $h(x)=\frac{e^x}{1+e^x}$, $g(x)=\frac{e^x}{1+e^x}$ 를 선택하였을 경우 즉 $(a_1,a_2)=(\mbox{sigmoid},\mbox{sigmoid})$ 인 경우의 보상. <br/> <br/> 
따라서 ${\cal R}=\\{r_1,r_2,r_3,r_4\\}$ 이다. 

- ㅁㄴㅇㄹ

### 3. Methods

- $a_{1:T}$를 list of action 이라고 하자. 즉 
\begin{align}
a_{1:T}:=\Big \\{a_1,\dots, a_T\Big \\}
\end{align}
이다. 여기에서 $a_1,\dots,a_T$ 는 given $s_1,\dots,s_T$ 일때 $A_1,\dots,A_T$ 의 realization 이다. 

- $R$을 $a_{1:T}$를 넣었을 경우 얻게되는 reward 라고 하자. 

- 에이전트는 아래와 같은 expected reward 를 최대화 하고 싶어 한다. 
\begin{align}
\mathbb{E}_ {\mathbb{P}(a_{1:T} ; \theta_c)} R 
\end{align}


<center><img src="https://github.com/miruetoto/miruetoto.github.io/blob/master/img/NAS/nas_fig1.png?raw=true" width="60%" height="60%"></center>
<center><img src="https://github.com/miruetoto/miruetoto.github.io/blob/master/img/NAS/nas_fig2.png?raw=true"></center>
<center><img src="https://github.com/miruetoto/miruetoto.github.io/blob/master/img/NAS/nas_fig3.png?raw=true"></center>
<center><img src="https://github.com/miruetoto/miruetoto.github.io/blob/master/img/NAS/nas_fig4.png?raw=true"></center>
<center><img src="https://github.com/miruetoto/miruetoto.github.io/blob/master/img/NAS/nas_fig5.png?raw=true"></center>
<center><img src="https://github.com/miruetoto/miruetoto.github.io/blob/master/img/NAS/nas_fig6.png?raw=true"></center>
