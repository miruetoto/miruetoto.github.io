---
layout: post 
title: (리뷰) NAS
--- 

### About this doc 

- 이번 포스팅에서는 아래의 논문을 정리하겠다. <br/>
Zoph, B., & Le, Q. V. (2016). Neural architecture search with reinforcement learning. arXiv preprint arXiv:1611.01578.

- 인공지능을 이해할때 가장 경계해야할 부분이 **"지나친 의인화"** 라고 생각한다. 컴퓨터는 **학습**을 하는게 아니라 **계산**을 하는 것이다. **훈련**과 **예측**을 하는게 아니고 **계산**과 **계산**을 하는 것이다. 물론 의인화가 직관적인 이해에는 도움이 되지만 지나친 의인화는 학술적이라기 보다는 **상업적** 이라는 생각이 들게 만든다. 

- 이런 점에서 이 논문은 제시하는 기술력에 비해서 의인화가 너무 거창하다는 느낌을 지울 수 없다. 무려 **인공지능을 만드는 인공지능이다** 라는 슬로건으로 나온 논문이라 정말 엄청나게 혁신적인 알고리즘을 제시한 줄 알았는데 그렇지 않았다. 상당히 비효율적으로 보였다 심지어 몇몇 부분에서는 저자들이 다른분야를 제대로 이해하고 쓴건 맞는가? 라는 의문도 들었다. (1) 먼저 RNN 방식을 왜 사용해야 하는지에 대한 이유가 석연치 않다. 저자들이 설명을 하지 않아 그 motivation 을 이해하는것이 쉽지 않다. 적어도 내가 이해하는 한도내에서는 RNN을 쓸 필요가 없다. (2) 저자들이 제시한 방법은 결국 강화학습을 활용하여 아키텍처를 설계하는 하이퍼파라미터를 옵티마이징 하는것인데 다른방법에 비하여 얼마나 효율적으로 옵티마이징 하는지 비교하지 않았다. 애초에 굳이 강화학습을 이용해야 했는지 자체도 의문이다. 


### Toy Example 

- 이해를 돕기 위해서 가벼운 예제를 도입하였다. 개인적으로 이 논문의 방법이 문제가 많다고 생각하지만 우선 이 예제를 설명하는 범위내에서는 이러한 문제점을 언급하지 않겠다. 이 챕터에서는 단지 저자들이 주장하는 NAS가 어떻게 동작하는지 그리고 어떻게 구현할 수 있는지 이해하는데에만 초점을 맞추겠다. 중간중간에 "아니 이걸 이렇게 (비효율적으로) 구한다고?" 싶은것도 많고 저자들의 생각에 비판의 여지도 많겠겠만 (틀린거 같아) 이것은 다음챕터에서 몰아서 논의하고 이번에는 그냥 이해에만 초점을 맞추도록 하자. 

- 아래와 같은 데이터셋 $(x_i,y_i)$ 이 있다고 가정하자. 
\begin{align}
y_ i=2 x_ i + \epsilon_i
\end{align}
여기에서 $\epsilon_i$ 는 에러텀이다. 위와 같은 자료에서 우리가 학습하고 싶은 것은 
\begin{align}
\sum_{i=1}^{n}\big(y_i-f(x_i)\big)^2
\end{align}
을 최소화 하는 어떠한 함수 $f(x)$ 이다. 그리고 당연히 이때 $f(x)=2x$ 이다. 

- 아래는 $f(x)$를 구현한것이다. 
<center><img src="https://github.com/miruetoto/miruetoto.github.io/blob/master/img/NAS/my_fig1.png?raw=true" width="100%" height="100%"></center>

- $f(x)$를 학습하기 위해서 each observation $i$에 대하여 아래와 같은 구조를 가지는 MLP 를 쌓았다고 가정하자. 
\begin{align}
y_i = g\big(h(x_i u) v\big) 
\end{align}
여기에서 $u$, $v$는 weight 이고 $g$ 와 $h$ 는 activation function 이다. $g\big(h(x_i u) v\big)=2x $ 가 되는것이 최적이므로 $g$ 와 $h$ 를 모두 linear activation 으로 설정하고 $uv=2 $ 가 되도록 하는 weight 를 설정하면 된다. 이게 최선이다. 

- 이 상황에서 우리는 activation 의 선택지를 sigmoid 와 linear 로 주고 적절한 activation 을 선택하게끔 만드는 NAS를 구성할 것이다. 즉 우리의 예제에서 NAS가 선택해야 하는 조합의 수는 아래의 4가지이다. <br/><br/>
**(1)** $h(x)=x$, $g(x)=x$ <br/>
**(2)** $h(x)=x$, $g(x)=\frac{e^x}{1+e^x}$ <br/>
**(3)** $h(x)=\frac{e^x}{1+e^x}$, $g(x)=x$ <br/>
**(4)** $h(x)=\frac{e^x}{1+e^x}$, $g(x)=\frac{e^x}{1+e^x}$ <br/> <br/> 
참고로 최선의 선택은 당연히 **(1)** 이다. 

- NAS는 여기에서 2개의 action 을 취하게 된다. 하나는 $h$를 고르는 action 이고 하나는 $g$를 고르는 action 이다. 편의상 이 action을 각각 $A_1$ 과 $A_2$ 라고 하자. $A_1$, $A_2$ 가 대문자인 이유는 확률변수이기 때문이다. 그리고 $A_1, A_2$의 실현값을 각각 $a_1,a_2$ 라고 하자. 선택할 수 있는 action 은 linear activation 을 고르거나 sigmoid activatio 을 고르거나 둘중하나 이므로 ${\cal A}=\\{\mbox{"linear"}, \mbox{"sigmoid"} \\}$ 이다. 편의상 ${\cal A}=\\{0,1\\}$ 이라고 정의하자. 

- 이제 **에이전트(=컨트롤러)** 와 **엔바이러먼트(혹은 신)** 을 정의하자. 여기에서 **에이전트**는 적절한 $h$와 $g$를 고르는 주체이고 **신**은 에이전트의 선택이 적절한지 적절하지 않은지 보상을 주는 존재이다. 참고로 여기에서 에이전트가 $(h,g)$를 고른다는 말은 적절한 $(a_1,a_2)$를 고른다는 말과 같고 이말은 $f(x)$를 추론하기 위한 적절한 아키텍처를 고른다는 말과 같음을 상기하자. **신**이 주는 보상은 에이전트의 아키텍처에 대한 MSE값 즉 
\begin{align}
\sum_{i=1}^{n} \big(y_i-\hat{f}(x)\big)^2
\end{align}
값이다. 에이전트가 고를 수 있는 아키텍처는 4개이므로 신이 줄수 있는 보상도 아래의 4개이다. (고정된 $(x_i,y_i)$에 대하여 MSE는 항상 하나의 값만 가지므로이 경우 보상함수는 **랜덤함수가 아니다.** 그래서 딱 아래 4개의 경우밖에 없다.) <br/><br/>
**(1)** $r_{00}:=r\big(A_1=0,A_2=0\big):\quad$ 에이전트(=컨트롤러)가 $(\mbox{linear},\mbox{linear})$ 를 활성화함수로 선택했을때 보상. <br/>
**(2)** $r_{01}:=r\big(A_1=0,A_2=1\big):\quad$: 에이전트(=컨트롤러)가 $(\mbox{linear},\mbox{sigmoid})$ 를 활성화함수로 선택했을때 보상.  <br/>
**(3)** $r_{10}:=r\big(A_1=1,A_2=0\big):\quad$ 에이전트(=컨트롤러)가 $(\mbox{sigmoid},\mbox{linear})$ 를 활성화함수로 선택했을때 보상.  <br/>
**(4)** $r_{11}:=r\big(A_1=1,A_2=1\big):\quad$ 에이전트(=컨트롤러)가 $(\mbox{sigmoid},\mbox{sigmoid})$ 를 활성화함수로 선택했을때 보상.  <br/> <br/> 
따라서 ${\cal R}=\\{r_{00},r_{01},r_{10},r_{11}\\}$ 이다. 

- 이 예제에 한정하여 $r_1,\dots,r_4$을 구해보면 아래와 같다. <br/>
<center><img src="https://github.com/miruetoto/miruetoto.github.io/blob/master/img/NAS/my_fig2.png?raw=true" width="100%" height="100%"></center>
<center><img src="https://github.com/miruetoto/miruetoto.github.io/blob/master/img/NAS/my_fig3.png?raw=true" width="100%" height="100%"></center>
<center><img src="https://github.com/miruetoto/miruetoto.github.io/blob/master/img/NAS/my_fig4.png?raw=true" width="100%" height="100%"></center>
<center><img src="https://github.com/miruetoto/miruetoto.github.io/blob/master/img/NAS/my_fig5.png?raw=true" width="100%" height="100%"></center>
<br/>
**2000-에폭** 이후에 구해지는 값은 $r_{00}=0.0535$, $r_{01}=91.1902$, $r_{10}=0.3961$, $r_{11}=34.4771$ 이다. 그냥 이 값들이 수렴값이라고 믿자. (언제까지 돌리고 있을순 없잖음?) 

- 컨트롤러의 액션에 따라 보상은 당연히 달라지게 된다. 그것은 아래와 같이 표현가능하다. 
\begin{align}
\mathbb{E}\big(R \big) = \sum_{a_2 \in {\cal A}} \sum_{a_1 \in {\cal A}} r\big(A_1=a_1,A_2=a_2) ~ \mathbb{P} ~ \big(A_2 = a_2 ~ \big\| ~ A_1=a_1 \big) \mathbb{P} \big(A_1 = a_1 \big)
\end{align}
여기에서 $\mathbb{P} \big(A_1 = 1 \big)$ 와 $\mathbb{P} ~ \big(A_2 = 1 ~ \big\| ~ A_1= a_1 \big)$은 아래와 같이 모델링 할 수 있다. 
\begin{align}
\begin{cases} 
\mathbb{P} \big(A_1 = 1 \big) = \frac{\exp(c)}{1+\exp(c)} \\\\ \\
\mathbb{P} ~ \big(A_2 = 1 ~ \big\| ~ A_1=a_1 \big)=\frac{\exp\big(w_2\tanh(w_1a_1+b_1)+b_2\big) }{1+ \exp\big(w_2\tanh(w_1a_1+b_1)+b_2\big) }
\end{cases}
\end{align}
이제 $\mathbb{E}\big(R \big)$ 은 아래와 같이 표현가능하다. 
\begin{align}
\mathbb{E}\big(R \big)&=r_{00}\frac{1}{1+e^c}\frac{1}{1+e^b}+r_{01}\frac{1}{1+e^c}\frac{e^b}{1+e^b} \\\\ \\
&+r_{10}\frac{e^c}{1+e^c}\frac{1}{1+e^{w+b}}+r_{11}\frac{e^c}{1+e^c}\frac{e^{w+b}}{1+e^{w+b}}
\end{align}
당연히 쉽게 눈치채겠지만 $c,b$의 값이 $-\infty$에 가까울수록 그리고 $w \approx 0$ 일수록 최적값이다. 

- 논문의 표기법을 따라서 $\mathbb{E}(R)=J(\theta_c)$ 라고 하자. 여기에서 $\theta_c=(c,w_1,b_1,w_2,b_2)'$이다. **컨트롤러**의 목적은 $J(\theta_c)$를 최소화하는 것이다. (원래 강화학습에서는 리워드를 $J$로 정의해서 $J$를 최대화하는 파라메터를 찾는데 나는 여기서 손실을 $J$로 정의했으므로 $J$를 최소화하는 파라메터를 찾아야 한다. 사실 이거는 정의하기 나름이라 $J$를 최소화 하든 최대화 하든 별로 안중요하다. 논리적으로 맞게 구하기만 하면 된다.) 이제 $J(\theta_c)$를 최소화 하기 위한 미분을 계산해야 한다. 즉 
\begin{align}
\frac{\partial }{\partial c }J(\theta_c)= \mbox{blah~blah~blah~blah~} \\\\ \\
\frac{\partial }{\partial w_1 }J(\theta_c)= \mbox{blah~blah~blah~blah~} \\\\ 
\frac{\partial }{\partial b_1 }J(\theta_c)= \mbox{blah~blah~blah~blah~} \\\\ 
\frac{\partial }{\partial w_2 }J(\theta_c)= \mbox{blah~blah~blah~blah~} \\\\ 
\frac{\partial }{\partial b_2 }J(\theta_c)= \mbox{blah~blah~blah~blah~} \\\\ 
\end{align}
를 계산하면 된다. 그런데 막상 미분하려고 보니까 좀 귀찮은거 같아서 코드로 구현하였다. 어차피 이해를 위해서는 미분텀의 closed form이 필요한게 아니고 특정 $\theta_c^*=(c^*,w_1^*,b_1^*,w_2^*,b_2^*)'$ 에서 각각의 편미분 값 자체가 필요한 거라서 계산하지 않았다. 아래는 각각 $J_{\theta_c}$와 임의의 $f$에서 그래디언트를 구하는 함수를 구현한 것이다. <br/>
<center><img src="https://github.com/miruetoto/miruetoto.github.io/blob/master/img/NAS/my_fig2.png?raw=true" width="100%" height="100%"></center>

- 이제 값들을 넣으면서 해석해보자. 

### 비판 

- 논문을 자세히 살펴보면 ${\cal S}$를 정의하지 않는다. 그래서 action을 취해서 무엇인가 변화하는게 없다. 

- 컨트롤러가 action $a_1,a_2,\dots,a_T$ 를 취할때마다 보상이 주어지는것도 아니고 상태가 변화하는 것도 아니다. **그냥 아무 일도 일어나지 않는다.** 

- 일반적인 강화학습에서는 **given $(s,a)$** 에서 보상이 랜덤이다. (똑같은 조건(=state)에서 똑같은 행동(=action)을 취했는데도 얻는 reward 가 달라진다는 의미임, ~~원래 MDP 자체가 될 놈은 뭘 해도 되고 안될놈은 뭘 해도 안되는 운빨세상임..~~) 아무튼 보통 강화학습에서는 
\begin{align}
r(s,a)=\mathbb{E} \big(R_{t+1} ~\big \|~ S_t=s,A_t=a \big)
\end{align}
라고 쓰는데 여기서는 $S_t=s$ 이고 $A_t=a$ 임을 알고 있다면 $R_{t+1}$의 값은 그냥 고정이니까 애초에 $\mathbb{E}$ 이라는 기호자체를 주장할 수 없다. 사실 이 예제에서는 $S_t$도 정의할 수 없고 (상태자체가 없다) 보상이 



### 3. Methods

- $a_{1:T}$를 list of action 이라고 하자. 즉 
\begin{align}
a_{1:T}:=\Big \\{a_1,\dots, a_T\Big \\}
\end{align}
이다. 여기에서 $a_1,\dots,a_T$ 는 given $s_1,\dots,s_T$ 일때 $A_1,\dots,A_T$ 의 realization 이다. 

- $R$을 $a_{1:T}$를 넣었을 경우 얻게되는 reward 라고 하자. 

- 에이전트는 아래와 같은 expected reward 를 최대화 하고 싶어 한다. 
\begin{align}
\mathbb{E}_ {\mathbb{P}(a_{1:T} ; \theta_c)} R 
\end{align}


<center><img src="https://github.com/miruetoto/miruetoto.github.io/blob/master/img/NAS/nas_fig1.png?raw=true" width="60%" height="60%"></center>
<center><img src="https://github.com/miruetoto/miruetoto.github.io/blob/master/img/NAS/nas_fig2.png?raw=true"></center>
<center><img src="https://github.com/miruetoto/miruetoto.github.io/blob/master/img/NAS/nas_fig3.png?raw=true"></center>
<center><img src="https://github.com/miruetoto/miruetoto.github.io/blob/master/img/NAS/nas_fig4.png?raw=true"></center>
<center><img src="https://github.com/miruetoto/miruetoto.github.io/blob/master/img/NAS/nas_fig5.png?raw=true"></center>
<center><img src="https://github.com/miruetoto/miruetoto.github.io/blob/master/img/NAS/nas_fig6.png?raw=true"></center>
