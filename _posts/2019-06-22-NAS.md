---
layout: post 
title: (리뷰) NAS
--- 

### About this doc 

- 이번 포스팅에서는 아래의 논문을 정리하겠다. <br/>
Zoph, B., & Le, Q. V. (2016). Neural architecture search with reinforcement learning. arXiv preprint arXiv:1611.01578.

- 인공지능을 이해할때 가장 경계해야할 부분이 **"지나친 의인화"** 라고 생각한다. 컴퓨터는 **학습**을 하는게 아니라 **계산**을 하는 것이다. **훈련**과 **예측**을 하는게 아니고 **계산**과 **계산**을 하는 것이다. 물론 의인화가 직관적인 이해에는 도움이 되지만 지나친 의인화는 학술적이라기 보다는 **상업적** 이라는 생각이 들게 만든다. 머 모델 몇개 돌려놓고 인공지능이 어쩌고 하는게 좀 웃기지 않나? 기술을 팔기 위한 상술같이 보인다. 

- 이런 점에서 이 논문은 제시하는 기술력에 비해서 의인화가 너무 거창하다는 느낌을 지울 수 없다. 무려 **인공지능을 만드는 인공지능이다** 라는 슬로건으로 나온 논문이라 정말 엄청나게 혁신적인 알고리즘을 제시한 줄 알았는데 그렇지 않았다. 상당히 비효율적으로 보였다 심지어 몇몇 부분에서는 저자들이 다른분야를 제대로 이해하고 쓴건 맞는가? 라는 의문도 들었다. (1) 먼저 RNN 방식을 왜 사용해야 하는지에 대한 이유가 석연치 않다. 저자들이 설명을 하지 않아 그 motivation 을 이해하는것이 쉽지 않다. 적어도 내가 이해하는 한도내에서는 RNN을 쓸 필요가 없다. (2) 저자들이 제시한 방법은 결국 강화학습을 활용하여 아키텍처를 설계하는 하이퍼파라미터를 옵티마이징 하는것인데 다른방법에 비하여 얼마나 효율적으로 옵티마이징 하는지 비교하지 않았다. 애초에 굳이 강화학습을 이용해야 했는지 자체도 의문이다. 


### Toy Example 

- 이해를 돕기 위해서 가벼운 예제를 도입하였다. 개인적으로 이 논문의 방법이 문제가 많다고 생각하지만 우선 이 예제를 설명하는 범위내에서는 이러한 문제점을 언급하지 않겠다. 이 챕터에서는 단지 저자들이 주장하는 NAS가 어떻게 동작하는지 그리고 어떻게 구현할 수 있는지 이해하는데에만 초점을 맞추겠다. 중간중간에 "아니 이걸 이렇게 (비효율적으로) 구한다고?" 싶은것도 많고 저자들의 생각에 비판의 여지도 많겠겠만 (틀린거 같아) 이것은 다음챕터에서 몰아서 논의하고 이번에는 그냥 이해에만 초점을 맞추도록 하자. 

- 아래와 같은 데이터셋 $(x_i,y_i)$ 이 있다고 가정하자. 
\begin{align}
y_ i=2 x_ i + \epsilon_i
\end{align}
여기에서 $\epsilon_i$ 는 에러텀이다. 위와 같은 자료에서 우리가 학습하고 싶은 것은 
\begin{align}
\sum_{i=1}^{n}\big(y_i-f(x_i)\big)^2
\end{align}
을 최소화 하는 어떠한 함수 $f(x)$ 이다. 그리고 당연히 이때 $f(x)=2x$ 이다. 

- $f(x)$를 학습하기 위해서 each observation $i$에 대하여 아래와 같은 구조를 가지는 MLP 를 쌓았다고 가정하자. 
\begin{align}
y_i = g\big(h(x_i u) v\big) 
\end{align}
여기에서 $u$, $v$는 weight 이고 $g$ 와 $h$ 는 activation function 이다. $g\big(h(x_i u) v\big)=2x $ 가 되는것이 최적이므로 $g$ 와 $h$ 를 모두 linear activation 으로 설정하고 $uv=2 $ 가 되도록 하는 weight 를 설정하면 된다. 

- 이 상황에서 우리는 activation 의 선택지를 sigmoid 와 linear 로 주고 적절한 activation 을 선택하게끔 만드는 NAS를 구성할 것이다. 즉 우리의 예제에서 NAS가 선택해야 하는 조합의 수는 아래의 4가지이다. <br/><br/>
**(1)** $h(x)=x$, $g(x)=x$ <br/>
**(2)** $h(x)=x$, $g(x)=\frac{e^x}{1+e^x}$ <br/>
**(3)** $h(x)=\frac{e^x}{1+e^x}$, $g(x)=x$ <br/>
**(4)** $h(x)=\frac{e^x}{1+e^x}$, $g(x)=\frac{e^x}{1+e^x}$ <br/> <br/> 
참고로 최선의 선택은 당연히 **(1)** 이다. 

- NAS는 여기에서 2개의 action 을 취하게 된다. 하나는 $h$를 고르는 action 이고 하나는 $g$를 고르는 action 이다. 편의상 이 action을 각각 $A_1$ 과 $A_2$ 라고 하자. $A_1$, $A_2$ 가 대문자인 이유는 확률변수이기 때문이다. 그리고 $A_1, A_2$의 실현값을 각각 $a_1,a_2$ 라고 하자. 선택할 수 있는 action 은 linear activation 을 고르거나 sigmoid activatio 을 고르거나 둘중하나 이므로 ${\cal A}=\\{\mbox{"linear"}, \mbox{"sigmoid"} \\}$ 이다. 편의상 아래와 같은 확률을 정의하자. 
\begin{align}
\begin{cases}
p = \mathbb{P} ~ \big(A_1 = \mbox{"linear"} \big) \\\\ 
q_1 = \mathbb{P} ~ \big(A_2 = \mbox{"linear"} ~ \big\| ~ a_1=\mbox{"linear"} \big) \\\\ 
q_2 = \mathbb{P} ~ \big(A_2 = \mbox{"linear"} ~ \big\| ~ a_1=\mbox{"sigmoid"} \big) 
\end{cases}
\end{align}
이다. 

- 이제 **에이전트(=컨트롤러)** 와 **엔바이러먼트(혹은 신)** 을 정의하자. 여기에서 **에이전트**는 적절한 $h$와 $g$를 고르는 주체이고 **신**은 에이전트의 선택이 적절한지 적절하지 않은지 보상을 주는 존재이다. 참고로 여기에서 에이전트가 $(h,g)$를 고른다는 말은 적절한 $(a_1,a_2)$를 고른다는 말과 같고 이말은 $f(x)$를 추론하기 위한 적절한 아키텍처를 고른다는 말과 같음을 상기하자. **신**이 주는 보상은 에이전트의 아키텍처에 대한 MSE값 즉 
\begin{align}
\sum_{i=1}^{n} \big(y_i-\hat{f}(x)\big)^2
\end{align}
값이다. 에이전트가 고를 수 있는 아키텍처는 4개이므로 신이 줄수 있는 보상도 아래의 4개이다. (고정된 $(x_i,y_i)$에 대하여 MSE는 항상 하나의 값만 가지므로이 경우 보상함수는 **랜덤함수가 아니다.** 그래서 딱 아래 4개의 경우밖에 없다.) <br/><br/>
**(1) $r_1$:**  $(a_1,a_2)=(\mbox{linear},\mbox{linear})$ 인 경우의 보상. <br/>
**(2) $r_2$:**  $(a_1,a_2)=(\mbox{linear},\mbox{sigmoid})$ 인 경우의 보상. <br/>
**(3) $r_3$:**  $(a_1,a_2)=(\mbox{sigmoid},\mbox{linear})$ 인 경우의 보상. <br/>
**(4) $r_4$:**  $(a_1,a_2)=(\mbox{sigmoid},\mbox{sigmoid})$ 인 경우의 보상. <br/> <br/> 
따라서 ${\cal R}=\\{r_1,r_2,r_3,r_4\\}$ 이다. ~~사실 이 예제의 경우 (2)와 (3)의 아키텍처는 같은 아키텍처이므로 에이전트가 선택할 수 있는 아키텍처도 3개이고 따라서 사실상 **신**이 주는 보상도 3개 뿐이다.. 예제를 만들고 보니까 생각났다.. 그래서 잘 만든 예제는 아니다. 그런데 그냥 4개라고 치자.. 어차피 예제일뿐이고 이후의 논리를 전개하는데 그것이 중요한 요소는 아니다..~~

- 보상은 아래

### 비판 

- 논문을 자세히 살펴보면 ${\cal S}$를 정의하지 않는다. 그래서 action을 취해서 무엇인가 변화하는게 없다. 

- 컨트롤러가 action $a_1,a_2,\dots,a_T$ 를 취할때마다 보상이 주어지는것도 아니고 상태가 변화하는 것도 아니다. **그냥 아무 일도 일어나지 않는다.** 

- 일반적인 강화학습에서는 **given $(s,a)$** 에서 보상이 랜덤이다. (똑같은 조건(=state)에서 똑같은 행동(=action)을 취했는데도 얻는 reward 가 달라진다는 의미임, ~~원래 MDP 자체가 될 놈은 뭘 해도 되고 안될놈은 뭘 해도 안되는 운빨세상임..~~) 아무튼 보통 강화학습에서는 
\begin{align}
r(s,a)=\mathbb{E} \big(R_{t+1} ~\big \|~ S_t=s,A_t=a \big)
\end{align}
라고 쓰는데 여기서는 $S_t=s$ 이고 $A_t=a$ 임을 알고 있다면 $R_{t+1}$의 값은 그냥 고정이니까 애초에 $\mathbb{E}$ 이라는 기호자체를 주장할 수 없다. 사실 이 예제에서는 $S_t$도 정의할 수 없고 (상태자체가 없다) 보상이 



### 3. Methods

- $a_{1:T}$를 list of action 이라고 하자. 즉 
\begin{align}
a_{1:T}:=\Big \\{a_1,\dots, a_T\Big \\}
\end{align}
이다. 여기에서 $a_1,\dots,a_T$ 는 given $s_1,\dots,s_T$ 일때 $A_1,\dots,A_T$ 의 realization 이다. 

- $R$을 $a_{1:T}$를 넣었을 경우 얻게되는 reward 라고 하자. 

- 에이전트는 아래와 같은 expected reward 를 최대화 하고 싶어 한다. 
\begin{align}
\mathbb{E}_ {\mathbb{P}(a_{1:T} ; \theta_c)} R 
\end{align}


<center><img src="https://github.com/miruetoto/miruetoto.github.io/blob/master/img/NAS/nas_fig1.png?raw=true" width="60%" height="60%"></center>
<center><img src="https://github.com/miruetoto/miruetoto.github.io/blob/master/img/NAS/nas_fig2.png?raw=true"></center>
<center><img src="https://github.com/miruetoto/miruetoto.github.io/blob/master/img/NAS/nas_fig3.png?raw=true"></center>
<center><img src="https://github.com/miruetoto/miruetoto.github.io/blob/master/img/NAS/nas_fig4.png?raw=true"></center>
<center><img src="https://github.com/miruetoto/miruetoto.github.io/blob/master/img/NAS/nas_fig5.png?raw=true"></center>
<center><img src="https://github.com/miruetoto/miruetoto.github.io/blob/master/img/NAS/nas_fig6.png?raw=true"></center>
