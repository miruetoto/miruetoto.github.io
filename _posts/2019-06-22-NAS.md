---
layout: post 
title: (리뷰) NAS
--- 

### About this doc 

- 이번 포스팅에서는 아래의 논문을 정리하겠다. <br/>
Zoph, B., & Le, Q. V. (2016). Neural architecture search with reinforcement learning. arXiv preprint arXiv:1611.01578.

- 개인적으로 석연치 않은 논문이다. 내가 이해를 잘 못하는 것인지 아니면 모르겠지만 저자들이 사용한 방법에 근본적인 문제점이 있는것 같다. (1) 먼저 RNN 방식을 왜 사용해야 하는지 명확하게 설명하지 않았다. 적어도 나에게는 그 이유가 명확하게 와닿지 않았다. (2) 저자들의 논문은 AI를 설계하는 AI를 만든다는 법을 설명한다고 했는데 이건 단순히 AI 아키텍처들의 여러조합 중 가장 성능을 잘 내는 조합을 골라내는 것 뿐이다. 심지어 아키텍처의 조합도 유한개이다. (3) 저자들이 제시한 방법은 결국 아키텍처를 설계하는 하이퍼파라미터를 옵티마이징 하는것인데 다른방법에 비하여 얼마나 효율적으로 옵티마이징 하는지 비교하지 않았다. 


### Toy Example 

### 3. Methods
- $a_{1:T}$를 list of action 이라고 하자. 즉 
\begin{align}
a_{1:T}:=\Big \\{a_1,\dots, a_T\Big \\}
\end{align}
이다. 여기에서 $a_1,\dots,a_T$ 는 given $s_1,\dots,s_T$ 일때 $A_1,\dots,A_T$ 의 realization 이다. 

- $R$을 $a_{1:T}$를 넣었을 경우 얻게되는 reward 라고 하자. 

- 에이전트는 아래와 같은 expected reward 를 최대화 하고 싶어 한다. 
\begin{align}
\mathbb{E}_ {\mathbb{P}(a_{1:T} ; \theta_c)} R 
\end{align}


<center><img src="https://github.com/miruetoto/miruetoto.github.io/blob/master/img/NAS/nas_fig1.png?raw=true" width="60%" height="60%"></center>
<center><img src="https://github.com/miruetoto/miruetoto.github.io/blob/master/img/NAS/nas_fig2.png?raw=true"></center>
<center><img src="https://github.com/miruetoto/miruetoto.github.io/blob/master/img/NAS/nas_fig3.png?raw=true"></center>
<center><img src="https://github.com/miruetoto/miruetoto.github.io/blob/master/img/NAS/nas_fig4.png?raw=true"></center>
<center><img src="https://github.com/miruetoto/miruetoto.github.io/blob/master/img/NAS/nas_fig5.png?raw=true"></center>
<center><img src="https://github.com/miruetoto/miruetoto.github.io/blob/master/img/NAS/nas_fig6.png?raw=true"></center>
