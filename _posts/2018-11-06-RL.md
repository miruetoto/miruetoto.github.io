---
layout: post
title: (정리) Reinforcement Learning
---

### About this document

- 이 포스팅은 아래의 교재를 참고하였다. <br/>
Sutton, R. S., & Barto, A. G. (2018). Reinforcement learning: An introduction. MIT press.

--- 

### 3. Finite markov decision process 

- 설명의 편의를 위해서 아래그림과 같이 $4\times 4$개의 격자가 있는 세계에서 로봇이 움직이는 일반적인 예제를 특정하자. 음영처리된 부분에 도착하면 로봇이 더이상 움직이지 않는다고 가정하자. 참고로 이렇게 일정한 시간이 지나면 언제가 끝이나는 task 를 **에피소드-태스크** 라고 한다. 이 예제를 포함하여 바둑이나 장기와 같은것도 일정한 시간이 지나면 언젠가 끝나기 때문에 **에피소드-테스크** 의 한 예이다. 반대로 시간이 지나도 끝나지 않는 task 를 **컨티뉴잉-태스크** 라고 한다.
<center><img src="https://github.com/miruetoto/miruetoto.github.io/blob/master/img/RL/RL_fig1.png?raw=true" height="60%" width="60%"></center>

- **state**: $4\times 4$ 격자위에서 로봇이 움직이고 있으므로 로봇이 존재할 수 있는 all possible states 는 총 16개이다. 여기에서 음영처리된 부분에 로봇이 도착하면 task가 종료되는데 이런 특징을 가지는 상태를 **terminal-state** 라고 한다. 일반적으로 시점 $t$에서 가능한 state 들의 집합 ${\cal S}$은 terminal state를 제외한 집합을 고려한다. 즉 이 예제의 경우는 
\begin{align}
{\cal S}:=\Big\\{1,2,\dots,14\Big\\}
\end{align}
이다. 시점 $t+1$에서는 음영부분 즉 terminal-state 까지 고려한 집합을 생각해야 한다. 이런 집합을 기호로 ${\cal S}^+$ 로 표시한다. 이 예제에서는 
\begin{align}
{\cal S}^+:=\Big\\{0,1,2,\dots,14,15\Big\\}
\end{align}
가 된다. 여기에서 $s=0$ 이 나타내는 state는 $s=1$ 옆의 음영이고 $s=15$ 가 의미하는 state 는 $s=14$ 옆의 음영이다. (사실 이정도는 말안해도 센스로 알것이라 생각한다.) 강화학습에서는 주로 2개의 시점 $t$와 $t+1$을 많이 생각한다. 시점 $t$에서의 상태를 $S_t$ 라고 하고 시점 $t+1$에서의 상태를 $S_{t+1}$이라고 한다. 엄밀하게 말하면 $S_t, S_{t+1}$은 모두 확률변수이다. 확률변수의 realization은 $s_t$와 $s_{t+1}$로 표시하는 것이 마땅할것 같은데 편의상 $s,s'$으로 표시한다. 그리고 일반적으로 아래를 가정한다. 
\begin{align}
\begin{cases}
s \in {\cal S} \\\\ \\
s' \in {\cal S}^+
\end{cases}
\end{align}

- **action**: 로봇이 취할 수 있는 액션을 정의하자. 본디 로봇은 동서남북으로 움직일수 있으므로 로봇이 취할 수 있는 all possible actions은 4가지 행동이다. 따라서 
\begin{align}
{\cal A}=\Big\\{\mbox{up, down, right, left} \Big\\}
\end{align}
다만 경우에 따라서 특정상태에서 취할수 있는 행동에 제약이 있을 수 있다. 가령 예를 들면 위의 예제에서 
\begin{align}
s \in \Big\\{1,2,3,4,7,8,11,12,13,14\Big\\}
\end{align}
인 경우와 같이 가장자리에 위치할 경우 그리드 밖으로 나가게 만드는 action 자체를 금지할 수 있다. 예를 들어서 $s=14$라면 $a=\mbox{down}$ 을 취할 수 없다는 식으로 의미이다. 이처럼 현재시점 $t$에서 가지는 상태 $S_t$에 따라서 action이 달라질 수 있다. 이런 경우를 매 시점 매 상태마다 취할 수 있는 action space가 다르니까 ${\cal A}(S_t)$와 같은 기호를 고려 하는 것이 마땅하다. 여기에서 ${\cal A}(S_t)$ 는 상태 $S_t$에서 로봇이 가질 수 있는 모든 action들의 집합을 의미한다. 즉 
\begin{align}
\begin{cases}
A_t \in {\cal A}(S_t) \\\\ \\
A_{t+1} \in {\cal A}(S_{t+1})
\end{cases}
\end{align}
이다. 혹은 아래처럼 표시하기도 한다. 
\begin{align}
\begin{cases}
a \in {\cal A}(s) \\\\ 
a' \in {\cal A}(s')
\end{cases}
\end{align}

- **reward**: 행동 $A_t$로 부터 얻어지는 보상을 $R_{t+1}$라고 정의한다. 이 책에서 $t$시점의 행동에 대한 보상은 $t+1$ 에 주어진다고 가정하므로 $R_t$가 아니라 $R_{t+1}$ 로 정의하였다. 받을 수 있는 모든 보상의 집합을 ${\cal R}$이라고 정의하자. 예를 들어서 그리드 밖으로 나가면 $-1$점씩, 그리고 terminal-state에 도달하면 +100점씩, 그외에는 무조건 0점씩 준다고 하면 
\begin{align}
{\cal R}=\Big\\{-1,0,100\Big\\}
\end{align}
이다. 

- 그런데 같은 상태에서 같은 행동을 취해도 다른 보상을 줄 수 있다. (될놈될.. 잘 보면 $R_t$가 랜덤변수임..) 따라서 아래식과 같이 상태 $S_t=s$ 에서 행동 $A_t=a$를 취했을 때 얻는 보상 $R_{t+1}$의 평균과 같은 개념을 생각해 볼 수 있다. 
\begin{align}
r(s,a):= \mathbb{E} \Big( R_{t+1} ~ \Big\|~ S_t=s, A_t=a \Big)
\end{align}
이것을 **expected rewards for (state,action) pairs** 라고 한다. 주의할것이 $r$은 $t+1$시점에서의 보상 $R_{t+1}$의 실현값이고 $r(s,a)$는 given $(s,a)$에서 $R_{t+1}$의 평균값이라는 것이다. 이것은 아래처럼 계산가능하다. 
\begin{align}
r(s,a):= \sum_{r \in {\cal R}} r \sum_{s' \in {\cal S}}  p(s',r~\|~ s,a) 
\end{align}
여기에서 $p(s',r~ \| ~ s,a)$는 말 그대로 $(s,a)$가 given 일때 $(s',r)$가 일어날 확률을 의미한다. 

- **probability of $(s',r)$ given $(s,a)$**<br/><br/>
환경은 $(s,a)$가 정해지면 $(s',r)$을 던져준다. 이 확률을 $p(s',r ~ \| ~ s,a)$ 라고 한다. 즉 
\begin{align}
p(s',r ~ \| ~ s,a) := Pr\Big( S_{t+1}=s', R_{t+1}=r ~ \Big\| ~ S_t=s, A_t=a \Big).
\end{align}
이다. 위의 확률은 아래와 같은 함수로 해석가능하다. 
\begin{align}
{\bf\tilde  P}: {\cal S} \times {\cal A} \times {\cal R} \times {\cal S}^+  \to [0,1]
\end{align}
이다. 여기에서 **틸드**를 쓰는 이유는 conditionality를 강조하기 위함이다. 아무튼 ${\bf \tilde P}$를 정의하기 위해서는 크기가 
\begin{align}
\Big(\|{\cal S}\|,\|{\cal A}\|,\|{\cal R}\|,\|{\cal S}^+\|\Big)
\end{align}
인 4차원 array 혹은 tensor 에 각각 $[0,1]$ 사이의 값을 코딩해야 한다. 편의상 이러한 array 를 ${\bf\tilde  P}[s,a,r,s']$이라고 생각하자. 확률에 $0$값을 줄 수 있다는 것을 이용하면 
\begin{align}
{\bf\tilde  P}: {\cal S}^+ \times {\cal A} \times {\cal R} \times {\cal S}^+ \to [0,1]
\end{align}
이라고 정의해도 괜찮다. 따라서 일반성을 잃지 않고 ${\cal S}^+ = {\cal S}$ 라고 놓아도 무방하다. 
\begin{align}
{\bf\tilde  P}: {\cal S} \times {\cal A} \times {\cal R} \times {\cal S}  \to [0,1]
\end{align}
로 정의할 수 있고 ${\bf\tilde  P}[s,a,r,s']$ 의 차원을 $\Big(\|{\cal S}\|,\|{\cal A}\|,\|{\cal R}\|,\|{\cal S}\|\Big)$ 로 생각해도 무방하다. 

- 여기에서 ${\bf\tilde  P}[s,a,r,s']$ 은 **환경(environment)** 가 가지고 있는 궁극의 테이블 (혹은 비밀노트?) 라고 보면 된다. 환경이 에이전트에게 주는 모든 종류의 피드백은 ${\bf\tilde  P}[s,a,r,s']$ 에 근거한다. 예를들어 ${\bf\tilde  P}[s,a,r,s']$ 를 이용하면 아래와 같이 **state-transition probabilites** 를 구할 수 있다. 
\begin{align}
p(s'|s,a):= Pr\Big(S_{t+1}=s'~  \Big\| ~ S_t=s, A_t=a \Big)= \sum_{r \in {\cal R}}p(s',r~|~s,a):=P_{ss'}^{a}.
\end{align}
위의 식은 그냥 array ${\bf P}$에서 $r$차원을 marginally out 한 것이다. 또한 ${\bf P}[s,a,r,s']$ 로 부터 **expected rewards for (state,action,next-state) triples** 을 아래와 같이 구할 수 있다. 
\begin{align}
r(s,a,s'):=\mathbb{E}\Big( R_{t+1} ~~ \Big\|  ~~ S_t=s, A_t=a, S_{t+1}=s' \Big)=\frac{\sum_{r \in {\cal R}}rp(s',r|s,a)}{p(s'|s,a)}:=R_{ss'}^{a}.
\end{align}
위에서 정의된 $P_{ss'}^{a}$와 $R_{ss'}^{a}$를 ${\bf P}[s,a,r,s']$ 를 활용하여 얻어내는 방법 즉 코딩하는 방법도 생각해보자. 노동력 낭비라 생각해서 여기에 답을 쓰진 않겠다. 하지만 한번씩 이렇게 생각해보는 것이 내용을 이해하는데 도움이 될 것이다. (원래 책 읽으면서 자신만의 언어로 잘 바꾸면서 읽어야함) 

- **policy** : 환경이 가지고 있는 궁극의 테이블이 ${\bf P}[s,a,r,s']$ 이라고 언급하였다. 에이전트가 가지는 궁극의 테이블은 무엇인가? 그것은 바로 아래와 같이 정의되는 **policy** 이다. 
\begin{align}
\pi_t:=\pi_t(a|s):=\mathbb{P}\Big(A_t=a ~ \Big\| ~ S_t=s\Big) \in \Pi.
\end{align}
이건 간단하게 말해서 에이전트가 상태 $S_t=s$ 에서 행동 $A_t=a$ 을 할 확률을 의미한다. 따라서 polish는 차원이 ${\cal S} \times {\cal A}$ 인 ${\boldsymbol \Pi}[s,a]$ 와 같은 테이블에 $[0,1]$ 사이의 값들을 기록한 것으로 생각 할 수 있다. 포스팅 도입부에 소개한 $4\times 4$ 그리드가 있는 예제를 다시 떠올려보자. 에이전트가 $s$에 상관없이 로봇을 동서남북 아무방향이라 랜덤으로 움직이는 정책을 가지고 있다 가정하면 
\begin{align}
\forall (s,a) \in {\cal S}\times{\cal A}: {\boldsymbol \Pi}[s,a] = 0.25 
\end{align}
와 같이 된다. 

- 환경이 가진 테이블과 다르게 에이전트가 가진 테이블 **policy** 는 매시간 변경가능하다. 변경을 하는 이유는 좀 나은 $G_t$값을 얻기 위해서이다. 여기에서 $G_t$를 **expected return** 이라고 하고 아래와 같이 정의한다. 
\begin{align}
G_t=R_{t+1}+\gamma R_{t+2}+\gamma^2 R_{t+3}+\dots = \sum_{k=0}^{\infty}\gamma^kR_{t+k+1}
\end{align}
$G_t$는 단기보상 $R_{t+1}$와 장기보상 $\sum_{k=1}^{\infty}\gamma^kR_{t+k+1}$ 으로 나눌 수 있다. 

- **value function**
\begin{align}
v_{\pi}(s):=\mathbb{E}_ {\pi} \Big(G_t ~~ \Big\| ~~ S_t=s\Big) = \mathbb{E}_ {\pi} \bigg( \sum_{k=0}^{\infty}\gamma^k R_{t+k+1} \bigg\| S_t=s\bigg).
\end{align}

- **action-value function for policy $\pi$**
\begin{align}
q_{\pi}(s,a):=\mathbb{E}_ {\pi} \Big(G_t ~~ \Big\| ~~ S_t=s, A_t=a \Big)=\mathbb{E}_ {\pi}\bigg(\sum_{k=0}^{\infty}\gamma^k R_{t+k+1} \bigg\| S_t=s, A_t=a \bigg).
\end{align}

- **optimal polish** 
\begin{align}
\pi^*:= \Big\\{\pi: v_{\pi}(s) \geq v_{\pi'}(s) \mbox{ for all } s \in {\cal S} \mbox{ and } \pi' \in {\Pi} \Big\\} 
\end{align}

- **optimal state-value function** 
\begin{align}
v_{* }(s):=\max_{\pi} v_{\pi}(s), \quad \forall s \in {\cal S}
\end{align}

- **optimal action-value function**
\begin{align}
q_{* }(s,a):= \max_{\pi}q_{\pi}(s,a), \quad \forall s \in {\cal S} \mbox{ and } a \in {\cal A}(s)
\end{align}

- 이때 **optimal state-value function** 와 **optimal action-value function** 의 정의를 잘 생각해보면 아래와 같은 관계가 성립함을 알 수 있다. 
\begin{align}
q_{* }(s,a):= \mathbb{E} \Big(R_{t+1} + \gamma v_{* }(S_{t+1}) ~~ \Big\| ~~ S_t=s, A_t=a \Big). 
\end{align}

--- 

- **(Bellman equation)** 
\begin{align}
v_{\pi}(s)=\sum_{a}\pi(a|s)\sum_{s',r}p(s',r|s,a)\Big[r+\gamma v_{\pi}(s')\Big] 
\end{align}

- **(Bellman equation of $q$)** 
\begin{align}
q_{\pi}(s,a)=\sum_{s',r} p(s',r | s,a)\Big[r+\gamma v_{\pi}(s')\Big]. 
\end{align}

- **(Bellman optimality equation)** 
\begin{align}
v_* (s)=\max_{a \in {\cal A}(s)}\sum_{s',r} p(s',r | s,a)[r+\gamma v_* (s')]. 
\end{align}

- **(Bellman optimality equation for $q_* $)** 
\begin{align}
q_* (s,a)=\sum_{s',r} p(s',r | s,a)\Big[r+\gamma \max_{a' \in {\cal A}(s')}q_* (s',a')\Big]. 
\end{align}

---

### 4. Dynamic Programming 

- Given $\pi$ 에서 모든 $s \in {\cal S}$에 대한 $v_{\pi}(s)$ 를 계산하는 방법은 $k=1,2,\dots$에 대하여 아래를 반복하는 것이다.
\begin{align}
v_{k+1}(s)=\sum_{a}\pi(a|s)\sum_{s',r}p(s',r|s,a)\Big[r+\gamma v_{k}(s')\Big] 
\end{align}

- Given $\pi$ 에 대하여 $v_{\pi}(s)$를 계산하였다고 치자. 그러면 모든 상태 $s \in {\cal S}$ 에서 정책 $\pi$를 따를 경우 평균적으로 어떠한 보상을 받을지 알 수 있다. 하지만 그렇다고 해서 정책 $\pi$를 바꿔서 새로운 정책 $\pi'$을 얼마나 좋은 정책인지는 알 수 있지만 (we know how good it is to follow the current policy from $s$) 

--- 
### 간단한 그리드세계에서 Q러닝과 살사 

- $4\times 4$개의 격자가 있는 세계를 상상하자. 로봇이 움직일 수 있는 공간은 총 16개이다. 각각의 상태를 아래와 같의 정의하자. 편의상 ${\cal S}:=\\{s_1,\dots,s_{16}\\}$이라고 정의하자. 로봇이 취할 수 있는 액션을 정의하자. 로봇은 동서남북으로 움직일수 있으므로 4가지 행동을 취할 수 있다.
편의상 ${\cal A}:=\\{a_1,\dots,a_4\\}$라고 정의하자. 특정 상태 $s\in {\cal S}$에서 다른 특정 상태 $s'\in {\cal S}$로 이동할 확률을 모두 정의한 매트릭스 ${\bf P}$를 만들고 transition matrix라고 하자. 위의 예제에서는 $|{\cal S}|=16$이다. 따라서 ${\bf P}$의 차원은 $16 \times 16$이다. 위의 예제에서 초기상태 $s$에서 다음상태 $s'$으로 이동할 확률이 모두 $\frac{1}{16}$로 동일하다면 ${\bf P}=\frac{1}{16}{\bf J}$와 같이 정의할 수 있다. 

- 강화학습은 보통 MDP에서 정의되므로 트랜지션 매트릭스 $P$는 액션을 고려하여 다음과 같은 함수가 되어야 함이 마땅하다.
\begin{align}
P:{\cal S}\times{\cal A} \rightarrow {\cal S}
\end{align}  
즉 이전상태와 이전상태에서의 액션이 기븐되어야 다음상태를 알 수 있다. 따라서 아래와 같이 트랜지션 매트릭스를 정의한다.<br/><br/>
```
P<-rep(0,16*4*16)
dim(P)<-c(16,4,16)
P[1,1,1]<- somevalue
...
P[16,4,16]<- somevalue
```  
여기에서 $P[16,4,16]$은 상태 $s[16]$에서 적당한 액션 $a[4]$를 취했을때 상태 $s[16]$으로 전환될 확률을 의미한다. 그나마 게임은 룰이 명확하기 때문에 $P$를 정의할 수 있다. 이때 $P[s,a,s']=P(s'|s,a)$이다. 
  
- 보상함수에 대하여 정의하자. 가령 예를들면 보상함수를 아래와 같이 정의할 수 있다. 
  1. 상태 $s[1]$혹은 상태 $s[16]$에 도달하면 5점의 점수를 받는다.  

  2. 그리드 밖으로 나가면 -1점의 점수를 얻는다. (그리고 다음상태는 이전상태와 동일하다)  
  
  3. 그외에 경우에는 0점의 점수를 받는다.  


- 보상은 16개의 상태에서 4개의 행동을 하는 경우에서 모두 정의할 수 있다. 따라서 보상 $r(s,a)$은 아래와 같이 쓸 수 있다.<br/><br/>
```
r<-rep(0,16*4)
dim(r)<-c(16,4)
```
그런데 같은상태에서 같은행동을 취해도 다른보상을 줄 수 있다. (될놈될..) 따라서 위에 정의된 $r[s,a]$들은 사실 보상들의 평균이라고 보는 것이 옳은 해석이다. 예컨데 $r[s,a]$는 상태 $s$에서 행동 $a$를 취하였을때 얻으리라 기대되는 평균적인 보상값이다. 보통 $r[s,a]$는 모두 클리어하게 정의할 수 있는데 이것은 우리가 MDP를 가정하기 때문이다. 

- 이때 보상 $r$이 현재상태와 현재상태의액션에 대한 함수이지 다음상태에 대한 함수는 아님을 기억하자. 즉 나중상태의 정보만으로 보상을 완벽히 정의할 수 없다는 의미이다. 예를 들어서 로봇이 $s'=(3,4)$의 위치에 있다고 하자. 1) $s=(2,4)$이고 $a=a[1]$이어서 $s'=(3,4)$가 된 경우에는 보상이 0이다. 2) 하지만 $s=(3,4)$였는데 $a=a[4]$와 같이 되어서 그리드밖으로 튀어나가 다음상태가 $s'=(3,4)$가 된 경우는 보상이 -1이다. 다시 말하면 다음상태 $s'=(3,4)$가 같다고 해서 항상 그 보상이 같은 것은 아니다. 

- 특정 상태 $s \in {\cal S}$가 기븐되었을 경우 특정 액션 $a \in {\cal A}$을 선택할 확률을 정의한 매트릭스 $\pi$를 만들자. 이를 정책이라고 하자. 예를들어 각 상태 16개에서 선택할 수 있는 액션의 확률이 모두 동일한 정책 $\pi_1(a|s)$은 아래와 같이 정의할 수 있다.<br/><br/>
```
pi_1<-rep(1/4,16*4)
dim(pi_1)<-c(16,4) 
```  
이때 $\pi_1[s,a]=\pi_1(a|s)=P(a|s)$이다. 

- 특정 상태 $s \in {\cal S}$에 대하여 정책 $\pi_1(a|s)$가 좋은 정책인지 나쁜 정책인지 평가할 수 있다. 이는 특정상태에서 정책 $\pi_1(a|s)$을 썼을때 얻게되는 보상의 기대값으로 정의할 수 있다. 이런식으로 모든 상태 $s \in {\cal S}$에 대하여 정책 $\pi_1(a|s)$를 썼을때 보상의 기대값을 계산할 수 있는데 이를 정책 $\pi_1(a|s)$대한 가치함수라고 하고 기호로는 $v_{\pi_1}:{\cal S}\rightarrow \mathbb{R}$로 쓴다. 
\begin{align}
v(\pi_1,s):=v_{\pi_1}(s)=\sum_{a\in {\cal A}}\pi_1(a\vert s)\left(r(s,a)+\gamma \sum_{s' \in {\cal S}}P(s' \vert s,a)v_{\pi_1}(s')\right)
\end{align}

- 특정정책 $\pi_1(a \vert s)$이 고정되면 가치함수를 계산할 수 있다. 최적가치함수는 최적의정책 $\hat{\pi}(a \vert s)$로 부터 계산되는 가치함수 $v_{\pi}(s)$이다. 즉 최적의정책을 알고 있다면 최적의가치함수를 계산할 수 있다. 반대로 최적의가치함수값을 알고있으면 그것을 통하여 최적의정책을 찾을 수 있다. 따라서 결국 강화학습의 문제는 결국 ***최적의 value function을 찾아보자!*** 로 요약된다. 

- 최적의가치함수를 찾는 과정으로 **polish iteration 알고리즘**이 있다. 이것은 초기에 임의의 정책을 초기화하고 그담에 그것을 바탕으로 가치함수를 계산하고 계산된 가치함수를 활용하여 정책을 업그레이드하고 다시 그것으로 가치함수를 계산하는 식으로 진행한다. 반복이 진행될수록 정책은 점점 최적정책 $\hat{\pi}(a \vert s)$로 수렴하고 가치함수는 점점 최적가치함수 $v_{\hat{\pi}}(s)$로 수렴할 것이라 기대된다. 

- **polish iteration 알고리즘**은 매우 시간이 많이 걸린다. 사실 특정한 정책이 결정되었을 경우 가치함수를 계산하는 것이 말처럼 쉬운일이 아니다. 왜냐하면 특정 정책이 결정되었을 경우 가치함수를 계산하려면 순환식을 풀어야 하기 때문이다. 이런식으로 풀면 먼저 목표지점에 인접한 셀들의 가치함수가 먼저 계산되고 이 값들이 바깥의 상태로 전파되면서 값들이 결정된다. 

- 가령 예를 들어서 모든 셀에서 동서남북 랜덤으로 움직이는 정책 쓴다고 하자. 모든셀의 가치는 처음에 $0$으로 초기화 하자. 목표지점은 $(1,1)$과 $(4,4)$ 2군데 이다. 이 정책에 의해서 상태$(1,2)$가 가지는 평균가치는 $\frac{5+0+0-1}{4}=1$으로 계산된다. 요런식으로 모든 셀에 대한 가치를 구할 수 있다. 

- 좀 더 스마트하게 가치를 계산할 수 있다. 현재 상황은 $P(s' \vert s,a)$를 완벽하게 알고 있는 상황이기 때문에 현재상태 $(1,2)$에서 어떤 액션을 취해야 셀 $(1,2)$의 가치가 최적화 되는지 알 수 있다. (***남쪽으로 가야한다!!***) 어떻게 행동해야 최적인지 이미 계산가능한 상황에서 정책 $\pi_1$를 수동적으로 따르는 것은 어리석으므로 $\pi_1$에서 $(1,2)$에 해당하는 부분만 바꿔서 수정하고 그거에 따른 가치함수를 계산한다. 즉 $\pi_1(a \vert s=(1,2))$를 기존의 동서남북모두에 $1/4$의 확률을 주는것에서 남쪽으로 가는 액션에만 $1$의 확률을 주게 바꾼다. 그리고 바뀐정책을 기반하여 가치함수를 계산한다. (계산값은 $5$이다.) 즉 가치함수를 아래의 식으로 업데이트한다. 
\begin{align}
v(s)^{(t+1)}=\max_{a \in {\cal A}^{(t)}(s)}\pi(a \vert s)\left(r(s,a)+\gamma \sum_{s' \in {\cal S}}P(s' \vert s,a)v^{(t)}(s')\right)
\end{align}

- 여기에서 
\begin{align}
r(s,a)+\gamma \sum_{s' \in {\cal S}}P(s' \vert s,a)v^{(t)}(s')
\end{align}  
는 상태 $s$에서 행동$a$가 결정되었을 경우 얻게 되는 보상의 기대값으로 정의될 수 있다. 이를 보통 $q_{\pi}(s,a)$라고 정의하고 **큐함수**라고 부른다. 큐함수는 단기보상과 장기보상으로 나눌수 있다. 즉 위의 식에서 $r(s,a)$는 단기보상이 되고, $\gamma \sum_{s' \in {\cal S}}P(s' \vert s,a)v^{(t)}(s')$는 장기보상이 된다. 이때 장기보상을 구하기 위해서 정책정보 $\pi$가 필요하므로 $q_{\pi}(s,a)$는 $\pi$에 의존하는 함수가 된다. 

- 큐함수는 value function과 동등한 역할을 한다. 강화학습의 목표는 최적의 value function을 찾거나 최적의 큐함수를 찾거나 둘중에 하나만 이루면 된다. 

- 이때 ${\cal A}^{(t)}(s)$는 현재시점 $t$에서 정책을 고려하였을때 상태 $s$에서 선택가능한 액션들의 집합을 의미한다. 식
\begin{align}
\max_{a \in {\cal A}^{(t)}(s)}\pi(a \vert s)q_{\pi}(s,a)
\end{align}  
을 계산하여 $v(s)^{(t+1)}$를 업데이트 할 때 ${\cal A}^{(t)}(s)$의 값도 ${\cal A}^{(t+1)}(s)$로 업데이트한다. 위의 예제로 예를들면 ${\cal A}^{(0)}=\\{a[1],a[2],a[3],a[4]\\}$에서 ${\cal A}^{(1)}=\\{a[3]\\}$으로 업데이트 한다. (남쪽으로 가야한다!!) 이러한 방식으로 정책을 부분적으로 수정하면서 가치함수를 계속 업데이트 하는 방식을 **value iteration 알고리즘**이라고 한다. 

- **polish iteration 알고리즘**은 최적의정책과 최적가치함수를 동시에 개선하는 방식이지만 **value iteration 알고리즘**은 최적가치함수를 먼저 찾고 그러부터 최적의정책을 찾는 방식이다. 따라서 **value iteration 알고리즘**이 더 빠르다. 

- 지금까지는 $P(s' \vert s,a)$를 완벽하게 알고 있다고 가정하고 문제를 푸는 방법들이었다. 이걸 모르면 어떻게 할 것인가? 그냥 특정한 정책 $\pi$에서 에피소드 $e$를 생성한다. 마코프가정을 만족하니까 하나의 에피소드 $e$에서 여러개의 ***상태샘플*** $z \in Z(s)$ 혹은 ***상태행동샘플*** $z \in Z(s,a)$가 수집할 수 있다. (알파고의 경우 16만개의 기보(=에피소드 $e$)에서 3천만개의 샘플 $z$를 수집했다고 함). 각 샘플에 대하여 보상함수 $r(z)$를 구한다. 이것을 가지고 아래와 같이 $v_{\pi}(s)$ 혹은 $q_{\pi}(s,a)$를 근사시킨다. 즉 
\begin{align}
v_{\pi}(s)=\frac{1}{|Z(s)|}\sum_{z \in Z(s)} r(z)
\end{align}
혹은 
\begin{align}
q_{\pi}(s,a)=\frac{1}{|Z(s,a)|}\sum_{z \in Z(s,a)} r(z)
\end{align}
와 같이 한다. 

- 요런식으로 샘플을 만들어서 하면 환경모델이 없어도 된다는 장점이 있다. 또한 특정상태만 골라서 그것에 대해서만 가치함수를 계산하여 계산량을 줄일수도 있다. 또 마코프성질을 크게 벗어나는 경우에도 성능저하가 심하지 않다는 장점이 있다. 이러한 방식을 몬테카를로 방식이라고 한다. 

- 하지만 이러한 방식은 하나의 에피소드가 끝나서 보상이 결정되기 전까지는 업데이트가 이루어지지 않는다. 유한에피소드이고 에피소드가 빨리 끝나면 다행이지만 그렇지 않으면 어떻게하는가? 에피소드가 끝나기 전에 업데이트를 하면 좋겠다. 이게 템포랄-디퍼런스의(temporal-differencce)의 핵심이다. 

- 템포랄-디퍼런스 방식은 살사와 Q러닝이 있다. 

- 살사는 어떻게 동작하는가? 
1) 먼저 특정상태 $s$를 랜덤하게 생성한다. 2) $\max_{a \in {\cal A}} q(s,a)$를 만족하는 액션 $a$를 선택한다. 3) 이 선택에 따라서 $r(s,a)$를 받고 다음상태 $s'$로 이동한다. 4) $s'$에서도 $\max_{a' \in {\cal A}} q(s',a')$를 만족하는 액션 $a'$를 선택한다. 5) 샘플 $(s,a,r(s,a),s',a')$을 모으면 이것을 이용하여 아래와 같이 업데이트 한다. 
\begin{align}
q(s_t,a_t) \leftarrow (1-\rho) q(s_t,a_t) + \rho \left(r(s_t,a_t) + \gamma q(s_{t+1},a_{t+1})\right)
\end{align}

- 살사의 약점은 무엇인가? $a_t$를 매우 적절하게 선택하여도 (탐험에 의해서) $a_{t+1}$가 잘못선택되면 $q(s_t,a_t)$의 값도 같이 낮아진다는 것이다. 따라서 $q(s_t,a_t)$의 값이 낮은 원인이 1) $a_t$가 잘못되었는지 2) $a_{t+1}$이 잘못되었는지 알 수 없다는 것이다. 이 한계를 극복한게 Q러닝인데 Q러닝은 아래의 식을 통하여 $q(s_t,a_t)$를 업데이트한다. 
\begin{align}
q(s_t,a_t) \leftarrow (1-\rho) q(s_t,a_t) + \rho \left(r(s_t,a_t) + \gamma \max \\{q(s_{t+1},a[1]),\dots,q(s_{t+1},a[4])\\}\right)
\end{align}
이렇게 되면 $s_{t+1}$에서의 잘못된 선택 $a_{t+1}$에 의해서 $q(s,a)$가 낮아질 일이 없다. 즉 $q(s,a)$는 오로지 $s_t$시점에서 행동한 액션 $a_t$에 대한 패널티 혹은 상이된다. 요런방식을 off-polish방식이라고 한다. 

--- 

### 복잡한 그리드월드 

- 강화학습의 문제는 $q(s,a)$를 잘 구하면 게임이 끝난다고 하였다. 그리드 세계에서는 $(s,a)$의 조합수가 유한하기 때문에 모든 조합수에 대한 단+장기보상을 조사하면 되었다. 그런데 $(s,a)$의 조합수가 무궁무진하다면? 가령 예를 들면 로봇이 $(0,0)$, $(0,1)$이런곳에만 위치할 수 있는 것이 아니고 $(0,3.45)$와 같은 곳에도 위치할 수 있다면?? $(s,a)$의 모든 조합수를 표시할 테이블을 만들수도 없거니와 설령 만든다 해도 그 테이블에 있는 모든상태에 대해서 테이블의 값을 업데이트 하는건 너무나 멍청한 짓이다. 하지만 모든 $s \in [0,1] \times [0,1]$에 대하여 보상값을 기록하겠다는 생각을 버리면 의외로 문제가 간단하다. 바로 함수의 근사를 이용하는 것이다. 

- 우선 이해를 위해서 살사알고리즘을 복습해보자. ${\cal S}=\\{1,2,3,4\\}$이라고 하자. ${\cal A}=\\{1,-1\\}$이라고 하자. 각 상태는 1~4까지 1차원으로 있으며 각 상태에서 로봇은 오른쪽으로 1만큼($a=1$) 혹은 왼쪽으로 1만큼($a=-1$) 움직일 수 있다. 상태 3에 가면 1만큼 보상을 받는다. $s=1$에서 왼쪽으로 가는 선택을 하면 그리드 밖으로 나가게 되고 이경우 $-1$의 보상을 받는다. 나머지는 모두 $0$의 보상을 받는다. $q(s,a)$의 도메인은 $3\times 2$ 개의 셀을 가진 테이블로 표현가능하다. 살사알고리즘을 사용하여 $q(s,a)$를 어떻게 업데이트 하는지 생각해보자. $q(s,a)$는 초기에 모두 0으로 셋팅한다. 1) 처음에 상태 $s$를 생성한다. 2) $\epsilon$탐욕에 의해서 $q^{old}(s,a)$로 부터 행동 $a$를 선택한다. 그런데 초기에는 모두 $q(s,a)$이 0이므로 랜덤한 행동을 선택할 것이다. 선택된 행동이 $a=-1$이다. 그러면 $-1$의 보상을 받고 종료된다. 이떼 $q(s,a)$는 아래와 같은 식으로 업데이트 된다. 
\begin{align}
q^{new}(s,a) \leftarrow (1-\rho)q^{old}(s,a) + \rho \left( r(s,a)+\gamma q^{old}(s',a') \right) 
\end{align}
따라서 $q(1,-1)= -1$로 업데이트가 된다. 

- 이제 다시 1) 상태 $s$를 생성한다. 생성된 상태는 또 $s=1$이다. 2) $q(1,-1)=-1$이고 $q(1,1)=0$이므로 탐욕에 의해서 $a=1$인 행동을 선택한다. 3) 상태 2로 가고 보상으로 0을 받는다. 4) 상태2에서는 $q(2,1)=q(2,-1)=0$이므로 또 아무행동이나 선택한다. $a'=1$을 선택하였다. 5) 여기까지 진행하면 샘플 $(s,a,r,s',a')=(1,1,0,2,1)$이 수집된다. $(s,a)=(1,1)$에서의 업데이트를 하자. 
\begin{align}
q^{new}(1,1) \leftarrow (1-\rho)q^{old}(1,1)+ \rho \left( r(1,1)+\gamma q^{old}(2,1) \right)
\end{align}
여기에서 $q^{old}(1,1)=0$, $r(1,1)=0$, $q^{old}(2,1)=0$이므로 $q^{new}(1,1)=0$으로 된다. (그니까 여기서는 업데이트는 일어나지 않았음) 

- 아직 에피소드가 끝난것이 아니다. $(s,a)=(2,1)$에 대한 업데이트도 해야한다. 보상은 $r(2,1)=0$을 받을 것이고 편의상 $s'=3$에서 (운좋게) $a'=1$을 선택했다고 하자. 샘플은 $(s,a,r,s',a')=(2,1,0,3,1)$이다. $q^{new}(2,1)=0$이 될 것이다. 그리고 $q^{new}(3,1)=1$이 된다. 이제 $q(s,a)$는 아래와 같다. <br/><br/>
  1. $q(1,-1)=-1$, $q(1,1)=0$. 
  2. $q(2,-1)=0$, $q(2,1)=0$. 
  3. $q(3,-1)=0$, $q(3,1)=1$. <br/>
<br/>

- 에피소드를 더 만들어보자. $s=2$를 만들면 $(s,a)=(2,-1)$ 혹은 $(s,a)=(2,1)$중에 하나의 행동을 할것이다. 만약에 $(s,a)=(2,-1)$을 한다면 보상으로 0을 받고 $(s',a')=(1,1)$이 만들어져서 다시 $s=2$상태로 올 것이다. 이때 $q(2,-1)=0$으로 업데이트된다. 만약에 $(s,a)=(2,1)$을 선택하면 보상으로 0을 받고 $(s',a')=(3,1)$이 만들어진다. $q(3,1)=1$이므로 $q(2,1)=\gamma$로 업데이트 된다. 따라서 최종적인 수렴결과는 아래와 같다. <br/><br/>
  1. $q(1,-1)=-1$, $q(1,1)=0$.
  2. $q(2,-1)=0$, $q(2,1)=\gamma$.
  3. $q(3,-1)=0$, $q(3,1)=1$. <br/>
<br/>

### DQN 

- 이제까지 살사알고리즘을 활용하여 $q: {\cal S}\times {\cal A} \rightarrow \mathbb{R}$를 알아보았다. 이제 DNN을 활용하여 $q(s,a)$를 찾는 방
법을 고려하여 보자. 레이어를 1개만 쌓는다면 모델은 $q(s,a)=f(s w_1+a w_2)$와 같은 꼴이 될것이다. loss를 아래와 같이 준다. 
\begin{align}
\left(q(s_t,a_t) - r(s_t,a_t)+\gamma q(s_{t+1},a_{t+1})\right)^2
\end{align}
이런식으로 설계를 하자. 살사알고리즘에서 활용한 예제를 다시 복귀시켜보자. <br/><br/>
  1. 일단 $q(s,a)=0$으로 셋팅한다. 
  2. $(s,a)=(1,-1)$상황에서 보상 $r(1,-1)=-1$로 받고 종료됨. 다음상태없음. <br/>
<br/>

- 우선 여기까지 진행이되면 
\begin{align}
(q(1,-1) - r(1,-1))^2
\end{align}
를 최소화 하기 위해서 $q(1,-1)\approx 1$인 곡면 $f(s w_1+a w_2)$을 DNN이 찾기위해서 노력한다. 곡면이 구해지면 $q(s,a)$를 업데이트하고 다시 샘플을 얻는다. 

- 지금은 $q: {\cal S} \times {\cal A} \rightarrow \mathbb{R}$을 근사한다고 생각하였지만 $q : {\cal S} \rightarrow {\cal A} \times \mathbb{R}$을 근사한다고 생각할 수 도 있다. (왜 인지 모르겠지만 이렇게 한다고 한다.) 즉 DNN은 각 입력에 $s \in {\cal S}$대하여 액션 $a \in {\cal A}$와 그 액션에 해당하는 큐함수값 $q(s,a)$를 반환한다고 생각할 수 도 있다. 다시 위의 예제를 떠올려보자. 2번의 에피소드를 돌렸고 4개의 샘플을 모았다. 지금까지 샘플은 아래와 같다. <br/><br/> 
  1. $(s,a,r,s',a')=(1,-1,-1,d,d)$. 
  2. $(s,a,r,s',a')=(1,1,0,2,1)$.
  3. $(s,a,r,s',a')=(2,1,0,3,1)$.
  4. $(s,a,r,s',a')=(3,1,1,d,d)$.<br/>
<br/>

- 처음부터 생각하여 보자. 이 경우 DNN노드의 입력은 $s$이다. 즉 1차원의 값을 입력으로 받는다. 출력은 2차원이다. 따라서 최종적인 유닛은 2개이어야 한다. 레이어를 2개쌓고 처음에는 유닛을 3개 그담에는 유닛을 2개만 만들자. 따라서 첫번째 레이어에서는 가중치매트릭스를 $W1_ {1 \times 3}$ 와 같이 정의하고 두번째 레이어에서는 가중치매트릭스를 $W2_ {3 \times 2}$와 같이 정의한다. 활성화 함수 $f$는 렐루를 쓴다. 구조는 아래와 같다. <br/><br/>
  1. $y[i,1]=f(w2[1,1]f(w1[1]s[1])+w2[2,1]f(w1[2]s[1])+w2[3,1]f(w1[3]s[1])$
  2. $y[i,2]=f(w2[1,2]f(w1[1]s[1])+w2[2,2]f(w1[2]s[1])+w2[3,2]f(w1[3]s[1])$ <br/><br/>

-
