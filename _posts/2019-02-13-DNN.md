---
layout: post 
title: (정리) DNN 
---

### About this document
- 이 포스트는 DNN에 대하여 다룬다.

### Overview
***신경망의 역사***
- 1943년에 최초의 신경망이 매컬럭(Maculloch)과 피츠(Pits)에 의해 제안되었다. 1958년에는 로젠블렛(Rosenblatt)이 퍼셉트론을 제안하였다. 

- 1960년대에는 신경망으로 모든 문제를 해결할것처럼 과장되어 학계와 매스컴의 주목을 받았다. 하지만 1969년에 민스키(Minky)와 페퍼트(Papert)는 퍼셉트론의 한계를 수학적으로 증명하였다(퍼셉트론은 선형분류기에 불과하므로 XOR 분류도 하지 못함). 이후에 신경망 연구는 크게 퇴조하였다. 

- 1986년에 루멜하트(Rumelhart)와 동료들이 은닉층을 가진 다층 퍼셉트론과 오류 역전파알고리즘을 제안하였다. 이 신경망이 필기숫자인식과 같은 문제를 훌륭하게 해결하여 이후에 다시 신경망연구가 활기를 찾았다. 하지만 역전파알고리즘은 층수가 많은 깊은 신경망일수록 학습이 되지 않는 문제가 생겼다. 정확히 말하면 트레이닝에러는 줄어들지만 테스트에러는 줄어들지않는 과적합 문제가 생겼다. 

- 1990년대에 들어 SVM이 신경망보다 좋은 성능을 보여 신경망은 잠시 SVM에 밀리는 형국이 되었다. 이후 현재에는 다시 신경망이 기계학습의 주류로 자리잡았다. 

***신경망의 종류***
- 앞먹임신경망과 순환신경망: DNN(=DMLP)과 CNN과 같은것을 feedforword 신경망이라 하며 RNN이나 LSTM과 같은것을 순환신경망이라고 한다. 

- 얕은신경망과 깊은신경망: 층이 1~2개정도를 얕은신경망이라 하며 은닉층이 두꺼운 것을 깊은신경망이라고 한다. 

- 결정론신경망과 스토캐스틱신경망: 결정론 신경망에서는 입력이 같으면 항상 같은 출력이 나온다. 계산식에 임의성이 없다. 스토캐스틱신경망은 계산식에 난수가 들어가 입력이 같아도 매번 다른 출력이 나온다. RBM과 DBN이 대표적인 스토캐스틱신경망이다. 결정론적신경망은 분류나 회귀와 같은 예측만 가능하지만 스토캐스틱신경망은 예측뿐아니라 유사한 패턴을 생성하는 생성모델로도 활용가능하다. 

### DNN 아키텍처 

- 일반적으로 교재에 설명되어 있는 DNN아키텍쳐의 경우 하나의 observation만 고려한다. 왜냐하면 아키텍쳐는 observation별로 모두 동일하기 때문이다. 즉 하나의 observation에 대한 아키텍처만 설계하면 다른 observation에 대한 아키텍쳐도 동일하게 **copy and paste** 로 설계하면된다. 

- $i$번째 관측치의 DNN은 대략 아래와 같은 느낌의 구조를 가지고 있다. 
\begin{align}
{\bf y}_ i = f_2\big(f_1({\bf x}_ i {\bf W_1}+{\bf b_1}){\bf W_2}+{\bf b_2}\big){\bf W_3}+{\bf b_3} + {\boldsymbol\epsilon}_ i
\end{align} 
각 요소들의 디멘젼을 **굳이 따지면** 아래와 같이 정리할 수 있다(이런거 까지 다 써야하나 싶긴하지만 처음 한번은 디멘젼을 따져보는게 좋다).

<center><style type="text/css">
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}
.tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}
.tg .tg-c3ow{border-color:inherit;text-align:center;vertical-align:top}
.tg .tg-7btt{font-weight:bold;border-color:inherit;text-align:center;vertical-align:top}
.tg .tg-fymr{font-weight:bold;border-color:inherit;text-align:left;vertical-align:top}
.tg .tg-0pky{border-color:inherit;text-align:left;vertical-align:top}
</style>
<table class="tg">
  <tr>
    <td class="tg-0pky">${\bf x}_ i$</td>
    <td class="tg-0pky">$1\times p$ row-vector</td>
  </tr>
  <tr>
    <td class="tg-0pky">${\bf W_1}$</td>
    <td class="tg-0pky">$p\times p_1$ matrix</td>
  </tr>
  <tr>
    <td class="tg-0pky">${\bf b_1}$</td>
    <td class="tg-0pky">$1\times p_1$ row-vector</td>
  </tr>
  <tr>
    <td class="tg-0pky">$f_1$</td>
    <td class="tg-0pky">$\mathbb{R}^{1 \times p_1} \rightarrow \mathbb{R}^{1 \times p_1}$ vector function</td>
  </tr>
  <tr>
    <td class="tg-0pky">${\bf W_2}$</td>
    <td class="tg-0pky">$p_1\times p_2$ matrix</td>
  </tr>
  <tr>
    <td class="tg-0pky">${\bf b_2}$</td>
    <td class="tg-0pky">$1\times p_2$ row-vector</td>
  </tr>
  <tr>
    <td class="tg-0pky">$f_2$</td>
    <td class="tg-0pky">$\mathbb{R}^{1 \times p_2} \rightarrow \mathbb{R}^{1 \times p_2}$ vector function</td>
  </tr>
  <tr>
    <td class="tg-0pky">${\bf W_3}$</td>
    <td class="tg-0pky">$p_2\times p_3$ matrix</td>
  </tr>
  <tr>
    <td class="tg-0pky">${\bf b_3}$</td>
    <td class="tg-0pky">$1\times p_3$ row-vector</td>
  </tr>
  <tr>
    <td class="tg-0pky">${\bf y}_i$</td>
    <td class="tg-0pky">$1\times p_3$ row-vector</td>
  </tr>
  <tr>
    <td class="tg-0pky">${\boldsymbol\epsilon}_ i$</td>
    <td class="tg-0pky">$1\times p_3$ row-vector</td>
  </tr>  
</table></center>

- 잘 살펴보면 입력데이터 ${\bf x}_ i$를 어떠한 변환 ${\bf W}_ 1$를 활용하여 선형변환하고 그 뒤에 비선형함수 $f_1$를 씌우는 것을 반복하는 형태이다. 진짜 별 내용이 없다. 

- 여러개의 observation의 경우에는 아래와 같이 간단하게 나타낼 수 있다. 
\begin{align}
{\bf y} = f_2\big(f_1({\bf X} {\bf W_1}+{\bf b_1}){\bf W_2}+{\bf b_2}\big){\bf W_3}+{\bf b_3} + {\boldsymbol \epsilon}
\end{align} 
매우 간결하다. 하지만 사실 이건 거의 나만 쓰는 노테이션이다. 책에서는 보통 저렇게 표현안하고 하나의 observation만 주로 표현하는데 그 이유는 저 노테이션이 혼동의 여지가 다분하기 때문이다(하지만 나는 간결한게 좋기 때문에 그냥 위와 같이 쓸란다). 당장 ${\bf b_1}$만 보아도 $i$-th observation만 고려하는 식에서는 디멘젼이 $1\times p_1$ row-vector이지만 모든 observation을 고려하는 식에서는 $n\times p_1$이 된다. 그 외에도 ${\bf y}$가 $n\times 1$ 벡터가 아니라 $n\times p_3$ 매트릭스라서 원래는 ${\bf Y}$로 쓰는게 더 맞을것 같지만 위에는 관습적으로 ${\bf y}$를 썼다는 둥의 사소한 문제가 있다(왠지 이렇게 써야지 종속변수 같은 느낌이 더 들어서 나는 저렇게 쓴다. 또한 나중에 손실함수 미분하면서 알겠지만 어차피 WLOG ${\bf y}$ 를 $n\times 1$ vector로 생각할 수 있다는 사실을 알게 된다). 마지막으로 가장 심각하게 헷갈리는건 $f$가 $\mathbb{R}^{n\times p_2} \rightarrow \mathbb{R}^{n \times p_2}$와 같이 matrix 를 matrix 로 맵핑하는 함수처럼 보이는데 실제로는 row-vector 를 row-vector 로 맵핑하는 형태의 함수이다. 이건 당연한게 observation 마다 다른 활성화 함수를 적용할 수 는 없는 노릇이니 **each row** 에 적용되는 $f$는 동일할 수밖에 없다. 그리고 사실 엄밀하게 말하면 row-vector 를 row-vector로 맵핑하는 경우도 거의 없고 (소프트맥스 정도만 여기에 해당함) 아래와 같이 scalar to scalar mapping 인 경우가 대부분이다. 주로 사용되는 $f$는 아래와 같다. <br/><br/>
**(1)** $f$가 항등함수인 경우: 
\begin{align}
f ~ \big(({\bf X W+b})_ {ij}\big)=({\bf X W+b})_ {ij}
\end{align}
<br/>
**(2)** $f$가 로지스틱함수(=시그모이드함수)인 경우:
\begin{align}
f~ \big(({\bf X W+b})_ {ij}\big)=\frac{e^{({\bf X W+b})_ {ij}}}{1+e^{({\bf X W+b})_ {ij}}}=\frac{1}{1+e^{-({\bf X W+b})_ {ij}}}
\end{align}
<br/>
**(3)** $f$가 소프트맥스함수인 경우:
\begin{align}
f \big({\bf x}_ i {\bf W}+{\bf b}\big) = \frac{e^{ {\bf x}_ i {\bf W}+{\bf b} } }{e^{ {\bf x}_ i {\bf W}+{\bf b} }{\bf 1}}
\end{align}
<br/>
소프트맥스 함수에서 $\exp$와 $/$는 **elementwise operation** 이다. 따라서 위에서 $e^{ {\bf x}_ i {\bf W}+{\bf b} }$는 row-vector가 된다(왜냐하면 ${\bf x}_ i {\bf W}+{\bf b}$ 가 row-vector 이니까). 따라서 분자는 row-vector 이고 분모는 row-vector $\times$ col-vector 꼴이니까 scalar가 된다. 그래서 $f \big({\bf x}_ i {\bf W}+{\bf b} \big)$ 의 차원은 row-vector 가 된다(이런 당연한 것도 첨에는 한번씩 다 따져보는게 좋다). 즉 소프트맥스의 경우 입력이 row-vector ${\bf x}_ i {\bf W}+{\bf b} $이고, 출력 $f \big({\bf x}_ i {\bf W}+{\bf b} \big)$ 역시 row-vector 이므로 row-vector 를 row-vector 로 맵핑하는 함수가 된다. 

- 참고로 각층의 입력에 $\bf 1$ 벡터를 추가하면 바이어스텀을 생략할 수 있다. 앞으로는 그냥 바이어스텀을 생략하고 쓰겠다. 따라서 모델은 아래와 같이 된다. 
\begin{align}
{\bf y} = f_2\big(f_1({\bf X} {\bf W_1}){\bf W_2} \big){\bf W_3} + {\boldsymbol \epsilon}
\end{align} 

### 기울기계산 

- 신경망을 아래와 같이 쓸 수 있다. 
\begin{align}
{\bf y} = f_2\big(f_1({\bf X} {\bf W_1}){\bf W_2}\big){\bf W_3} + {\boldsymbol \epsilon}
\end{align} 
우리의 목적은 
\begin{align}
J=tr\Big(\big({\bf y}-{\bf\hat y}\big)'\big({\bf y}-{\bf\hat y}\big)\Big)
\end{align}
을 최소화하는 적당한 $\bf W_1, W_2, W_3$을 찾는 것이다. 주의할 점은 
\begin{align}
\big({\bf y}-{\bf\hat y}\big)'\big({\bf y}-{\bf\hat y}\big)
\end{align}
은 $p_3\times p_3$ 매트릭스 이지만 $J$는 스칼라라는 점이다. 트레이스는 선형함수이므로 앞으로 진행되는 논의에서 ${\bf y}$ 는 $n \times 1$ vector 라고 생각해도 무방하다(앞으로 미분만 할건데, 더해진것을 미분하는것은 각각 미분해서 더한것과 같으므로 요런식의 WLOG가 가능하다). 즉 $p_3=1$ 로 두어도 무방하다는 의미이다. 따라서 앞으로는 ${\bf y}$ 는 그냥 $n \times 1$ vector 라고 가정하겠다. 

- 따라서 아래를 연립해서 풀면된다. 
\begin{align}
\begin{cases}
\frac{\partial J}{\partial {\bf W_1} }=0 \\\\ 
\frac{\partial J}{\partial {\bf W_2} }=0 \\\\ \\
\frac{\partial J}{\partial {\bf W_3} }=0
\end{cases}
\end{align}

- 편의상 아래와 같이 기호를 정의하자. 
\begin{align}
\begin{cases}
{\bf \tilde X_3} := g\big(f({\bf X} {\bf W_1}){\bf W_2} \big) \\\\ \\
{\bf \tilde X_2} := f({\bf X} {\bf W_1}) \\\\ \\
{\bf \tilde X_1} := {\bf X}
\end{cases}
\end{align}
그러면 아래와 같이 쓸 수 있다
\begin{align}
{\bf y} & = f_2\big(f_1({\bf X} {\bf W_1}){\bf W_2}\big){\bf W_3} + {\boldsymbol \epsilon}  \\\\ \\
& = f_2\big(f_1({\bf \tilde X_1} {\bf W_1}){\bf W_2}\big){\bf W_3} + {\boldsymbol \epsilon} \\\\ \\ 
& = f_2({\bf \tilde X_2}{\bf W_2}){\bf W_3} + {\boldsymbol \epsilon} \\\\ \\ 
& = {\bf \tilde X_3}{\bf W_3} + {\boldsymbol \epsilon} 
\end{align} 

- 이제 하나씩 풀어보자. 
$J={\boldsymbol\epsilon}'{\boldsymbol\epsilon}=\big({\bf y}-f_2(f_1({\bf X} {\bf W_1}){\bf W_2}){\bf W_3}\big)'\big({\bf y}-g(f({\bf X} {\bf W_1}){\bf W_2}){\bf W_3}\big)$으로 표현가능하다. 먼저 $\frac{\partial J}{\partial \bf W_3}=0$을 계산하면 아래와 같이 된다. 
\begin{align}
\big(f_2(f_1({\bf X} {\bf W_1}){\bf W_2})\big)'\big(f_2(f_1({\bf X} {\bf W_1}){\bf W_2} ){\bf W_3}\big) = \big(f_2(f_1({\bf X} {\bf W_1}){\bf W_2})\big)'{\bf y} 
\end{align}
따라서 
\begin{align}
{\bf \hat W_3}=\big( f_2(f_1({\bf X} {\bf W_1}){\bf W_2})\big)'\big(f_2(f_1({\bf X} {\bf W_1}){\bf W_2})\big)^{-1}\big(f_1(f_1({\bf X} {\bf W_1}){\bf W_2})\big)' {\bf y}
\end{align}
이다. 

- ${\bf \hat W_3}$ 은 아래와 같이 간단히 표현가능하다. 
\begin{align}
{\bf \hat W_3}=\big( {\bf \tilde X_3'}{\bf \tilde X_3}\big)^{-1}{\bf \tilde X_3'} {\bf y}
\end{align}

- 이제 $\frac{\partial J}{\partial \bf W_2}$ 를 계산하여 보자.  
\begin{align}
\frac{\partial J}{\partial {\bf W_2}}=\frac{\partial J}{\partial {\bf \tilde X_3'} }\frac{\partial {\bf \tilde X_3'}}{\partial {\bf W_2} }
\end{align}
참고로 ${\bf \tilde X_3'}$ 는 $n\times p_2$ 이고 ${\bf W_2}$는 $p_1 \times p_2$ 이므로 아래의 미분은 잘 정의된다. 
\begin{align}
\frac{\partial {\bf \tilde X_3'} }{\partial {\bf W_2}. 
\end{align}

### 용어정리

- **배치**: 

- **미니배치**: 

- **에폭**: 
